{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1923d9",
   "metadata": {},
   "source": [
    "# AIS Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304e4b6",
   "metadata": {},
   "source": [
    "___\n",
    "Setup and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d1426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "\n",
    "# NOTE: The 'utils.plot_ship_trajectory' function requires an external 'utils.py' file.\n",
    "# from utils import plot_ship_trajectory \n",
    "\n",
    "# Global Constants\n",
    "WINDOW = 20\n",
    "HORIZON = 1\n",
    "STEP = 1\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "LR = 1e-4\n",
    "DEVICE = \"cpu\"  # set to \"cuda\" if GPU\n",
    "\n",
    "CSV_PATH = \"data/ais_data_5min_clean.csv\"\n",
    "\n",
    "# Physics-aware features:\n",
    "FEATURES = [\n",
    "    \"Latitude\",\n",
    "    \"Longtitude\",\n",
    "    \"SOG\",\n",
    "    \"COG\",\n",
    "    \"vx\",\n",
    "    \"vy\",\n",
    "    \"dSOG\",\n",
    "    \"dCOG\"\n",
    "]\n",
    "\n",
    "TARGET_DIM = 2  # Δlat, Δlon\n",
    "\n",
    "# For Visualization\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56c2e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "___\n",
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d7dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mean absolute Error for Latitude and Longitude\n",
    "def compute_mae(pred, true):\n",
    "    if not isinstance(pred, np.ndarray):\n",
    "        # Detaches the tensor from the computation graph and stops tracking for gradients\n",
    "        pred = pred.detach().cpu().numpy().astype(np.float32)\n",
    "    if not isinstance(true, np.ndarray):\n",
    "        true = true.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # This line performs element-wise subtraction\n",
    "    err = pred - true\n",
    "\n",
    "    mae_lat = np.mean(np.abs(err[..., 0]))\n",
    "    mae_lon = np.mean(np.abs(err[..., 1]))\n",
    "\n",
    "    return mae_lat, mae_lon\n",
    "\n",
    "# Root Mean Square Error for Latitude and Longitude\n",
    "def compute_rmse(pred, true):\n",
    "    if not isinstance(pred, np.ndarray):\n",
    "        pred = pred.detach().cpu().numpy().astype(np.float32)\n",
    "    if not isinstance(true, np.ndarray):\n",
    "        true = true.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    err = pred - true\n",
    "\n",
    "    rmse_lat = np.sqrt(np.mean(err[..., 0] ** 2))\n",
    "    rmse_lon = np.sqrt(np.mean(err[..., 1] ** 2))\n",
    "\n",
    "    return rmse_lat, rmse_lon\n",
    "\n",
    "# Calculates the distance between two geographical points using the Haversine formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    Calculates the great-circle distance between two points (lat1, lon1)\n",
    "    and (lat2, lon2) using the Haversine formula\n",
    "    '''\n",
    "    R = 6371000.0\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat / 2) ** 2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Apply the haversine distance to every predicted/true coordinate pair\n",
    "def compute_haversine_metrics(pred_coords, true_coords):\n",
    "    distances = []\n",
    "    n = min(len(pred_coords), len(true_coords))\n",
    "\n",
    "    for i in range(n):\n",
    "        p = pred_coords[i]\n",
    "        t = true_coords[i]\n",
    "        d = haversine(p[0], p[1], t[0], t[1])\n",
    "        distances.append(d)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    # Calculate Mean Haversine Distance and Haversine RMSE\n",
    "    mean_haversine = distances.mean()\n",
    "    rmse_haversine = np.sqrt((distances ** 2).mean())\n",
    "    return mean_haversine, rmse_haversine\n",
    "\n",
    "# Mean Absolute Error between the predicted speed and the true speed of a vessel\n",
    "def compute_speed_error(pred_coords, true_coords, seconds = 60.0):\n",
    "    speeds_pred = []\n",
    "    speeds_true = []\n",
    "\n",
    "    n = min(len(pred_coords) - 1, len(true_coords) - 1)\n",
    "\n",
    "    for i in range(n):\n",
    "        dp = haversine(pred_coords[i][0], pred_coords[i][1],\n",
    "                       pred_coords[i + 1][0], pred_coords[i + 1][1])\n",
    "        dt = haversine(true_coords[i][0], true_coords[i][1],\n",
    "                       true_coords[i + 1][0], true_coords[i + 1][1])\n",
    "        # Speed is calculated as Distance / Time\n",
    "        speeds_pred.append(dp / seconds)\n",
    "        speeds_true.append(dt / seconds)\n",
    "    # Calculates the absolute difference between the predicted and true speeds for all intervals\n",
    "    return np.mean(np.abs(np.array(speeds_pred) - np.array(speeds_true)))\n",
    "\n",
    "'''\n",
    "Quantifies the prediction error in how sharply the vessel turns by comparing the predicted rate\n",
    "of change of the course (Course Over Ground - COG) against the true rate of change\n",
    "'''\n",
    "def compute_turn_rate_error(pred_coords, true_coords):\n",
    "    # calculates COG between successive points based on the change\n",
    "    # in latitude (dlat) and longitude (dlon)\n",
    "    def compute_cog(lat, lon):\n",
    "        dlat = np.diff(lat)\n",
    "        dlon = np.diff(lon)\n",
    "        return np.degrees(np.arctan2(dlon, dlat))\n",
    "\n",
    "    n = min(len(pred_coords), len(true_coords))\n",
    "    pred = pred_coords[:n]\n",
    "    true = true_coords[:n]\n",
    "\n",
    "    pred_cog = compute_cog(pred[:, 0], pred[:, 1])\n",
    "    true_cog = compute_cog(true[:, 0], true[:, 1])\n",
    "\n",
    "    # Ensure cogs have at least two points to compute turn rate\n",
    "    if len(pred_cog) < 2 or len(true_cog) < 2:\n",
    "        return 0.0  # Return 0 if not enough data for turn rate\n",
    "\n",
    "    # The turn rate is the difference between two successive COG values\n",
    "    # Represents the change in heading\n",
    "    pred_turn = np.diff(pred_cog)\n",
    "    true_turn = np.diff(true_cog)\n",
    "\n",
    "    n2 = min(len(pred_turn), len(true_turn))\n",
    "    return np.mean(np.abs(pred_turn[:n2] - true_turn[:n2]))\n",
    "\n",
    "# Presenting all trajectory evaluation metrics\n",
    "def evaluate_metrics(pred_coords, actual_coords):\n",
    "    n = min(len(pred_coords), len(actual_coords))\n",
    "    pred = pred_coords[:n]\n",
    "    true = actual_coords[:n]\n",
    "    mae_lat, mae_lon = compute_mae(pred, true)\n",
    "    mean_hav, rmse_hav = compute_haversine_metrics(pred, true)\n",
    "    speed_err = compute_speed_error(pred, true, seconds = 300)\n",
    "    turn_err = compute_turn_rate_error(pred, true)\n",
    "\n",
    "    print(\"\\n===== METRICS =====\")\n",
    "    print(f\"MAE Latitude:     {mae_lat:.6f}\")\n",
    "    print(f\"MAE Longitude:    {mae_lon:.6f}\")\n",
    "    print(f\"Mean Haversine:   {mean_hav:.3f} m\")\n",
    "    print(f\"Haversine RMSE:   {rmse_hav:.3f} m\")\n",
    "    print(f\"Speed Error:      {speed_err:.4f} m/s\")\n",
    "    print(f\"Turn-Rate Error:  {turn_err:.4f} deg\")\n",
    "    print(\"===================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"mae_lat\": mae_lat,\n",
    "        \"mae_lon\": mae_lon,\n",
    "        \"mean_haversine\": mean_hav,\n",
    "        \"rmse_haversine\": rmse_hav,\n",
    "        \"speed_error\": speed_err,\n",
    "        \"turn_rate_error\": turn_err\n",
    "    }\n",
    "\n",
    "# Calculate velocity components and deltas\n",
    "def compute_physics_features(df):\n",
    "    \"\"\"Add vx, vy, dSOG, dCOG features per segment.\"\"\"\n",
    "    dfs = []\n",
    "\n",
    "    for (mmsi, seg), g in df.groupby([\"MMSI\", \"Segment\"], observed=True):\n",
    "        g = g.sort_values(\"Timestamp\").copy()\n",
    "\n",
    "        # Convert COG to radians\n",
    "        rad = np.radians(g[\"COG\"].values)\n",
    "\n",
    "        '''\n",
    "        Calculates the horizontal (vx) and vertical (vy) velocity components from the ship's\n",
    "        Speed Over Ground (SOG) and Course Over Ground (COG)\n",
    "        '''\n",
    "        g[\"vx\"] = g[\"SOG\"] * np.cos(rad)\n",
    "        g[\"vy\"] = g[\"SOG\"] * np.sin(rad)\n",
    "\n",
    "        # Difference (delta) in SOG between the current and previous time step\n",
    "        g[\"dSOG\"] = g[\"SOG\"].diff().fillna(0)\n",
    "        # Handle 360-degree wrap-around for COG difference\n",
    "        dCOG_raw = g[\"COG\"].diff()\n",
    "        dCOG_wrapped = (dCOG_raw + 540) % 360 - 180\n",
    "        g[\"dCOG\"] = dCOG_wrapped.fillna(0)\n",
    "\n",
    "        dfs.append(g)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# PyTorch Haversine (for autoregressive rollout)\n",
    "def haversine_torch(lat1, lon1, lat2, lon2):\n",
    "    # Earth radius in meters\n",
    "    R = 6371000.0\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Convert degrees to radians using torch.deg2rad\n",
    "    dlat_rad = torch.deg2rad(dlat)\n",
    "    dlon_rad = torch.deg2rad(dlon)\n",
    "    lat1_rad = torch.deg2rad(lat1)\n",
    "    lat2_rad = torch.deg2rad(lat2)\n",
    "\n",
    "    # Haversine formula using torch functions\n",
    "    a = torch.sin(dlat_rad / 2) ** 2 + \\\n",
    "        torch.cos(lat1_rad) * torch.cos(lat2_rad) * torch.sin(dlon_rad / 2) ** 2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
    "\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ebf50",
   "metadata": {},
   "source": [
    "___\n",
    "Model Definition and Training, Validation and Testing Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e96b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "'''\n",
    "Define Transformer Encoder\n",
    "Attention mechanism allows the model to learn that predicting the next turn might rely heavily on the last few steps of dCOG (change in course) and COG (current course),\n",
    "while predicting speed might rely more on the current SOG (speed over ground) and the dSOG history\n",
    "'''\n",
    "class AISTransformer(nn.Module):\n",
    "    '''\n",
    "    Embedding size -> embed_Size\n",
    "    Attention heads allowing the model to focus on different parts of the input window simultaneously -> num_heads\n",
    "    Feed-Forward network Dimension (2 layers MLP) -> ff_dim\n",
    "    '''\n",
    "    def __init__(self, input_size, embed_size=64, num_heads=2,\n",
    "                 num_layers=2, ff_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, embed_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.encoder.gradient_checkpointing = True\n",
    "\n",
    "        # predict 1-step delta at a time (autoregressive decoder)\n",
    "        self.decoder = nn.Linear(embed_size, TARGET_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward only encodes input window; decoding happens outside.\"\"\"\n",
    "        # The input x is (B,W,F) and should be float32, which is ensured by the fix.\n",
    "        z = self.embedding(x)\n",
    "        z = self.encoder(z)\n",
    "        return z[:, -1, :]  # last timestep representation\n",
    "\n",
    "'''\n",
    "Implements a ship-based (or group-based) data split, ensuring that all trajectory windows belonging to a specific ship (MMSI)\n",
    "are placed entirely into one of the three sets: Training, Validation, or Testing\n",
    "'''\n",
    "def ship_based_split(meta, train_ratio=0.64, val_ratio=0.16, test_ratio=0.20, seed=42):\n",
    "    \"\"\"\n",
    "    Split ships into:\n",
    "      - Train: 64%\n",
    "      - Validation: 16%\n",
    "      - Test: 20%\n",
    "    \"\"\"\n",
    "    if not np.isclose(train_ratio + val_ratio + test_ratio, 1.0):\n",
    "        raise ValueError(\"train_ratio + val_ratio + test_ratio must sum to 1.0\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ships = np.array(meta[\"mmsi\"].unique())\n",
    "    np.random.shuffle(ships)\n",
    "\n",
    "    total = len(ships)\n",
    "\n",
    "    # Compute boundaries\n",
    "    cutoff_train = int(total * train_ratio)\n",
    "    cutoff_val = int(total * (train_ratio + val_ratio))\n",
    "\n",
    "    # Splits\n",
    "    train_ships = ships[:cutoff_train]\n",
    "    val_ships = ships[cutoff_train:cutoff_val]\n",
    "    test_ships = ships[cutoff_val:]\n",
    "\n",
    "    split_dict = {\n",
    "        \"train_ships\": train_ships.tolist(),\n",
    "        \"val_ships\": val_ships.tolist(),\n",
    "        \"test_ships\": test_ships.tolist(),\n",
    "        \"ratios\": {\n",
    "            \"train_ratio\": train_ratio,\n",
    "            \"val_ratio\": val_ratio,\n",
    "            \"test_ratio\": test_ratio\n",
    "        },\n",
    "        \"total_unique_ships\": total\n",
    "    }\n",
    "\n",
    "    with open(\"ship_splits.json\", \"w\") as f:\n",
    "        json.dump(split_dict, f, indent=4)\n",
    "\n",
    "    print(\"\\n============================\")\n",
    "    print(\" Ship-based Split Summary \")\n",
    "    print(\"============================\")\n",
    "    print(f\"Total unique ships: {total}\")\n",
    "    print(f\"Train ships ({len(train_ships)}): {train_ships.tolist()}\")\n",
    "    print(f\"Val ships   ({len(val_ships)}): {val_ships.tolist()}\")\n",
    "    print(f\"Test ships  ({len(test_ships)}): {test_ships.tolist()}\")\n",
    "    print(\"============================\\n\")\n",
    "\n",
    "    train_idx = meta.index[meta[\"mmsi\"].isin(train_ships)].to_numpy()\n",
    "    val_idx = meta.index[meta[\"mmsi\"].isin(val_ships)].to_numpy()\n",
    "    test_idx = meta.index[meta[\"mmsi\"].isin(test_ships)].to_numpy()\n",
    "\n",
    "    return train_idx, val_idx, test_idx, train_ships, val_ships, test_ships\n",
    "\n",
    "# Generate the dataset\n",
    "def build_dataset():\n",
    "    print(\"Loading CSV...\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = compute_physics_features(df)\n",
    "\n",
    "    raw_windows = []\n",
    "    raw_targets = []\n",
    "    meta_rows = []\n",
    "\n",
    "    print(\"Building raw (unscaled) windows...\")\n",
    "    for (mmsi, seg), g in df.groupby([\"MMSI\", \"Segment\"], observed=True):\n",
    "        g = g.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "        if len(g) < WINDOW + HORIZON + 1:\n",
    "            continue\n",
    "\n",
    "        lat = g[\"Latitude\"].values\n",
    "        lon = g[\"Longtitude\"].values\n",
    "        dlat = lat[1:] - lat[:-1]\n",
    "        dlon = lon[1:] - lon[:-1]\n",
    "        deltas = np.stack([dlat, dlon], axis=1)\n",
    "\n",
    "        feat = g[FEATURES].values\n",
    "\n",
    "        for i in range(0, len(g) - WINDOW - HORIZON, STEP):\n",
    "            # For each segment, it slides a window\n",
    "\n",
    "            X_window_raw = feat[i:i + WINDOW]\n",
    "            # contains the delta lon and delta lat for the next HORIZON\n",
    "            future_raw = deltas[i + WINDOW - 1:i + WINDOW - 1 + HORIZON]\n",
    "\n",
    "            if future_raw.shape[0] != HORIZON:\n",
    "                continue\n",
    "\n",
    "            raw_windows.append(X_window_raw)\n",
    "            raw_targets.append(future_raw)\n",
    "\n",
    "            meta_rows.append({\n",
    "                \"mmsi\": mmsi,\n",
    "                \"segment\": seg,\n",
    "                \"last_lat\": lat[i + WINDOW - 1],\n",
    "                \"last_lon\": lon[i + WINDOW - 1]\n",
    "            })\n",
    "    # The collected windows are converted into NumPy arrays\n",
    "    raw_X = np.array(raw_windows, dtype=np.float32)\n",
    "    raw_y = np.array(raw_targets, dtype=np.float32)\n",
    "    meta = pd.DataFrame(meta_rows)\n",
    "\n",
    "    print(f\"Built RAW dataset: X={raw_X.shape}, y={raw_y.shape}, meta={meta.shape}\")\n",
    "\n",
    "    # Ship-based split (done ONCE here)\n",
    "    train_idx, val_idx, test_idx, train_ships, val_ships, test_ships = ship_based_split(meta)\n",
    "\n",
    "    # Fit scalers on train\n",
    "    train_feat = raw_X[train_idx].reshape(-1, raw_X.shape[-1])\n",
    "    train_deltas = raw_y[train_idx].reshape(-1, 2)\n",
    "\n",
    "    global_scaler_X = StandardScaler().fit(train_feat)\n",
    "    global_scaler_y = StandardScaler().fit(train_deltas)\n",
    "\n",
    "    # Transform entire dataset\n",
    "    X_scaled = global_scaler_X.transform(raw_X.reshape(-1, raw_X.shape[-1])).reshape(raw_X.shape).astype(np.float32)\n",
    "    y_scaled = global_scaler_y.transform(raw_y.reshape(-1, 2)).reshape(raw_y.shape).astype(np.float32)\n",
    "\n",
    "    print(\"Dataset scaling done. No leakage.\")\n",
    "\n",
    "    segment_scalers = {\n",
    "        (meta.loc[i, \"mmsi\"], meta.loc[i, \"segment\"]): (global_scaler_X, global_scaler_y)\n",
    "        for i in range(len(meta))\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        X_scaled, y_scaled, meta,\n",
    "        segment_scalers,\n",
    "        train_idx, val_idx, test_idx,\n",
    "        global_scaler_X, global_scaler_y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc123a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates the future trajectory of a ship over a given horizon by using its own predictions as new inputs for the next time step\n",
    "def autoregressive_rollout(model, window, scaler_X, scaler_y, horizon=HORIZON, seconds = 60.0):\n",
    "    device = window.device\n",
    "    B = window.size(0)\n",
    "\n",
    "    # scaler stats\n",
    "    X_mean = torch.tensor(scaler_X.mean_, device=device, dtype=torch.float32)\n",
    "    X_std  = torch.tensor(scaler_X.scale_, device=device, dtype=torch.float32)\n",
    "    y_mean = torch.tensor(scaler_y.mean_, device=device, dtype=torch.float32)\n",
    "    y_std  = torch.tensor(scaler_y.scale_, device=device, dtype=torch.float32)\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    # Inverse-scale last raw features\n",
    "    # Get the last known physical state\n",
    "    last_raw = window[:, -1, :] * X_std + X_mean\n",
    "\n",
    "    prev_lat = last_raw[:, 0]\n",
    "    prev_lon = last_raw[:, 1]\n",
    "\n",
    "    # During training we don't update physics features\n",
    "    prev_raw = last_raw.clone()\n",
    "\n",
    "    for _ in range(horizon):\n",
    "        # Encode window\n",
    "        z = model(window)\n",
    "        delta_scaled = model.decoder(z)\n",
    "        pred_list.append(delta_scaled)\n",
    "\n",
    "        # Inverse-scale Δlat/Δlon\n",
    "        delta_real = delta_scaled * y_std + y_mean\n",
    "        dlat = delta_real[:, 0]\n",
    "        dlon = delta_real[:, 1]\n",
    "\n",
    "        # Update pos\n",
    "        cur_lat = prev_lat + dlat\n",
    "        cur_lon = prev_lon + dlon\n",
    "\n",
    "        # Distance & SOG\n",
    "        # Calculates the physical distance (dist_m) between the previous and current predicted points\n",
    "        dist_m = haversine_torch(prev_lat, prev_lon, cur_lat, cur_lon)\n",
    "        SOG = dist_m / seconds # minute AIS interval\n",
    "\n",
    "        # COG (Course Over Ground)\n",
    "        dY = cur_lat - prev_lat\n",
    "        dX = cur_lon - prev_lon\n",
    "        COG = (torch.rad2deg(torch.atan2(dX, dY)) + 360) % 360\n",
    "\n",
    "        # Velocity Vectors (vx, vy)\n",
    "        vx = SOG * torch.cos(torch.deg2rad(COG))\n",
    "        vy = SOG * torch.sin(torch.deg2rad(COG))\n",
    "\n",
    "        # Deltas (dSOG, dCOG)\n",
    "        dSOG = SOG - prev_raw[:, 2]\n",
    "        dCOG = ((COG - prev_raw[:, 3] + 540) % 360) - 180\n",
    "\n",
    "        # Create the new feature vector with all calculated values\n",
    "        new_raw = torch.stack(\n",
    "            [cur_lat, cur_lon, SOG, COG, vx, vy, dSOG, dCOG],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # Scale new row\n",
    "        new_scaled = (new_raw - X_mean) / X_std\n",
    "\n",
    "        # Slide window\n",
    "        window = torch.cat([window[:, 1:], new_scaled.unsqueeze(1)], dim=1)\n",
    "\n",
    "        # Update prev\n",
    "        prev_lat = cur_lat\n",
    "        prev_lon = cur_lon\n",
    "        prev_raw = new_raw\n",
    "\n",
    "    return torch.stack(pred_list, dim=1), window\n",
    "\n",
    "def train_model():\n",
    "    (\n",
    "        X, y, meta,\n",
    "        segment_scalers,\n",
    "        train_idx, val_idx, test_idx,\n",
    "        global_scaler_X, global_scaler_y\n",
    "    ) = build_dataset()\n",
    "\n",
    "    train_ds = TrajectoryDataset(X[train_idx], y[train_idx])\n",
    "    val_ds = TrajectoryDataset(X[val_idx], y[val_idx])\n",
    "    test_ds = TrajectoryDataset(X[test_idx], y[test_idx])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
    "    pin_memory=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=0,\n",
    "    pin_memory=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=0,\n",
    "    pin_memory=False)\n",
    "\n",
    "    # Ensure model is on the correct device\n",
    "    model = AISTransformer(input_size=len(FEATURES)).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    # The Huber Loss is selected. This loss is less sensitive to extreme outliers\n",
    "    # compared to Mean Squared Error (MSE), making training more robust for noisy data\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_mae_lat = 0\n",
    "        total_mae_lon = 0\n",
    "        total_rmse_lat = 0\n",
    "        total_rmse_lon = 0\n",
    "\n",
    "        # The inner loop iterates through all batches (xb, yb) in the training data\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            # clears gradients from the previous step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # The model generates predictions by running the autoregressive rollout over the input window\n",
    "            pred_scaled, _ = autoregressive_rollout(\n",
    "                model, xb,\n",
    "                global_scaler_X, global_scaler_y,\n",
    "                horizon=HORIZON,\n",
    "                seconds=300\n",
    "            )\n",
    "\n",
    "            pred_t = pred_scaled.float()  # Ensure float32\n",
    "\n",
    "            loss = criterion(pred_t, yb)\n",
    "            mae_lat, mae_lon = compute_mae(pred_t, yb)\n",
    "            rmse_lat, rmse_lon = compute_rmse(pred_t, yb)\n",
    "\n",
    "            # calculates the gradients of the loss with respect to all model parameters\n",
    "            loss.backward()\n",
    "            # updates the parameters using the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae_lat += mae_lat\n",
    "            total_mae_lon += mae_lon\n",
    "            total_rmse_lat += rmse_lat\n",
    "            total_rmse_lon += rmse_lon\n",
    "\n",
    "        n_batches = len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {total_loss / n_batches:.6f}\")\n",
    "        print(f\"Train MAE(lat): {total_mae_lat / n_batches:.6f}, \"\n",
    "              f\"MAE(lon): {total_mae_lon / n_batches:.6f}\")\n",
    "\n",
    "        # disable training features and save memory/time,\n",
    "        # as gradients are not needed for evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "                pred_scaled, _ = autoregressive_rollout(\n",
    "                    model, xb,\n",
    "                    global_scaler_X, global_scaler_y,\n",
    "                    horizon=HORIZON, \n",
    "                    seconds = 300)\n",
    "                pred_t = pred_scaled.float()\n",
    "\n",
    "                val_loss += criterion(pred_t, yb).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # If the calculated val_loss is significantly better than best_val_loss, the model's state\n",
    "        # (weights and context) is saved to \"ais_model_best.pth\", and patience is reset\n",
    "        if val_loss < best_val_loss - MIN_DELTA:\n",
    "            best_val_loss = val_loss\n",
    "            # Save model for best val loss\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"segment_scalers\": segment_scalers,\n",
    "                \"meta\": meta,\n",
    "                \"train_idx\": train_idx,\n",
    "                \"val_idx\": val_idx,\n",
    "                \"test_idx\": test_idx,\n",
    "                \"global_scaler_X\": global_scaler_X,\n",
    "                \"global_scaler_y\": global_scaler_y,\n",
    "                \"X\": X, \"y\": y\n",
    "            }, \"ais_model_best.pth\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Load best model for final testing\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Loading best model for test evaluation...\")\n",
    "        best_ckpt = torch.load(\"ais_model_best.pth\", map_location=DEVICE, weights_only=False)\n",
    "        model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    '''\n",
    "    The final model is evaluated once on the completely unseen test set (test_loader),\n",
    "    providing an unbiased estimate of the model's performance on new ships\n",
    "    '''\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred_scaled, _ = autoregressive_rollout(\n",
    "                model, xb,\n",
    "                global_scaler_X, global_scaler_y,\n",
    "                horizon=HORIZON,\n",
    "                seconds = 300\n",
    "            )\n",
    "            pred_t = pred_scaled.float()\n",
    "            test_loss += criterion(pred_t, yb).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    # Final save of the model (could be the best or the last epoch)\n",
    "    final_save_path = \"ais_model_final.pth\"\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"segment_scalers\": segment_scalers,\n",
    "        \"meta\": meta,\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"global_scaler_X\": global_scaler_X,\n",
    "        \"global_scaler_y\": global_scaler_y,\n",
    "        \"X\": X, \"y\": y\n",
    "    }, final_save_path)\n",
    "\n",
    "    print(f\"Model state saved to {final_save_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac2779",
   "metadata": {},
   "source": [
    "___\n",
    "Prediction of a ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de2288da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs autoregressive prediction for a single starting window (idx), incorporating the model's physics-aware features at every stage\n",
    "def predict_sample_autoregressive(idx, seconds=60.0):\n",
    "    \"\"\"Autoregressive prediction using ALL physics-aware features.\"\"\"\n",
    "    try:\n",
    "        ckpt = torch.load(\"ais_model_final.pth\", map_location=DEVICE, weights_only=False)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'ais_model_final.pth' not found. Run train_model() first.\")\n",
    "        return np.array([])\n",
    "\n",
    "    meta = ckpt[\"meta\"]\n",
    "    X = ckpt[\"X\"]\n",
    "    \n",
    "    # Use global scalers from the checkpoint\n",
    "    scaler_X, scaler_y = ckpt[\"global_scaler_X\"], ckpt[\"global_scaler_y\"]\n",
    "\n",
    "    # Load the model\n",
    "    model = AISTransformer(input_size=len(FEATURES)).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Initial scaled window (already float32 from build_dataset fix)\n",
    "    window = X[idx].copy()\n",
    "\n",
    "    # Last observed actual coordinates\n",
    "    prev_lat = float(meta.loc[idx, \"last_lat\"])\n",
    "    prev_lon = float(meta.loc[idx, \"last_lon\"])\n",
    "\n",
    "    # Extract last feature values (scaled → inverse scaled)\n",
    "    last_feat_scaled = window[-1]\n",
    "    last_feat = scaler_X.inverse_transform(last_feat_scaled.reshape(1, -1))[0]\n",
    "\n",
    "    prev_SOG = last_feat[2]\n",
    "    prev_COG = last_feat[3]\n",
    "\n",
    "    pred_coords = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # The function loops HORIZON number of times, generating one future step in each iteration\n",
    "        for _ in range(HORIZON):\n",
    "            # Model input: ensure it is float32\n",
    "            x = torch.tensor(window[np.newaxis, ...], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "            # Predict Δlat, Δlon (scaled)\n",
    "            z = model(x)\n",
    "            # Take the first (and only) batch item, move to CPU, convert to numpy\n",
    "            delta_scaled = model.decoder(z)[0].cpu().numpy()\n",
    "\n",
    "            # Inverse-transform deltas\n",
    "            delta_real = scaler_y.inverse_transform(delta_scaled.reshape(1, -1))[0]\n",
    "            dlat, dlon = delta_real\n",
    "\n",
    "            # Update coordinates (now floats/scalars)\n",
    "            cur_lat = prev_lat + dlat\n",
    "            cur_lon = prev_lon + dlon\n",
    "            pred_coords.append([cur_lat, cur_lon])\n",
    "\n",
    "            # --- Physics Update for Autoregressive Step (using torch for haversine) ---\n",
    "            p_lat = torch.tensor(prev_lat, dtype=torch.float32, device=DEVICE)\n",
    "            p_lon = torch.tensor(prev_lon, dtype=torch.float32, device=DEVICE)\n",
    "            c_lat = torch.tensor(cur_lat, dtype=torch.float32, device=DEVICE)\n",
    "            c_lon = torch.tensor(cur_lon, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            # Calculates the physical distance (dist_m) between the previous and current predicted points\n",
    "            dist_m_tensor = haversine_torch(p_lat, p_lon, c_lat, c_lon)\n",
    "            dist_m = dist_m_tensor.item()\n",
    "            # -------------------------------------------------------------------------\n",
    "\n",
    "            SOG = dist_m /seconds\n",
    "\n",
    "            dY = cur_lat - prev_lat\n",
    "            dX = cur_lon - prev_lon\n",
    "            COG = (np.degrees(np.arctan2(dX, dY)) + 360) % 360\n",
    "\n",
    "            vx = SOG * np.cos(np.radians(COG))\n",
    "            vy = SOG * np.sin(np.radians(COG))\n",
    "\n",
    "            dSOG = SOG - prev_SOG\n",
    "            dCOG = (COG - prev_COG + 540) % 360 - 180  # normalize turn rate\n",
    "\n",
    "            # New raw feature row (unscaled)\n",
    "            new_row_array = np.array([[\n",
    "                cur_lat, cur_lon, SOG, COG, vx, vy, dSOG, dCOG\n",
    "            ]], dtype=np.float32)\n",
    "\n",
    "            # Scale new row for next window\n",
    "            new_scaled = scaler_X.transform(new_row_array)[0]\n",
    "\n",
    "            # Slide window\n",
    "            window = np.vstack([window[1:], new_scaled.astype(np.float32)])\n",
    "\n",
    "            # Update prev state\n",
    "            # The predicted position and physics features are updated\n",
    "            prev_lat, prev_lon = cur_lat, cur_lon\n",
    "            prev_SOG, prev_COG = SOG, COG\n",
    "\n",
    "    return np.array(pred_coords)\n",
    "\n",
    "\n",
    "# creates an interactive HTML map using the folium library to visually compare the entire actual trajectory against the concatenation of all predicted steps\n",
    "def visualize_full_ship_route(full_actual, pred_future, mmsi, save_path=None):\n",
    "    if len(full_actual) == 0:\n",
    "        print(\"No actual data to plot.\")\n",
    "        return\n",
    "\n",
    "    m = folium.Map(location=[full_actual[0][0], full_actual[0][1]], zoom_start=9)\n",
    "\n",
    "    # FULL ACTUAL TRAJECTORY (blue)\n",
    "    folium.PolyLine(\n",
    "        locations=[[lat, lon] for lat, lon in full_actual],\n",
    "        color=\"blue\",\n",
    "        weight=4,\n",
    "        opacity=0.9,\n",
    "        tooltip=\"Actual Route (Full)\"\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Start and end markers for actual track\n",
    "    folium.Marker(\n",
    "        location=[full_actual[0][0], full_actual[0][1]],\n",
    "        popup=\"Start (Actual)\",\n",
    "        icon=folium.Icon(color=\"green\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[full_actual[-1][0], full_actual[-1][1]],\n",
    "        popup=\"End (Actual)\",\n",
    "        icon=folium.Icon(color=\"blue\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    # PREDICTED ROUTE (red)\n",
    "    if len(pred_future) > 0:\n",
    "        folium.PolyLine(\n",
    "            locations=[[lat, lon] for lat, lon in pred_future],\n",
    "            color=\"red\",\n",
    "            weight=4,\n",
    "            opacity=0.9,\n",
    "            tooltip=\"Predicted Route\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    # The map is saved as an interactive HTML file\n",
    "    if save_path is None:\n",
    "        save_path = f\"ship_{mmsi}_actual_vs_predicted.html\"\n",
    "\n",
    "    m.save(save_path)\n",
    "    print(f\"Map saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Manages the overall evaluation process for all segments belonging to a single ship\n",
    "def evaluate_full_ship(mmsi):\n",
    "    try:\n",
    "        ckpt = torch.load(\"ais_model_final.pth\", map_location=DEVICE, weights_only=False)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'ais_model_final.pth' not found. Run train_model() first.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Build model and load weights\n",
    "    model = AISTransformer(input_size=len(FEATURES)).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Load scalers & meta\n",
    "    meta = ckpt[\"meta\"]\n",
    "    try:\n",
    "        df_full = pd.read_csv(CSV_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {CSV_PATH}. Cannot get actual trajectory.\")\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    full_ship_df = df_full[df_full[\"MMSI\"] == mmsi].sort_values(\"Timestamp\")\n",
    "    # Create the true target trajectory\n",
    "    full_actual = full_ship_df[[\"Latitude\", \"Longtitude\"]].to_numpy()\n",
    "\n",
    "    # Get windows for this ship\n",
    "    # It filters the dataset metadata (meta) to find all starting input windows that belong to the specified ship\n",
    "    ship_meta = meta[meta[\"mmsi\"] == mmsi].copy()\n",
    "    if ship_meta.empty:\n",
    "        print(f\"No windows found for MMSI {mmsi}\")\n",
    "        return full_actual, np.array([])\n",
    "\n",
    "    # Keep consistent order\n",
    "    ship_meta[\"window_index\"] = ship_meta.index\n",
    "    ship_meta_sorted = ship_meta.sort_values([\"segment\", \"window_index\"])\n",
    "\n",
    "    all_pred_coords = []\n",
    "    for idx in ship_meta_sorted[\"window_index\"]:\n",
    "        # We exclusively use the correct autoregressive prediction function now\n",
    "        pred_coords = predict_sample_autoregressive(idx, seconds = 300)\n",
    "        all_pred_coords.extend(pred_coords.tolist())\n",
    "\n",
    "    # Returns the complete true trajectory (full_actual) and the complete predicted trajectory (pred_future) for the entire ship's route.\n",
    "    pred_future = np.array(all_pred_coords)\n",
    "    return full_actual, pred_future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e6a65",
   "metadata": {},
   "source": [
    "___\n",
    "Execution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c04769c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:44:42.966731Z",
     "start_time": "2025-11-25T19:44:27.940101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MMSI  Segment            Timestamp       SOG   COG  Longtitude  \\\n",
      "0  200000000        1  2025-02-27 00:00:00  6.790661  86.8   11.623667   \n",
      "1  200000000        1  2025-02-27 00:05:00  6.790661  90.2   11.656255   \n",
      "2  200000000        1  2025-02-27 00:10:00  6.842105  90.7   11.688977   \n",
      "3  200000000        1  2025-02-27 00:15:00  6.790661  91.0   11.721940   \n",
      "4  200000000        1  2025-02-27 00:20:00  6.636328  96.9   11.754877   \n",
      "\n",
      "    Latitude   distance_m  speed_mps_track  \n",
      "0  56.123468     0.000000         0.000000  \n",
      "1  56.122908  2020.798694        33.679978  \n",
      "2  56.122620  2028.419709        33.806995  \n",
      "3  56.122575  2043.119472        34.051991  \n",
      "4  56.122122  2042.136349        34.035606  \n",
      "Loading CSV...\n",
      "Building raw (unscaled) windows...\n",
      "Built RAW dataset: X=(44534, 20, 8), y=(44534, 1, 2), meta=(44534, 4)\n",
      "\n",
      "============================\n",
      " Ship-based Split Summary \n",
      "============================\n",
      "Total unique ships: 279\n",
      "Train ships (178): [210935000, 231884000, 247386800, 244090800, 257064430, 255915569, 220614000, 248891000, 257203000, 244140000, 219025774, 255724000, 255806370, 229084000, 246855000, 230695000, 246843000, 218041490, 248693000, 255915649, 212821000, 219009081, 246191000, 257206000, 258159000, 255802540, 219136000, 212673000, 209530000, 230712000, 210307000, 244750595, 210350000, 245176000, 255806258, 219018986, 255808000, 255815000, 209336000, 219003383, 212880000, 245901000, 219000245, 210359000, 231860000, 244890472, 210071000, 219029722, 245258000, 257970000, 219543000, 244615184, 257306000, 209974000, 244831000, 209531000, 257221000, 231613000, 211286440, 219281000, 257358000, 255805672, 211870240, 241798000, 209982000, 246754000, 255915683, 219003217, 232053416, 244678000, 230696000, 236385000, 257314000, 219019936, 210056000, 246724000, 255802940, 219775000, 230719000, 219581000, 245281000, 215698000, 257207000, 255735000, 212198000, 245250000, 244110823, 215933000, 257840000, 255803560, 245909000, 211138490, 257339000, 219022047, 248789000, 231025000, 255915583, 230631000, 246371000, 230351000, 245918000, 230357000, 258430000, 219003966, 230688000, 209114000, 229697000, 219019067, 230643000, 244267000, 244830818, 220600000, 230709000, 244271000, 246179000, 255806151, 219011283, 232025009, 210731000, 255806196, 248483000, 255915748, 244321000, 257737000, 209318000, 230707000, 215908000, 244790715, 246265000, 218858000, 244901000, 219164000, 246110000, 209536000, 211718360, 210553000, 255769000, 240168000, 215211000, 220464000, 245313000, 258059000, 212613000, 255915584, 246320000, 236111791, 210382000, 258006930, 244017000, 245029000, 200000000, 255915598, 228468800, 255802950, 255801580, 228467900, 211833390, 218085000, 244554000, 246996000, 257076220, 248221000, 209535000, 258152000, 255915582, 210552000, 246443000, 209239000, 231123000, 211202400, 255805653, 244870420, 245871000, 255802840, 238265000, 218615000, 235500000, 232031538]\n",
      "Val ships   (45): [257859000, 255806328, 219004907, 255806471, 218816000, 212801000, 246970000, 244768000, 212542000, 231355000, 255805899, 244632000, 210336000, 255806303, 255806024, 219027995, 212530000, 248020000, 256717000, 212882000, 220253000, 255806364, 256336000, 244870189, 212797000, 244140468, 246669000, 250005981, 209151000, 230039000, 215489000, 235098061, 209056000, 255806476, 229673000, 215114000, 244870428, 219026522, 248886000, 211686000, 256591000, 209356000, 245257000, 247018700, 256959000]\n",
      "Test ships  (56): [230617000, 219551000, 219031429, 232035207, 215253000, 219355000, 209415000, 209621000, 218030270, 219329000, 232049178, 210046000, 244997000, 219007705, 258009780, 235112981, 255915951, 255806310, 257374000, 218805000, 255915668, 215685000, 230091950, 215195000, 256617000, 245286000, 256683000, 246625000, 209903000, 249934000, 246539000, 255805589, 257071710, 245241000, 218005000, 212900000, 255814000, 256956000, 210155000, 244850116, 246670000, 257076850, 244335000, 232046826, 244575000, 228468600, 219309000, 255802570, 231048000, 258648000, 210144000, 246606000, 219007671, 230042000, 257964000, 229476000]\n",
      "============================\n",
      "\n",
      "Dataset scaling done. No leakage.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 21760, 6840, 4304, 23700) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimitrioud\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimitrioud\\miniforge3\\envs\\dtu02452\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m mmsi = \u001b[32m220464000\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Uncomment the line below to train the corrected model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- Evaluation ---\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating full trajectory for MMSI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmmsi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    112\u001b[39m total_rmse_lon = \u001b[32m0\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# The inner loop iterates through all batches (xb, yb) in the training data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# clears gradients from the previous step\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimitrioud\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimitrioud\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimitrioud\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1454\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1455\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1456\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimitrioud\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1298\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1297\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1299\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 21760, 6840, 4304, 23700) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        print(df.head())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {CSV_PATH}. Cannot proceed with training or evaluation.\")\n",
    "        \n",
    "    # The MMSI to evaluate. Default is 220464000\n",
    "    mmsi = 220464000 \n",
    "    \n",
    "    # Uncomment the line below to train the corrected model\n",
    "    train_model()\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(f\"\\nEvaluating full trajectory for MMSI: {mmsi}\")\n",
    "    actual, predicted = evaluate_full_ship(mmsi)\n",
    "\n",
    "    if len(predicted) > 0 and len(actual) > 0:\n",
    "        metrics = evaluate_metrics(predicted, actual)\n",
    "        \n",
    "        # --- Visualization ---\n",
    "        visualize_full_ship_route(actual, predicted, mmsi)\n",
    "        print(f\"Evaluation of ship with mmsi{mmsi} finished\")\n",
    "    else:\n",
    "        print(\"Skipping evaluation and visualization due to missing data or model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
