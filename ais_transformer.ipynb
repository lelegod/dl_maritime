{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1923d9",
   "metadata": {},
   "source": [
    "# AIS Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304e4b6",
   "metadata": {},
   "source": [
    "___\n",
    "Setup and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "id": "43d1426e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:46:00.739729Z",
     "start_time": "2025-11-29T18:46:00.733081Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "\n",
    "# NOTE: The 'utils.plot_ship_trajectory' function requires an external 'utils.py' file.\n",
    "# from utils import plot_ship_trajectory \n",
    "\n",
    "# Global Constants\n",
    "WINDOW = 20\n",
    "HORIZON = 1\n",
    "STEP = 1\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "LR = 1e-4\n",
    "DEVICE = \"cpu\"  # set to \"cuda\" if GPU\n",
    "\n",
    "CSV_PATH = \"data/ais_data_5min_clean.csv\"\n",
    "\n",
    "# Physics-aware features:\n",
    "FEATURES = [\n",
    "    \"Latitude\",\n",
    "    \"Longtitude\",\n",
    "    \"SOG\",\n",
    "    \"COG\",\n",
    "    \"vx\",\n",
    "    \"vy\",\n",
    "    \"dSOG\",\n",
    "    \"dCOG\"\n",
    "]\n",
    "\n",
    "TARGET_DIM = len(FEATURES)\n",
    "\n",
    "# For Visualization\n",
    "import folium"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "6c56c2e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "___\n",
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "68d7dfce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:46:04.706589Z",
     "start_time": "2025-11-29T18:46:04.693362Z"
    }
   },
   "source": [
    "\n",
    "# Mean absolute Error for Latitude and Longitude\n",
    "def compute_mae(pred, true):\n",
    "    if not isinstance(pred, np.ndarray):\n",
    "        # Detaches the tensor from the computation graph and stops tracking for gradients\n",
    "        pred = pred.detach().cpu().numpy().astype(np.float32)\n",
    "    if not isinstance(true, np.ndarray):\n",
    "        true = true.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # This line performs element-wise subtraction\n",
    "    err = pred - true\n",
    "\n",
    "    mae_lat = np.mean(np.abs(err[..., 0]))\n",
    "    mae_lon = np.mean(np.abs(err[..., 1]))\n",
    "\n",
    "    return mae_lat, mae_lon\n",
    "\n",
    "# Root Mean Square Error for Latitude and Longitude\n",
    "def compute_rmse(pred, true):\n",
    "    if not isinstance(pred, np.ndarray):\n",
    "        pred = pred.detach().cpu().numpy().astype(np.float32)\n",
    "    if not isinstance(true, np.ndarray):\n",
    "        true = true.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    err = pred - true\n",
    "\n",
    "    rmse_lat = np.sqrt(np.mean(err[..., 0] ** 2))\n",
    "    rmse_lon = np.sqrt(np.mean(err[..., 1] ** 2))\n",
    "\n",
    "    return rmse_lat, rmse_lon\n",
    "\n",
    "# Calculates the distance between two geographical points using the Haversine formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    Calculates the great-circle distance between two points (lat1, lon1)\n",
    "    and (lat2, lon2) using the Haversine formula\n",
    "    '''\n",
    "    R = 6371000.0\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat / 2) ** 2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Apply the haversine distance to every predicted/true coordinate pair\n",
    "def compute_haversine_metrics(pred_coords, true_coords):\n",
    "    distances = []\n",
    "    n = min(len(pred_coords), len(true_coords))\n",
    "\n",
    "    for i in range(n):\n",
    "        p = pred_coords[i]\n",
    "        t = true_coords[i]\n",
    "        d = haversine(p[0], p[1], t[0], t[1])\n",
    "        distances.append(d)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    # Calculate Mean Haversine Distance and Haversine RMSE\n",
    "    mean_haversine = distances.mean()\n",
    "    rmse_haversine = np.sqrt((distances ** 2).mean())\n",
    "    return mean_haversine, rmse_haversine\n",
    "\n",
    "# Mean Absolute Error between the predicted speed and the true speed of a vessel\n",
    "def compute_speed_error(pred_coords, true_coords, seconds = 60.0):\n",
    "    speeds_pred = []\n",
    "    speeds_true = []\n",
    "\n",
    "    n = min(len(pred_coords) - 1, len(true_coords) - 1)\n",
    "\n",
    "    for i in range(n):\n",
    "        dp = haversine(pred_coords[i][0], pred_coords[i][1],\n",
    "                       pred_coords[i + 1][0], pred_coords[i + 1][1])\n",
    "        dt = haversine(true_coords[i][0], true_coords[i][1],\n",
    "                       true_coords[i + 1][0], true_coords[i + 1][1])\n",
    "        # Speed is calculated as Distance / Time\n",
    "        speeds_pred.append(dp / seconds)\n",
    "        speeds_true.append(dt / seconds)\n",
    "    # Calculates the absolute difference between the predicted and true speeds for all intervals\n",
    "    return np.mean(np.abs(np.array(speeds_pred) - np.array(speeds_true)))\n",
    "\n",
    "'''\n",
    "Quantifies the prediction error in how sharply the vessel turns by comparing the predicted rate\n",
    "of change of the course (Course Over Ground - COG) against the true rate of change\n",
    "'''\n",
    "def compute_turn_rate_error(pred_coords, true_coords):\n",
    "    # calculates COG between successive points based on the change\n",
    "    # in latitude (dlat) and longitude (dlon)\n",
    "    def compute_cog(lat, lon):\n",
    "        dlat = np.diff(lat)\n",
    "        dlon = np.diff(lon)\n",
    "        return np.degrees(np.arctan2(dlon, dlat))\n",
    "\n",
    "    n = min(len(pred_coords), len(true_coords))\n",
    "    pred = pred_coords[:n]\n",
    "    true = true_coords[:n]\n",
    "\n",
    "    pred_cog = compute_cog(pred[:, 0], pred[:, 1])\n",
    "    true_cog = compute_cog(true[:, 0], true[:, 1])\n",
    "\n",
    "    # Ensure cogs have at least two points to compute turn rate\n",
    "    if len(pred_cog) < 2 or len(true_cog) < 2:\n",
    "        return 0.0  # Return 0 if not enough data for turn rate\n",
    "\n",
    "    # The turn rate is the difference between two successive COG values\n",
    "    # Represents the change in heading\n",
    "    pred_turn = np.diff(pred_cog)\n",
    "    true_turn = np.diff(true_cog)\n",
    "\n",
    "    n2 = min(len(pred_turn), len(true_turn))\n",
    "    return np.mean(np.abs(pred_turn[:n2] - true_turn[:n2]))\n",
    "\n",
    "# Presenting all trajectory evaluation metrics\n",
    "def evaluate_metrics(pred_coords, actual_coords):\n",
    "    n = min(len(pred_coords), len(actual_coords))\n",
    "    pred = pred_coords[:n]\n",
    "    true = actual_coords[:n]\n",
    "    mae_lat, mae_lon = compute_mae(pred, true)\n",
    "    mean_hav, rmse_hav = compute_haversine_metrics(pred, true)\n",
    "    speed_err = compute_speed_error(pred, true, seconds = 300)\n",
    "    turn_err = compute_turn_rate_error(pred, true)\n",
    "\n",
    "    print(\"\\n===== METRICS =====\")\n",
    "    print(f\"MAE Latitude:     {mae_lat:.6f}\")\n",
    "    print(f\"MAE Longitude:    {mae_lon:.6f}\")\n",
    "    print(f\"Mean Haversine:   {mean_hav:.3f} m\")\n",
    "    print(f\"Haversine RMSE:   {rmse_hav:.3f} m\")\n",
    "    print(f\"Speed Error:      {speed_err:.4f} m/s\")\n",
    "    print(f\"Turn-Rate Error:  {turn_err:.4f} deg\")\n",
    "    print(\"===================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"mae_lat\": mae_lat,\n",
    "        \"mae_lon\": mae_lon,\n",
    "        \"mean_haversine\": mean_hav,\n",
    "        \"rmse_haversine\": rmse_hav,\n",
    "        \"speed_error\": speed_err,\n",
    "        \"turn_rate_error\": turn_err\n",
    "    }\n",
    "\n",
    "# Calculate velocity components and deltas\n",
    "def compute_physics_features(df):\n",
    "    \"\"\"Add vx, vy, dSOG, dCOG features per segment.\"\"\"\n",
    "    dfs = []\n",
    "\n",
    "    for (mmsi, seg), g in df.groupby([\"MMSI\", \"Segment\"], observed=True):\n",
    "        g = g.sort_values(\"Timestamp\").copy()\n",
    "\n",
    "        # Convert COG to radians\n",
    "        rad = np.radians(g[\"COG\"].values)\n",
    "\n",
    "        '''\n",
    "        Calculates the horizontal (vx) and vertical (vy) velocity components from the ship's\n",
    "        Speed Over Ground (SOG) and Course Over Ground (COG)\n",
    "        '''\n",
    "        g[\"vx\"] = g[\"SOG\"] * np.cos(rad)\n",
    "        g[\"vy\"] = g[\"SOG\"] * np.sin(rad)\n",
    "\n",
    "        # Difference (delta) in SOG between the current and previous time step\n",
    "        g[\"dSOG\"] = g[\"SOG\"].diff().fillna(0)\n",
    "        # Handle 360-degree wrap-around for COG difference\n",
    "        dCOG_raw = g[\"COG\"].diff()\n",
    "        dCOG_wrapped = (dCOG_raw + 540) % 360 - 180\n",
    "        g[\"dCOG\"] = dCOG_wrapped.fillna(0)\n",
    "\n",
    "        dfs.append(g)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# PyTorch Haversine (for autoregressive rollout)\n",
    "def haversine_torch(lat1, lon1, lat2, lon2):\n",
    "    # Earth radius in meters\n",
    "    R = 6371000.0\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Convert degrees to radians using torch.deg2rad\n",
    "    dlat_rad = torch.deg2rad(dlat)\n",
    "    dlon_rad = torch.deg2rad(dlon)\n",
    "    lat1_rad = torch.deg2rad(lat1)\n",
    "    lat2_rad = torch.deg2rad(lat2)\n",
    "\n",
    "    # Haversine formula using torch functions\n",
    "    a = torch.sin(dlat_rad / 2) ** 2 + \\\n",
    "        torch.cos(lat1_rad) * torch.cos(lat2_rad) * torch.sin(dlon_rad / 2) ** 2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
    "\n",
    "    return R * c"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "599ebf50",
   "metadata": {},
   "source": [
    "___\n",
    "Model Definition and Training, Validation and Testing Phases"
   ]
  },
  {
   "cell_type": "code",
   "id": "04e96b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:46:08.680357Z",
     "start_time": "2025-11-29T18:46:08.662529Z"
    }
   },
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "'''\n",
    "Define Transformer Encoder\n",
    "Attention mechanism allows the model to learn that predicting the next turn might rely heavily on the last few steps of dCOG (change in course) and COG (current course),\n",
    "while predicting speed might rely more on the current SOG (speed over ground) and the dSOG history\n",
    "'''\n",
    "class AISTransformer(nn.Module):\n",
    "    '''\n",
    "    Embedding size -> embed_Size\n",
    "    Attention heads allowing the model to focus on different parts of the input window simultaneously -> num_heads\n",
    "    Feed-Forward network Dimension (2 layers MLP) -> ff_dim\n",
    "    '''\n",
    "    def __init__(self, input_size, output_size, embed_size=64, num_heads=2,\n",
    "                 num_layers=2, ff_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, embed_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.encoder.gradient_checkpointing = True\n",
    "\n",
    "        # predict 1-step delta at a time (autoregressive decoder)\n",
    "        self.decoder = nn.Linear(embed_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode the window → take last timestep → decode to prediction.\n",
    "        \"\"\"\n",
    "        z = self.embedding(x)\n",
    "        z = self.encoder(z)\n",
    "\n",
    "        # last hidden state (batch, embed_size)\n",
    "        last_vec = z[:, -1, :]\n",
    "\n",
    "        # final prediction (batch, TARGET_DIM)\n",
    "        out = self.decoder(last_vec)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "'''\n",
    "Implements a ship-based (or group-based) data split, ensuring that all trajectory windows belonging to a specific ship (MMSI)\n",
    "are placed entirely into one of the three sets: Training, Validation, or Testing\n",
    "'''\n",
    "def ship_based_split(meta, train_ratio=0.64, val_ratio=0.16, test_ratio=0.20, seed=42):\n",
    "    \"\"\"\n",
    "    Split ships into:\n",
    "      - Train: 64%\n",
    "      - Validation: 16%\n",
    "      - Test: 20%\n",
    "    \"\"\"\n",
    "    if not np.isclose(train_ratio + val_ratio + test_ratio, 1.0):\n",
    "        raise ValueError(\"train_ratio + val_ratio + test_ratio must sum to 1.0\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ships = np.array(meta[\"mmsi\"].unique())\n",
    "    np.random.shuffle(ships)\n",
    "\n",
    "    total = len(ships)\n",
    "\n",
    "    # Compute boundaries\n",
    "    cutoff_train = int(total * train_ratio)\n",
    "    cutoff_val = int(total * (train_ratio + val_ratio))\n",
    "\n",
    "    # Splits\n",
    "    train_ships = ships[:cutoff_train]\n",
    "    val_ships = ships[cutoff_train:cutoff_val]\n",
    "    test_ships = ships[cutoff_val:]\n",
    "\n",
    "    split_dict = {\n",
    "        \"train_ships\": train_ships.tolist(),\n",
    "        \"val_ships\": val_ships.tolist(),\n",
    "        \"test_ships\": test_ships.tolist(),\n",
    "        \"ratios\": {\n",
    "            \"train_ratio\": train_ratio,\n",
    "            \"val_ratio\": val_ratio,\n",
    "            \"test_ratio\": test_ratio\n",
    "        },\n",
    "        \"total_unique_ships\": total\n",
    "    }\n",
    "\n",
    "    with open(\"ship_splits.json\", \"w\") as f:\n",
    "        json.dump(split_dict, f, indent=4)\n",
    "\n",
    "    print(\"\\n============================\")\n",
    "    print(\" Ship-based Split Summary \")\n",
    "    print(\"============================\")\n",
    "    print(f\"Total unique ships: {total}\")\n",
    "    print(f\"Train ships ({len(train_ships)}): {train_ships.tolist()}\")\n",
    "    print(f\"Val ships   ({len(val_ships)}): {val_ships.tolist()}\")\n",
    "    print(f\"Test ships  ({len(test_ships)}): {test_ships.tolist()}\")\n",
    "    print(\"============================\\n\")\n",
    "\n",
    "    train_idx = meta.index[meta[\"mmsi\"].isin(train_ships)].to_numpy()\n",
    "    val_idx = meta.index[meta[\"mmsi\"].isin(val_ships)].to_numpy()\n",
    "    test_idx = meta.index[meta[\"mmsi\"].isin(test_ships)].to_numpy()\n",
    "\n",
    "    return train_idx, val_idx, test_idx, train_ships, val_ships, test_ships\n",
    "\n",
    "# Generate the dataset\n",
    "def build_dataset():\n",
    "    print(\"Loading CSV...\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = compute_physics_features(df)\n",
    "\n",
    "    raw_windows, raw_targets, meta_rows = [], [], []\n",
    "\n",
    "    print(\"Building raw (unscaled) windows...\")\n",
    "    for (mmsi, seg), g in df.groupby([\"MMSI\", \"Segment\"], observed=True):\n",
    "        g = g.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "        if len(g) < WINDOW + HORIZON + 1:\n",
    "            continue\n",
    "\n",
    "        feat = g[FEATURES].values\n",
    "\n",
    "        for i in range(0, len(g) - WINDOW - HORIZON, STEP):\n",
    "\n",
    "            X_window_raw = feat[i:i + WINDOW]              # (WINDOW, 8)\n",
    "            future_raw  = feat[i + WINDOW : i + WINDOW + HORIZON]  # (HORIZON, 8)\n",
    "\n",
    "            if future_raw.shape != (HORIZON, TARGET_DIM):\n",
    "                continue\n",
    "\n",
    "            raw_windows.append(X_window_raw)\n",
    "            raw_targets.append(future_raw)\n",
    "\n",
    "            meta_rows.append({\n",
    "                \"mmsi\": mmsi,\n",
    "                \"segment\": seg,\n",
    "                \"last_lat\": g[\"Latitude\"][i + WINDOW - 1],\n",
    "                \"last_lon\": g[\"Longtitude\"][i + WINDOW - 1]\n",
    "            })\n",
    "\n",
    "    raw_X = np.array(raw_windows, dtype=np.float32)          # (N, W, 8)\n",
    "    raw_y = np.array(raw_targets, dtype=np.float32)          # (N, H, 8)\n",
    "    meta  = pd.DataFrame(meta_rows)\n",
    "\n",
    "    print(f\"RAW dataset: X={raw_X.shape}, y={raw_y.shape}, meta={meta.shape}\")\n",
    "\n",
    "    # Ship split\n",
    "    train_idx, val_idx, test_idx, *_ = ship_based_split(meta)\n",
    "\n",
    "    # Fit scalers on training part only\n",
    "    train_feat    = raw_X[train_idx].reshape(-1, TARGET_DIM)\n",
    "    train_targets = raw_y[train_idx].reshape(-1, TARGET_DIM)\n",
    "\n",
    "    global_scaler_X = StandardScaler().fit(train_feat)\n",
    "    global_scaler_y = StandardScaler().fit(train_targets)\n",
    "\n",
    "    # Scale full dataset\n",
    "    X_scaled = global_scaler_X.transform(raw_X.reshape(-1, TARGET_DIM)) \\\n",
    "                                  .reshape(raw_X.shape).astype(np.float32)\n",
    "\n",
    "    y_scaled = global_scaler_y.transform(raw_y.reshape(-1, TARGET_DIM)) \\\n",
    "                                  .reshape(raw_y.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "    print(\"Dataset scaling done.\")\n",
    "\n",
    "    return (\n",
    "    X_scaled, y_scaled, meta,\n",
    "    train_idx, val_idx, test_idx,\n",
    "    global_scaler_X, global_scaler_y\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "dc123a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:46:13.012429Z",
     "start_time": "2025-11-29T18:46:13.002201Z"
    }
   },
   "source": [
    "# Simulates the future trajectory of a ship over a given horizon by using its own predictions as new inputs for the next time step\n",
    "def autoregressive_rollout(model, window, scaler_X, scaler_y,\n",
    "                           horizon=HORIZON, seconds=60.0):\n",
    "    device = window.device\n",
    "    B = window.size(0)\n",
    "\n",
    "    X_mean = torch.tensor(scaler_X.mean_, device=device)\n",
    "    X_std  = torch.tensor(scaler_X.scale_, device=device)\n",
    "    y_mean = torch.tensor(scaler_y.mean_, device=device)\n",
    "    y_std  = torch.tensor(scaler_y.scale_, device=device)\n",
    "\n",
    "    current = window.clone()  # (B, W, 8)\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(horizon):\n",
    "        pred_norm = model(current)                # (B, 8)\n",
    "        pred_raw = pred_norm * y_std + y_mean     # unnormalize for student forcing\n",
    "        preds.append(pred_norm.unsqueeze(1))      # (B, 1, 8)\n",
    "\n",
    "        # Normalize predicted raw output to X space for next step\n",
    "        pred_norm_for_X = (pred_raw - X_mean) / X_std\n",
    "\n",
    "        # Shift window left and insert new normalized feature vector\n",
    "        current = torch.roll(current, shifts=-1, dims=1)\n",
    "        current[:, -1, :] = pred_norm_for_X\n",
    "\n",
    "    preds = torch.cat(preds, dim=1)  # (B, H, 8)\n",
    "    return preds\n",
    "\n",
    "def train_model():\n",
    "    (\n",
    "        X, y, meta,\n",
    "        train_idx, val_idx, test_idx,\n",
    "        global_scaler_X, global_scaler_y\n",
    "    ) = build_dataset()\n",
    "\n",
    "    train_ds = TrajectoryDataset(X[train_idx], y[train_idx])\n",
    "    val_ds   = TrajectoryDataset(X[val_idx],   y[val_idx])\n",
    "    test_ds  = TrajectoryDataset(X[test_idx], y[test_idx])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    model = AISTransformer(\n",
    "        input_size=len(FEATURES),      # 8 inputs\n",
    "        output_size=len(FEATURES)      # 8 outputs\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    # The Huber Loss is selected. This loss is less sensitive to extreme outliers\n",
    "    # compared to Mean Squared Error (MSE), making training more robust for noisy data\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_mae_lat = 0\n",
    "        total_mae_lon = 0\n",
    "        total_rmse_lat = 0\n",
    "        total_rmse_lon = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # The model generates predictions by running the autoregressive rollout over the input window\n",
    "            pred_scaled = autoregressive_rollout(\n",
    "                model, xb,\n",
    "                global_scaler_X, global_scaler_y,\n",
    "                horizon=HORIZON,\n",
    "                seconds=300\n",
    "            )\n",
    "\n",
    "            pred_t = pred_scaled.float()\n",
    "\n",
    "            loss = criterion(pred_t, yb)\n",
    "            mae_lat, mae_lon = compute_mae(pred_t, yb)\n",
    "            rmse_lat, rmse_lon = compute_rmse(pred_t, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae_lat  += mae_lat\n",
    "            total_mae_lon  += mae_lon\n",
    "            total_rmse_lat += rmse_lat\n",
    "            total_rmse_lon += rmse_lon\n",
    "\n",
    "        n_batches = len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {total_loss / n_batches:.6f}\")\n",
    "        print(f\"Train MAE(lat): {total_mae_lat / n_batches:.6f}, \"\n",
    "              f\"MAE(lon): {total_mae_lon / n_batches:.6f}\")\n",
    "\n",
    "        # disable training features and save memory/time,\n",
    "        # as gradients are not needed for evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "                pred_scaled = autoregressive_rollout(\n",
    "                    model, xb,\n",
    "                    global_scaler_X, global_scaler_y,\n",
    "                    horizon=HORIZON,\n",
    "                    seconds=300\n",
    "                )\n",
    "\n",
    "                val_loss += criterion(pred_scaled.float(), yb).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # If the calculated val_loss is significantly better than best_val_loss, the model's state\n",
    "        # (weights and context) is saved to \"ais_model_best.pth\", and patience is reset\n",
    "        if val_loss < best_val_loss - MIN_DELTA:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"meta\": meta,\n",
    "                \"train_idx\": train_idx,\n",
    "                \"val_idx\": val_idx,\n",
    "                \"test_idx\": test_idx,\n",
    "                \"global_scaler_X\": global_scaler_X,\n",
    "                \"global_scaler_y\": global_scaler_y,\n",
    "                \"X\": X, \"y\": y\n",
    "            }, \"ais_model_best.pth\")\n",
    "\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Load best model for final testing\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Loading best model for test evaluation...\")\n",
    "        best_ckpt = torch.load(\"ais_model_best.pth\", map_location=DEVICE, weights_only=False)\n",
    "        model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred_scaled = autoregressive_rollout(\n",
    "                model, xb,\n",
    "                global_scaler_X, global_scaler_y,\n",
    "                horizon=HORIZON,\n",
    "                seconds=300\n",
    "            )\n",
    "            test_loss += criterion(pred_scaled.float(), yb).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    # Final save of the model (could be the best or the last epoch)\n",
    "    final_save_path = \"ais_model_final.pth\"\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"meta\": meta,\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"global_scaler_X\": global_scaler_X,\n",
    "        \"global_scaler_y\": global_scaler_y,\n",
    "        \"X\": X, \"y\": y\n",
    "    }, final_save_path)\n",
    "\n",
    "    print(f\"Model state saved to {final_save_path}.\")"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "b4ac2779",
   "metadata": {},
   "source": [
    "___\n",
    "Prediction of a ship"
   ]
  },
  {
   "cell_type": "code",
   "id": "de2288da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T19:02:47.655567Z",
     "start_time": "2025-11-29T19:02:47.641063Z"
    }
   },
   "source": [
    "# Performs autoregressive prediction for a single starting window (idx), incorporating the model's physics-aware features at every stage\n",
    "def predict_sample_autoregressive(idx, seconds=60.0):\n",
    "    \"\"\"Autoregressive prediction using ALL physics-aware features.\"\"\"\n",
    "    try:\n",
    "        ckpt = torch.load(\"ais_model_final.pth\", map_location=DEVICE, weights_only=False)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'ais_model_final.pth' not found. Run train_model() first.\")\n",
    "        return np.array([])\n",
    "\n",
    "    meta = ckpt[\"meta\"]\n",
    "    X = ckpt[\"X\"]\n",
    "    \n",
    "    # Use global scalers from the checkpoint\n",
    "    scaler_X, scaler_y = ckpt[\"global_scaler_X\"], ckpt[\"global_scaler_y\"]\n",
    "\n",
    "    # Load the model\n",
    "    model = AISTransformer(input_size=len(FEATURES)).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Initial scaled window (already float32 from build_dataset fix)\n",
    "    window = X[idx].copy()\n",
    "\n",
    "    # Last observed actual coordinates\n",
    "    prev_lat = float(meta.loc[idx, \"last_lat\"])\n",
    "    prev_lon = float(meta.loc[idx, \"last_lon\"])\n",
    "\n",
    "    # Extract last feature values (scaled → inverse scaled)\n",
    "    last_feat_scaled = window[-1]\n",
    "    last_feat = scaler_X.inverse_transform(last_feat_scaled.reshape(1, -1))[0]\n",
    "\n",
    "    prev_SOG = last_feat[2]\n",
    "    prev_COG = last_feat[3]\n",
    "\n",
    "    pred_coords = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # The function loops HORIZON number of times, generating one future step in each iteration\n",
    "        for _ in range(HORIZON):\n",
    "            # Model input: ensure it is float32\n",
    "            x = torch.tensor(window[np.newaxis, ...], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "            # Predict Δlat, Δlon (scaled)\n",
    "            z = model(x)\n",
    "            # Take the first (and only) batch item, move to CPU, convert to numpy\n",
    "            delta_scaled = model.decoder(z)[0].cpu().numpy()\n",
    "\n",
    "            # Inverse-transform deltas\n",
    "            delta_real = scaler_y.inverse_transform(delta_scaled.reshape(1, -1))[0]\n",
    "            dlat, dlon = delta_real\n",
    "\n",
    "            # Update coordinates (now floats/scalars)\n",
    "            cur_lat = prev_lat + dlat\n",
    "            cur_lon = prev_lon + dlon\n",
    "            pred_coords.append([cur_lat, cur_lon])\n",
    "\n",
    "            # --- Physics Update for Autoregressive Step (using torch for haversine) ---\n",
    "            p_lat = torch.tensor(prev_lat, dtype=torch.float32, device=DEVICE)\n",
    "            p_lon = torch.tensor(prev_lon, dtype=torch.float32, device=DEVICE)\n",
    "            c_lat = torch.tensor(cur_lat, dtype=torch.float32, device=DEVICE)\n",
    "            c_lon = torch.tensor(cur_lon, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            # Calculates the physical distance (dist_m) between the previous and current predicted points\n",
    "            dist_m_tensor = haversine_torch(p_lat, p_lon, c_lat, c_lon)\n",
    "            dist_m = dist_m_tensor.item()\n",
    "            # -------------------------------------------------------------------------\n",
    "\n",
    "            SOG = dist_m /seconds\n",
    "\n",
    "            dY = cur_lat - prev_lat\n",
    "            dX = cur_lon - prev_lon\n",
    "            COG = (np.degrees(np.arctan2(dX, dY)) + 360) % 360\n",
    "\n",
    "            vx = SOG * np.cos(np.radians(COG))\n",
    "            vy = SOG * np.sin(np.radians(COG))\n",
    "\n",
    "            dSOG = SOG - prev_SOG\n",
    "            dCOG = (COG - prev_COG + 540) % 360 - 180  # normalize turn rate\n",
    "\n",
    "            # New raw feature row (unscaled)\n",
    "            new_row_array = np.array([[\n",
    "                cur_lat, cur_lon, SOG, COG, vx, vy, dSOG, dCOG\n",
    "            ]], dtype=np.float32)\n",
    "\n",
    "            # Scale new row for next window\n",
    "            new_scaled = scaler_X.transform(new_row_array)[0]\n",
    "\n",
    "            # Slide window\n",
    "            window = np.vstack([window[1:], new_scaled.astype(np.float32)])\n",
    "\n",
    "            # Update prev state\n",
    "            # The predicted position and physics features are updated\n",
    "            prev_lat, prev_lon = cur_lat, cur_lon\n",
    "            prev_SOG, prev_COG = SOG, COG\n",
    "\n",
    "    return np.array(pred_coords)\n",
    "\n",
    "\n",
    "# creates an interactive HTML map using the folium library to visually compare the entire actual trajectory against the concatenation of all predicted steps\n",
    "def visualize_full_ship_route(full_actual, pred_future, mmsi, save_path=None):\n",
    "    if len(full_actual) == 0:\n",
    "        print(\"No actual data to plot.\")\n",
    "        return\n",
    "\n",
    "    actual_coords = [[row[0], row[1]] for row in full_actual]\n",
    "    pred_coords = [[row[0], row[1]] for row in pred_future] if len(pred_future) > 0 else []\n",
    "\n",
    "    # ---- INIT MAP ----\n",
    "    m = folium.Map(location=actual_coords[0], zoom_start=9)\n",
    "\n",
    "    # ---- FULL ACTUAL TRAJECTORY (blue) ----\n",
    "    folium.PolyLine(\n",
    "        locations=actual_coords,\n",
    "        color=\"blue\",\n",
    "        weight=4,\n",
    "        opacity=0.9,\n",
    "        tooltip=\"Actual Route (Full)\"\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Start & End markers\n",
    "    folium.Marker(\n",
    "        location=actual_coords[0],\n",
    "        popup=\"Start (Actual)\",\n",
    "        icon=folium.Icon(color=\"green\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=actual_coords[-1],\n",
    "        popup=\"End (Actual)\",\n",
    "        icon=folium.Icon(color=\"blue\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    # ---- PREDICTED FUTURE ROUTE (red) ----\n",
    "    if len(pred_coords) > 0:\n",
    "        folium.PolyLine(\n",
    "            locations=pred_coords,\n",
    "            color=\"red\",\n",
    "            weight=4,\n",
    "            opacity=0.9,\n",
    "            tooltip=\"Predicted Route\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    # ---- SAVE ----\n",
    "    if save_path is None:\n",
    "        save_path = f\"ship_{mmsi}_actual_vs_predicted.html\"\n",
    "\n",
    "    m.save(save_path)\n",
    "    print(f\"Map saved to {save_path}\")\n",
    "\n",
    "def evaluate_full_trajectory_autoregressive(mmsi, model_path, csv_path, device, eval_horizon):\n",
    "    print(f\"Loading model from {model_path} for MMSI {mmsi}...\")\n",
    "    try:\n",
    "        ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Model file not found. Ensure 'train_model()' was run successfully.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    model = AISTransformer(\n",
    "        input_size=len(FEATURES),\n",
    "        output_size=len(FEATURES)\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    scaler_X = ckpt[\"global_scaler_X\"]\n",
    "    scaler_y = ckpt[\"global_scaler_y\"]\n",
    "\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = compute_physics_features(df)\n",
    "    df_ship = df[df[\"MMSI\"] == mmsi].sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    feat = df_ship[FEATURES].values.astype(np.float32)\n",
    "\n",
    "    if len(feat) < WINDOW + eval_horizon:\n",
    "        print(f\"Error: Trajectory for MMSI {mmsi} is too short ({len(feat)} points) for WINDOW={WINDOW} + EVAL_HORIZON={eval_horizon}.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    X_raw = feat[:WINDOW]\n",
    "    actual_full_segment = feat[WINDOW:WINDOW + eval_horizon, :2]\n",
    "\n",
    "    # --- 3. Perform Autoregressive Prediction ---\n",
    "    X_norm = (X_raw - scaler_X.mean_) / scaler_X.scale_\n",
    "    window = torch.tensor(X_norm, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    pred_scaled = autoregressive_rollout(\n",
    "        model, window, scaler_X, scaler_y, horizon=eval_horizon, seconds=300\n",
    "    )\n",
    "\n",
    "    # The rollout function returns scaled predictions.\n",
    "    # Unnormalize the predictions for the metrics (which use raw coordinates)\n",
    "    y_mean = torch.tensor(scaler_y.mean_, device=device)\n",
    "    y_std  = torch.tensor(scaler_y.scale_, device=device)\n",
    "    pred_raw_full_features = pred_scaled * y_std + y_mean\n",
    "\n",
    "    # Extract only the Latitude and Longitude (indices 0 and 1)\n",
    "    predicted_coords = pred_raw_full_features.detach().cpu().numpy()[0, :, :2]\n",
    "\n",
    "    # Return the actual ground truth for the test period and the predicted coordinates\n",
    "    return actual_full_segment, predicted_coords\n"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "837e6a65",
   "metadata": {},
   "source": [
    "___\n",
    "Execution of the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "c04769c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T19:27:52.578686Z",
     "start_time": "2025-11-29T19:27:51.832015Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        print(df.head())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {CSV_PATH}. Cannot proceed with training or evaluation.\")\n",
    "        \n",
    "    # MMSI to evaluate\n",
    "    #mmsi = 220464000\n",
    "    mmsi = 231025000\n",
    "    #train_model()\n",
    "    df_ship = df[df[\"MMSI\"] == mmsi].sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    if len(df_ship) <= WINDOW:\n",
    "         TEST_HORIZON = 0\n",
    "    else:\n",
    "         # Calculate the prediction horizon dynamically: Total available points - WINDOW size\n",
    "         TEST_HORIZON = len(df_ship) - WINDOW\n",
    "\n",
    "    \n",
    "    # --- Evaluation ---\n",
    "    print(f\"\\nEvaluating full trajectory for MMSI: {mmsi}\")\n",
    "\n",
    "    actual, predicted = evaluate_full_trajectory_autoregressive(\n",
    "        mmsi=mmsi,\n",
    "        model_path=\"ais_model_final.pth\",\n",
    "        csv_path=CSV_PATH,\n",
    "        device=DEVICE,\n",
    "        eval_horizon=TEST_HORIZON\n",
    "    )\n",
    "\n",
    "    if len(predicted) > 0 and len(actual) > 0:\n",
    "        metrics = evaluate_metrics(predicted, actual)\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_full_ship_route(actual, predicted, mmsi)\n",
    "        print(f\"Evaluation of ship with MMSI {mmsi} finished\")\n",
    "    else:\n",
    "        print(\"Skipping evaluation and visualization due to missing data or model.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MMSI  Segment            Timestamp       SOG   COG  Longtitude  \\\n",
      "0  200000000        1  2025-02-27 00:00:00  6.790661  86.8   11.623667   \n",
      "1  200000000        1  2025-02-27 00:05:00  6.790661  90.2   11.656255   \n",
      "2  200000000        1  2025-02-27 00:10:00  6.842105  90.7   11.688977   \n",
      "3  200000000        1  2025-02-27 00:15:00  6.790661  91.0   11.721940   \n",
      "4  200000000        1  2025-02-27 00:20:00  6.636328  96.9   11.754877   \n",
      "\n",
      "    Latitude   distance_m  speed_mps_track  \n",
      "0  56.123468     0.000000         0.000000  \n",
      "1  56.122908  2020.798694        33.679978  \n",
      "2  56.122620  2028.419709        33.806995  \n",
      "3  56.122575  2043.119472        34.051991  \n",
      "4  56.122122  2042.136349        34.035606  \n",
      "\n",
      "Evaluating full trajectory for MMSI: 231025000\n",
      "Loading model from ais_model_final.pth for MMSI 231025000...\n",
      "\n",
      "===== METRICS =====\n",
      "MAE Latitude:     0.063119\n",
      "MAE Longitude:    1.650340\n",
      "Mean Haversine:   104878.867 m\n",
      "Haversine RMSE:   112659.122 m\n",
      "Speed Error:      3.3916 m/s\n",
      "Turn-Rate Error:  1.6064 deg\n",
      "===================\n",
      "\n",
      "Map saved to ship_231025000_actual_vs_predicted.html\n",
      "Evaluation of ship with MMSI 231025000 finished\n"
     ]
    }
   ],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dtu02452)",
   "language": "python",
   "name": "dtu02452"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
