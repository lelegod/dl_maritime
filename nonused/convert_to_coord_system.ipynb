{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc1eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 with 300000 rows...\n",
      "Processing chunk 2 with 300000 rows...\n",
      "Processing chunk 3 with 300000 rows...\n",
      "Processing chunk 4 with 300000 rows...\n",
      "Processing chunk 5 with 300000 rows...\n",
      "Processing chunk 6 with 300000 rows...\n",
      "Processing chunk 7 with 300000 rows...\n",
      "Processing chunk 8 with 300000 rows...\n",
      "Processing chunk 9 with 300000 rows...\n",
      "Processing chunk 10 with 300000 rows...\n",
      "Processing chunk 11 with 300000 rows...\n",
      "Processing chunk 12 with 300000 rows...\n",
      "Processing chunk 13 with 300000 rows...\n",
      "Processing chunk 14 with 300000 rows...\n",
      "Processing chunk 15 with 300000 rows...\n",
      "Processing chunk 16 with 300000 rows...\n",
      "Processing chunk 17 with 300000 rows...\n",
      "Processing chunk 18 with 142595 rows...\n",
      "âœ… Done. Saved ordered columns to: ais_clean_XY.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "INPUT_CSV   = \"ais_combined.csv\"\n",
    "OUTPUT_CSV  = \"ais_clean_XY.csv\"\n",
    "CHUNKSIZE   = 300_000\n",
    "\n",
    "# Set your custom (0,0) origin in lat/lon (degrees)\n",
    "lat0, lon0  = 56.0, 10.0\n",
    "\n",
    "# If Segment is missing, we will create it by splitting on gaps > MAX_GAP (per MMSI)\n",
    "MAX_GAP = pd.Timedelta(\"15min\")\n",
    "\n",
    "# Column candidates to tolerate naming differences\n",
    "LAT_CANDS = [\"Latitude\", \"lat\", \"Lat\"]\n",
    "LON_CANDS = [\"Longitude\", \"Longtitude\", \"lon\", \"Lon\"]\n",
    "MMSI_CANDS = [\"MMSI\", \"mmsi\"]\n",
    "SOG_CANDS  = [\"SOG\", \"sog\"]\n",
    "COG_CANDS  = [\"COG\", \"cog\"]\n",
    "TS_CANDS   = [\"Timestamp\", \"timestamp\", \"time\", \"DateTime\", \"datetime\"]\n",
    "SEG_CANDS  = [\"Segment\", \"segment\", \"SegmentID\", \"segment_id\"]\n",
    "\n",
    "\n",
    "def find_col(cols, candidates, required=True):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    if required:\n",
    "        raise ValueError(f\"None of {candidates} found in columns: {list(cols)}\")\n",
    "    return None\n",
    "\n",
    "def ensure_timestamp_series(s):\n",
    "    # Parse to timezone-aware UTC; drop invalid\n",
    "    ts = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    return ts\n",
    "\n",
    "def make_segments_if_missing(df, ts_col, mmsi_col):\n",
    "    \"\"\"\n",
    "    Create 'Segment' per MMSI by splitting when time gaps > MAX_GAP.\n",
    "    Assumes df[ts_col] is datetime64[ns, UTC].\n",
    "    \"\"\"\n",
    "    # sort per MMSI + time\n",
    "    df = df.sort_values([mmsi_col, ts_col])\n",
    "    # compute per-MMSI time diffs\n",
    "    diffs = df.groupby(mmsi_col)[ts_col].diff()\n",
    "    seg_id = diffs.gt(MAX_GAP).groupby(df[mmsi_col]).cumsum().astype(\"int64\")\n",
    "    return seg_id\n",
    "\n",
    "\n",
    "to_laea = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3035\", always_xy=True)\n",
    "X0, Y0 = to_laea.transform(lon0, lat0)\n",
    "\n",
    "# Read header to identify columns\n",
    "head = pd.read_csv(INPUT_CSV, nrows=0)\n",
    "cols = head.columns\n",
    "\n",
    "lat_col  = find_col(cols, LAT_CANDS)\n",
    "lon_col  = find_col(cols, LON_CANDS)\n",
    "mmsi_col = find_col(cols, MMSI_CANDS)\n",
    "sog_col  = find_col(cols, SOG_CANDS, required=False)  # SOG may be missing\n",
    "cog_col  = find_col(cols, COG_CANDS, required=False)  # COG may be missing\n",
    "ts_col   = find_col(cols, TS_CANDS)\n",
    "seg_col  = find_col(cols, SEG_CANDS, required=False)\n",
    "\n",
    "first_write = True\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(INPUT_CSV, chunksize=CHUNKSIZE)):\n",
    "    print(f\"Processing chunk {i+1} with {len(chunk)} rows...\")\n",
    "\n",
    "    # Keep only rows with valid lat/lon\n",
    "    chunk[lat_col] = pd.to_numeric(chunk[lat_col], errors=\"coerce\")\n",
    "    chunk[lon_col] = pd.to_numeric(chunk[lon_col], errors=\"coerce\")\n",
    "    chunk = chunk.dropna(subset=[lat_col, lon_col])\n",
    "\n",
    "    # Timestamp handling\n",
    "    chunk[ts_col] = ensure_timestamp_series(chunk[ts_col])\n",
    "    chunk = chunk.dropna(subset=[ts_col])\n",
    "\n",
    "    # MMSI as string (keeps leading zeros if ever present)\n",
    "    chunk[mmsi_col] = chunk[mmsi_col].astype(str)\n",
    "\n",
    "    # If SOG/COG exist, ensure numeric; else create NaNs so columns exist\n",
    "    if sog_col is not None:\n",
    "        chunk[sog_col] = pd.to_numeric(chunk[sog_col], errors=\"coerce\")\n",
    "    else:\n",
    "        chunk[\"SOG\"] = np.nan\n",
    "        sog_col = \"SOG\"\n",
    "\n",
    "    if cog_col is not None:\n",
    "        chunk[cog_col] = pd.to_numeric(chunk[cog_col], errors=\"coerce\")\n",
    "    else:\n",
    "        chunk[\"COG\"] = np.nan\n",
    "        cog_col = \"COG\"\n",
    "\n",
    "    # Segment handling: keep existing, else create by gap rule\n",
    "    if seg_col is None:\n",
    "        # Build segments per MMSI on this chunk's scope\n",
    "        chunk[\"Segment\"] = make_segments_if_missing(chunk, ts_col, mmsi_col)\n",
    "        seg_col = \"Segment\"\n",
    "    else:\n",
    "        # Normalize name to 'Segment' for uniform output\n",
    "        chunk[\"Segment\"] = chunk[seg_col].astype(\"int64\", errors=\"ignore\")\n",
    "\n",
    "    # PROJECT to meters (single CRS) and RECENTER to your origin\n",
    "    X, Y = to_laea.transform(chunk[lon_col].values, chunk[lat_col].values)\n",
    "    X_local = X - X0\n",
    "    Y_local = Y - Y0\n",
    "\n",
    "    # Assemble output with the exact column order you want\n",
    "    out = pd.DataFrame({\n",
    "        \"MMSI\":      chunk[mmsi_col].values,\n",
    "        \"SOG\":       chunk[sog_col].values,\n",
    "        \"COG\":       chunk[cog_col].values,\n",
    "        \"X\":         X_local,\n",
    "        \"Y\":         Y_local,\n",
    "        \"Timestamp\": chunk[ts_col].dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),  # ISO-like string with UTC offset\n",
    "        \"Segment\":   chunk[\"Segment\"].values\n",
    "    })\n",
    "\n",
    "    # Write/append\n",
    "    out.to_csv(\n",
    "        OUTPUT_CSV,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=first_write,\n",
    "        float_format=\"%.3f\"\n",
    "    )\n",
    "    first_write = False\n",
    "\n",
    "print(f\" Done. Saved ordered columns to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa73b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457c58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
