{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318a3958",
   "metadata": {},
   "source": [
    "# Imports, read csv, functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff852b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_rnn import add_xy_and_deltas\n",
    "from utils_rnn import split_train_val_test\n",
    "from utils_rnn import create_sequences\n",
    "from utils_rnn import autoregressive_predict\n",
    "from utils_rnn import reconstruct_positions\n",
    "from utils_rnn import plot_input_and_predictions\n",
    "from utils_rnn import folium_plot_trip_with_prediction\n",
    "from utils_rnn import mass_xy_to_latlon\n",
    "from utils_rnn import compute_errors\n",
    "from utils_rnn import haversine\n",
    "\n",
    "# --------------------------\n",
    "# Configurable parameters\n",
    "# --------------------------\n",
    "CSV_PATH = \"data/ais_data_5min_clean.csv\"   # <-- replace with your file\n",
    "INPUT_FEATURES = [\"dx\", \"dy\"]  # easy to change later\n",
    "OUTPUT_FEATURES = [\"dx\", \"dy\"] # identical for autoregression\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461492a3",
   "metadata": {},
   "source": [
    "# Train Validation Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf525c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: (71321, 9)\n",
      "Train size: (52983, 13)\n",
      "Val size: (5339, 13)\n",
      "Test size: (12999, 13)\n",
      "Feature shapes: (50543, 10, 2) (50543, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Expect columns: MMSI, segment, lat, lon, timestamp (optional)\n",
    "print(\"Loaded data:\", df.shape)\n",
    "\n",
    "# 2. Convert lat/lon to x/y and compute deltas\n",
    "df = add_xy_and_deltas(df)\n",
    "\n",
    "# 3. Split into train/val/test\n",
    "train_df, val_df, test_df = split_train_val_test(df)\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Val size:\", val_df.shape)\n",
    "print(\"Test size:\", test_df.shape)\n",
    "\n",
    "SEQ_LEN = 10\n",
    "X_train, y_train, _         = create_sequences(train_df, INPUT_FEATURES, OUTPUT_FEATURES, seq_len=SEQ_LEN)\n",
    "X_val, y_val, _             = create_sequences(val_df, INPUT_FEATURES, OUTPUT_FEATURES, seq_len=SEQ_LEN)\n",
    "X_test, y_test, test_meta   = create_sequences(test_df, INPUT_FEATURES, OUTPUT_FEATURES, seq_len=SEQ_LEN)\n",
    "\n",
    "num_sequences, seq_len, num_features = X_train.shape\n",
    "X_train_flat = X_train.reshape(-1, num_features)\n",
    "\n",
    "# WITHOUT TRANSFORM IT WORKS BEST\n",
    "\"\"\" # Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_flat)\n",
    "\n",
    "# Transform all sets\n",
    "X_train = scaler.transform(X_train_flat).reshape(num_sequences, seq_len, num_features)\n",
    "\n",
    "# Validation\n",
    "X_val = scaler.transform(X_val_original.reshape(-1, num_features)).reshape(X_val_original.shape)\n",
    "X_test = scaler.transform(X_test_original.reshape(-1, num_features)).reshape(X_test_original.shape)\n",
    "\n",
    "# Targets (Y) also normalized with same scaler\n",
    "y_train = scaler.transform(y_train_original)\n",
    "y_val   = scaler.transform(y_val_original)\n",
    "y_test  = scaler.transform(y_test_original) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Feature shapes:\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130eb92",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 21:16:14.016794: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-28 21:16:14.897234: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-28 21:16:14.900599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-28 21:16:16.628738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.1\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Import deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd98f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_rnn_model(input_shape, output_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Stacked RNN layers\n",
    "    model.add(SimpleRNN(2*128, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(SimpleRNN(2*64, return_sequences=False, activation='relu'))\n",
    "\n",
    "    # Dense layers for nonlinear mapping\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Final regression output\n",
    "    model.add(Dense(output_dim, kernel_regularizer=l2(1e-4)))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 10, 256)           66304     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 256)           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               49280     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124482 (486.26 KB)\n",
      "Trainable params: 124226 (485.26 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_targets = y_train.shape[1]\n",
    "\n",
    "model = build_rnn_model(\n",
    "    input_shape=(n_timesteps, n_features),\n",
    "    output_dim=n_targets\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "790/790 [==============================] - 16s 18ms/step - loss: 950381.0000 - mae: 699.6265 - val_loss: 670480.0000 - val_mae: 556.9824 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "790/790 [==============================] - 11s 14ms/step - loss: 809918.0000 - mae: 646.4664 - val_loss: 486022.3125 - val_mae: 474.8901 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 592517.6875 - mae: 554.7106 - val_loss: 336926.8438 - val_mae: 388.5743 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 421236.9375 - mae: 466.3307 - val_loss: 238413.5938 - val_mae: 328.2779 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "790/790 [==============================] - 11s 14ms/step - loss: 282633.0938 - mae: 377.9044 - val_loss: 126694.7734 - val_mae: 232.8397 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 171168.7656 - mae: 287.1958 - val_loss: 68631.7266 - val_mae: 166.8992 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 104707.3359 - mae: 218.9049 - val_loss: 45079.3633 - val_mae: 133.9905 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 79636.0625 - mae: 185.6819 - val_loss: 26549.1211 - val_mae: 86.3487 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 72256.2578 - mae: 174.2446 - val_loss: 22373.6562 - val_mae: 72.2214 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 69325.3672 - mae: 168.6586 - val_loss: 23490.9941 - val_mae: 77.3654 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 69493.2578 - mae: 168.8182 - val_loss: 22030.0469 - val_mae: 70.7627 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 67108.9219 - mae: 164.9206 - val_loss: 23131.5703 - val_mae: 75.5259 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 66037.1562 - mae: 162.6364 - val_loss: 19694.4746 - val_mae: 57.8123 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 66011.7266 - mae: 163.0226 - val_loss: 22294.3262 - val_mae: 74.5650 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 65102.4102 - mae: 161.6083 - val_loss: 20051.0117 - val_mae: 61.4503 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 64790.5703 - mae: 161.0909 - val_loss: 21736.4961 - val_mae: 70.2270 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 64628.1562 - mae: 160.3554 - val_loss: 20891.7266 - val_mae: 66.7620 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "790/790 [==============================] - 11s 13ms/step - loss: 64134.2812 - mae: 159.4955 - val_loss: 26068.4492 - val_mae: 84.5844 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 63207.7539 - mae: 158.1737 - val_loss: 20587.4961 - val_mae: 65.9406 - lr: 5.0000e-05\n",
      "Epoch 20/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 62925.8867 - mae: 157.4133 - val_loss: 21302.1309 - val_mae: 69.7973 - lr: 5.0000e-05\n",
      "Epoch 21/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 60820.1523 - mae: 154.7390 - val_loss: 20431.1758 - val_mae: 63.5042 - lr: 5.0000e-05\n",
      "Epoch 22/100\n",
      "790/790 [==============================] - 11s 13ms/step - loss: 61418.3555 - mae: 155.8261 - val_loss: 20800.8594 - val_mae: 66.4048 - lr: 5.0000e-05\n",
      "Epoch 23/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 60971.5000 - mae: 155.1443 - val_loss: 19348.9434 - val_mae: 58.4975 - lr: 5.0000e-05\n",
      "Epoch 24/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 60724.0977 - mae: 154.5081 - val_loss: 19634.2070 - val_mae: 61.5705 - lr: 5.0000e-05\n",
      "Epoch 25/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 60681.9375 - mae: 155.1297 - val_loss: 20219.9258 - val_mae: 64.6310 - lr: 5.0000e-05\n",
      "Epoch 26/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 60884.7812 - mae: 154.9114 - val_loss: 19671.0820 - val_mae: 61.3456 - lr: 5.0000e-05\n",
      "Epoch 27/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 61446.0508 - mae: 155.5209 - val_loss: 20011.2305 - val_mae: 63.4922 - lr: 5.0000e-05\n",
      "Epoch 28/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 59888.9414 - mae: 153.4935 - val_loss: 20160.0879 - val_mae: 64.9713 - lr: 5.0000e-05\n",
      "Epoch 29/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 59876.0938 - mae: 154.3691 - val_loss: 19447.7891 - val_mae: 59.9280 - lr: 2.5000e-05\n",
      "Epoch 30/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 59673.4805 - mae: 153.2104 - val_loss: 19442.9316 - val_mae: 60.8633 - lr: 2.5000e-05\n",
      "Epoch 31/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 59100.0625 - mae: 152.1057 - val_loss: 19325.2988 - val_mae: 59.6509 - lr: 2.5000e-05\n",
      "Epoch 32/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 58942.6445 - mae: 152.2516 - val_loss: 19473.0098 - val_mae: 60.2926 - lr: 2.5000e-05\n",
      "Epoch 33/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 59266.0859 - mae: 152.3018 - val_loss: 19401.6191 - val_mae: 60.0437 - lr: 2.5000e-05\n",
      "Epoch 34/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 59119.8281 - mae: 152.5857 - val_loss: 19391.0039 - val_mae: 60.2626 - lr: 2.5000e-05\n",
      "Epoch 35/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 58941.3984 - mae: 152.2369 - val_loss: 19689.0098 - val_mae: 61.7713 - lr: 2.5000e-05\n",
      "Epoch 36/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 59524.3750 - mae: 153.5014 - val_loss: 19189.5273 - val_mae: 57.7256 - lr: 2.5000e-05\n",
      "Epoch 37/100\n",
      "790/790 [==============================] - 10s 13ms/step - loss: 59030.2969 - mae: 152.2049 - val_loss: 19578.4629 - val_mae: 61.4926 - lr: 2.5000e-05\n",
      "Epoch 38/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 58060.7539 - mae: 150.7155 - val_loss: 19482.6836 - val_mae: 61.3596 - lr: 2.5000e-05\n",
      "Epoch 39/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 58551.5859 - mae: 151.9918 - val_loss: 19313.8555 - val_mae: 58.1863 - lr: 2.5000e-05\n",
      "Epoch 40/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 59176.1211 - mae: 152.1834 - val_loss: 19872.8438 - val_mae: 64.0788 - lr: 2.5000e-05\n",
      "Epoch 41/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 58354.3398 - mae: 151.2095 - val_loss: 19373.3867 - val_mae: 60.9298 - lr: 2.5000e-05\n",
      "Epoch 42/100\n",
      "790/790 [==============================] - 10s 12ms/step - loss: 58721.7812 - mae: 151.7983 - val_loss: 19414.3770 - val_mae: 61.1541 - lr: 1.2500e-05\n",
      "Epoch 43/100\n",
      "790/790 [==============================] - 12s 15ms/step - loss: 58499.2188 - mae: 151.3895 - val_loss: 19301.0605 - val_mae: 59.7962 - lr: 1.2500e-05\n",
      "Epoch 44/100\n",
      "790/790 [==============================] - 11s 13ms/step - loss: 58250.1719 - mae: 151.4356 - val_loss: 19268.5898 - val_mae: 59.6813 - lr: 1.2500e-05\n",
      "Epoch 45/100\n",
      "790/790 [==============================] - 12s 15ms/step - loss: 57978.7344 - mae: 150.6319 - val_loss: 19271.3652 - val_mae: 59.8574 - lr: 1.2500e-05\n",
      "Epoch 46/100\n",
      "790/790 [==============================] - 12s 15ms/step - loss: 57718.1406 - mae: 150.2606 - val_loss: 19343.6504 - val_mae: 60.6131 - lr: 1.2500e-05\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class GracefulInterrupt(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_training = False\n",
    "        signal.signal(signal.SIGINT, self.handle_sigint)\n",
    "\n",
    "    def handle_sigint(self, signum, frame):\n",
    "        print(\"\\nSIGINT received: Training will stop after this epoch.\\n\")\n",
    "        self.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.stop_training:\n",
    "            print(f\"Stopping at epoch {epoch+1}.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "    GracefulInterrupt()\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94991f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66e14ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 12309\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7256c9a05970>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/chris/ml_env/lib/python3.8/site-packages/keras/src/backend.py\", line 5161, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/home/chris/ml_env/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "50 out of 12309\n",
      "100 out of 12309\n",
      "150 out of 12309\n",
      "200 out of 12309\n",
      "250 out of 12309\n",
      "300 out of 12309\n",
      "350 out of 12309\n",
      "400 out of 12309\n",
      "450 out of 12309\n",
      "500 out of 12309\n",
      "550 out of 12309\n",
      "600 out of 12309\n",
      "650 out of 12309\n",
      "700 out of 12309\n",
      "750 out of 12309\n",
      "800 out of 12309\n",
      "850 out of 12309\n",
      "900 out of 12309\n",
      "950 out of 12309\n",
      "1000 out of 12309\n",
      "\n",
      "Step-wise error statistics (meters):\n",
      "Step 1: mean=2153.72, std=10714.67, median=1337.28, n=1001\n",
      "Step 2: mean=4219.32, std=14981.03, median=2653.05, n=1001\n",
      "Step 3: mean=6205.62, std=18122.04, median=3865.65, n=1001\n",
      "Step 4: mean=8117.80, std=20666.01, median=5097.54, n=1001\n",
      "Step 5: mean=9967.26, std=22800.32, median=6309.53, n=1001\n",
      "Step 6: mean=11748.00, std=24639.27, median=7490.57, n=1001\n",
      "Step 7: mean=13452.71, std=26227.77, median=8652.13, n=1001\n",
      "Step 8: mean=15080.49, std=27631.95, median=9828.50, n=1001\n",
      "Step 9: mean=16645.71, std=28888.65, median=11009.86, n=1001\n",
      "Step 10: mean=18156.51, std=30029.15, median=12173.10, n=1001\n"
     ]
    }
   ],
   "source": [
    "horizon = 10\n",
    "all_stats = []\n",
    "\n",
    "# container for per-step errors across all sequences\n",
    "step_errors = [[] for _ in range(horizon)]\n",
    "\n",
    "for seq_id in range(len(X_test)):\n",
    "    if seq_id > 1000:\n",
    "        break\n",
    "    if seq_id % 50 == 0:\n",
    "        print(seq_id, \"out of\", len(X_test))\n",
    "    \n",
    "    # --- Predict horizon steps ---\n",
    "    preds = autoregressive_predict(model, X_test[seq_id], horizon)\n",
    "    # --- Reconstruct positions ---\n",
    "    start_idx = test_meta[seq_id][\"end_index\"]  # last input row\n",
    "    start_xy = df.loc[start_idx, [\"x\",\"y\"]].values\n",
    "    pred_positions_xy = reconstruct_positions(preds, start_xy)[1:] # I want only the predictions, not the \"starting\" point (which is the last true)\n",
    "    \n",
    "    # --- Convert to lat/lon ---\n",
    "    pred_positions_latlon = mass_xy_to_latlon(pred_positions_xy)\n",
    "    target_indices = [test_meta[seq_id][\"target_index\"] + k for k in range(horizon)]\n",
    "    true_positions_latlon = df.loc[target_indices, [\"Latitude\",\"Longtitude\"]].values\n",
    "    \n",
    "    # --- Compute stats per step ---\n",
    "    # compute haversine distance for each step\n",
    "    for step in range(horizon):\n",
    "        err = haversine(tuple(true_positions_latlon[step]), tuple(pred_positions_latlon[step]))\n",
    "        step_errors[step].append(err)\n",
    "\n",
    "# --- Aggregate error per step ---\n",
    "print(\"\\nStep-wise error statistics (meters):\")\n",
    "for step in range(horizon):\n",
    "    errs = np.array(step_errors[step])\n",
    "    mean_e = np.mean(errs)\n",
    "    std_e = np.std(errs)\n",
    "    med_e = np.median(errs)\n",
    "    print(f\"Step {step+1}: mean={mean_e:.2f}, std={std_e:.2f}, median={med_e:.2f}, n={len(errs)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSL Deep Learning project",
   "language": "python",
   "name": "deep_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
