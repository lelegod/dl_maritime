{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318a3958",
   "metadata": {},
   "source": [
    "# Imports, read csv, functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff852b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_rnn import add_xy_and_deltas\n",
    "from utils_rnn import split_train_val_test\n",
    "from utils_rnn import create_sequences\n",
    "from utils_rnn import autoregressive_predict\n",
    "from utils_rnn import reconstruct_positions\n",
    "from utils_rnn import plot_input_and_predictions\n",
    "from utils_rnn import folium_plot_trip_with_prediction\n",
    "from utils_rnn import mass_xy_to_latlon\n",
    "from utils_rnn import compute_errors\n",
    "from utils_rnn import haversine\n",
    "\n",
    "# --------------------------\n",
    "# Configurable parameters\n",
    "# --------------------------\n",
    "CSV_PATH = \"data/ais_data_5min_clean.csv\"   # <-- replace with your file\n",
    "INPUT_FEATURES = [\"dx\", \"dy\"]  # easy to change later\n",
    "OUTPUT_FEATURES = [\"dx\", \"dy\"] # identical for autoregression\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461492a3",
   "metadata": {},
   "source": [
    "# Train Validation Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf525c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Expect columns: MMSI, segment, lat, lon, timestamp (optional)\n",
    "print(\"Loaded data:\", df.shape)\n",
    "\n",
    "# 2. Convert lat/lon to x/y and compute deltas\n",
    "df = add_xy_and_deltas(df)\n",
    "\n",
    "# 3. Split into train/val/test\n",
    "train_df, val_df, test_df = split_train_val_test(df)\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Val size:\", val_df.shape)\n",
    "print(\"Test size:\", test_df.shape)\n",
    "\n",
    "SEQ_LEN = 10\n",
    "X_train, y_train, _         = create_sequences(train_df, INPUT_FEATURES, OUTPUT_FEATURES, seq_len=SEQ_LEN)\n",
    "X_val, y_val, _             = create_sequences(val_df, INPUT_FEATURES, OUTPUT_FEATURES, seq_len=SEQ_LEN)\n",
    "X_test, y_test, test_meta   = create_sequences(test_df, INPUT_FEATURES, OUTPUT_FEATURES, seq_len=SEQ_LEN)\n",
    "\n",
    "num_sequences, seq_len, num_features = X_train.shape\n",
    "X_train_flat = X_train.reshape(-1, num_features)\n",
    "\n",
    "# WITHOUT TRANSFORM IT WORKS BEST\n",
    "\"\"\" # Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_flat)\n",
    "\n",
    "# Transform all sets\n",
    "X_train = scaler.transform(X_train_flat).reshape(num_sequences, seq_len, num_features)\n",
    "\n",
    "# Validation\n",
    "X_val = scaler.transform(X_val_original.reshape(-1, num_features)).reshape(X_val_original.shape)\n",
    "X_test = scaler.transform(X_test_original.reshape(-1, num_features)).reshape(X_test_original.shape)\n",
    "\n",
    "# Targets (Y) also normalized with same scaler\n",
    "y_train = scaler.transform(y_train_original)\n",
    "y_val   = scaler.transform(y_val_original)\n",
    "y_test  = scaler.transform(y_test_original) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Feature shapes:\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130eb92",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd98f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_rnn_model(input_shape, output_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Stacked RNN layers\n",
    "    model.add(SimpleRNN(2*128, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(SimpleRNN(2*64, return_sequences=False, activation='relu'))\n",
    "\n",
    "    # Dense layers for nonlinear mapping\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Final regression output\n",
    "    model.add(Dense(output_dim, kernel_regularizer=l2(1e-4)))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_targets = y_train.shape[1]\n",
    "\n",
    "model = build_rnn_model(\n",
    "    input_shape=(n_timesteps, n_features),\n",
    "    output_dim=n_targets\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class GracefulInterrupt(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_training = False\n",
    "        signal.signal(signal.SIGINT, self.handle_sigint)\n",
    "\n",
    "    def handle_sigint(self, signum, frame):\n",
    "        print(\"\\nSIGINT received: Training will stop after this epoch.\\n\")\n",
    "        self.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.stop_training:\n",
    "            print(f\"Stopping at epoch {epoch+1}.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "    GracefulInterrupt()\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94991f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e14ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 10\n",
    "all_stats = []\n",
    "\n",
    "# container for per-step errors across all sequences\n",
    "step_errors = [[] for _ in range(horizon)]\n",
    "\n",
    "for seq_id in range(len(X_test)):\n",
    "    if seq_id > 1000:\n",
    "        break\n",
    "    if seq_id % 50 == 0:\n",
    "        print(seq_id, \"out of\", len(X_test))\n",
    "    \n",
    "    # --- Predict horizon steps ---\n",
    "    preds = autoregressive_predict(model, X_test[seq_id], horizon)\n",
    "    # --- Reconstruct positions ---\n",
    "    start_idx = test_meta[seq_id][\"end_index\"]  # last input row\n",
    "    start_xy = df.loc[start_idx, [\"x\",\"y\"]].values\n",
    "    pred_positions_xy = reconstruct_positions(preds, start_xy)[1:] # I want only the predictions, not the \"starting\" point (which is the last true)\n",
    "    \n",
    "    # --- Convert to lat/lon ---\n",
    "    pred_positions_latlon = mass_xy_to_latlon(pred_positions_xy)\n",
    "    target_indices = [test_meta[seq_id][\"target_index\"] + k for k in range(horizon)]\n",
    "    true_positions_latlon = df.loc[target_indices, [\"Latitude\",\"Longtitude\"]].values\n",
    "    \n",
    "    # --- Compute stats per step ---\n",
    "    # compute haversine distance for each step\n",
    "    for step in range(horizon):\n",
    "        err = haversine(tuple(true_positions_latlon[step]), tuple(pred_positions_latlon[step]))\n",
    "        step_errors[step].append(err)\n",
    "\n",
    "# --- Aggregate error per step ---\n",
    "print(\"\\nStep-wise error statistics (meters):\")\n",
    "for step in range(horizon):\n",
    "    errs = np.array(step_errors[step])\n",
    "    mean_e = np.mean(errs)\n",
    "    std_e = np.std(errs)\n",
    "    med_e = np.median(errs)\n",
    "    print(f\"Step {step+1}: mean={mean_e:.2f}, std={std_e:.2f}, median={med_e:.2f}, n={len(errs)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSL Deep Learning project",
   "language": "python",
   "name": "deep_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
