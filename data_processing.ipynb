{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27781256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import create_mmsi_dict_from_file\n",
    "from utils import filter_stationary_ships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf88706",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/mmsi_type.txt\"\n",
    "mmsi_map = create_mmsi_dict_from_file(file_name)\n",
    "\n",
    "\n",
    "if mmsi_map:\n",
    "    print(\"--- Successfully created dictionary ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33829c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ais_combined.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fa552",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_with_types = df.copy()\n",
    "df_with_types['Type'] = df_with_types['MMSI'].astype(str).map(mmsi_map)\n",
    "df_with_types.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb85772",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mmsi = df['MMSI'].unique()\n",
    "unique_types = df_with_types['Type'].unique()\n",
    "\n",
    "print(\"Total unique MMSI count:\", len(unique_mmsi))\n",
    "print(\"Unique ship types in dataset:\", unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_type = ['Cargo ship', 'Cargo ship (HAZ-A)', 'Cargo ship (HAZ-B)', 'Cargo ship (HAZ-D)', 'Tanker', 'Tanker (HAZ-A)', 'Tanker (HAZ-B)', 'Tanker (HAZ-C)', 'Tanker (HAZ-D)']\n",
    "df_cargo = df_with_types[df_with_types['Type'].isin(allowed_type)]\n",
    "df_cargo = df_cargo.drop(columns=[\"Type\"], axis= 1)\n",
    "df_cargo.head()\n",
    "df_cargo_filtered = filter_stationary_ships(df_cargo) # This df has dropped stationary ships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2600ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import segment_and_renumber, haversine_m\n",
    "\n",
    "# Configuration parameters\n",
    "GAP_BREAK_MIN = 10          # minutes to start a new segment\n",
    "INTERP_LIMIT_MIN = 10        # interpolate gaps up to 10 minutes\n",
    "MAX_DISTANCE_M = 3000       # ~97 knots max distance per minute\n",
    "MAX_SOG_KNOTS = 40          # maximum speed over ground\n",
    "OUTPUT_PATH = \"data/ais_data_1min_clean.csv\"\n",
    "NUM_COLS = [\"SOG\", \"COG\", \"Longtitude\", \"Latitude\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: Data Preprocessing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort data by MMSI and Timestamp\n",
    "df_cargo = df_cargo.sort_values([\"MMSI\", \"Timestamp\"]).reset_index(drop=True)\n",
    "df_cargo[\"Timestamp\"] = pd.to_datetime(df_cargo[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Initial data shape: {df_cargo.shape}\")\n",
    "print(f\"Data types:\\n{df_cargo.dtypes}\\n\")\n",
    "\n",
    "# Segment trajectories based on time gaps\n",
    "print(\"Segmenting trajectories...\")\n",
    "df = segment_and_renumber(df_cargo, GAP_BREAK_MIN)\n",
    "\n",
    "# Downsample & interpolate per segment\n",
    "print(\"Downsampling to 1-minute intervals and interpolating...\")\n",
    "results = []\n",
    "\n",
    "for (mmsi, seg), g in df.groupby([\"MMSI\", \"Segment\"], observed=True):\n",
    "    g = g.set_index(\"Timestamp\")\n",
    "    \n",
    "    # Downsample to 1-minute intervals (keep last observation)\n",
    "    g1 = g.resample(\"1min\").last()\n",
    "    \n",
    "    # Interpolate numeric columns for short gaps only\n",
    "    g1[NUM_COLS] = g1[NUM_COLS].interpolate(\n",
    "        method=\"time\", limit=INTERP_LIMIT_MIN, limit_direction=\"both\"\n",
    "    )\n",
    "    \n",
    "    # Drop minutes still NaN (beyond real range or long gaps)\n",
    "    g1 = g1.dropna(subset=NUM_COLS, how=\"all\")\n",
    "    \n",
    "    # Fill identifiers\n",
    "    g1[\"MMSI\"] = mmsi\n",
    "    g1[\"Segment\"] = seg\n",
    "    \n",
    "    # Calculate distance and speed between consecutive points\n",
    "    lat = g1[\"Latitude\"].to_numpy()\n",
    "    lon = g1[\"Longtitude\"].to_numpy()\n",
    "    lat_prev, lon_prev = np.roll(lat, 1), np.roll(lon, 1)\n",
    "    lat_prev[0], lon_prev[0] = lat[0], lon[0]\n",
    "    \n",
    "    g1[\"distance_m\"] = haversine_m(lat, lon, lat_prev, lon_prev)\n",
    "    g1.loc[g1.index[0], \"distance_m\"] = 0.0\n",
    "    g1[\"speed_mps_track\"] = g1[\"distance_m\"] / 60.0\n",
    "    \n",
    "    # Filter unrealistic movement or SOG\n",
    "    g1 = g1[(g1[\"distance_m\"] < MAX_DISTANCE_M) & (g1[\"SOG\"] <= MAX_SOG_KNOTS)]\n",
    "    \n",
    "    results.append(g1)\n",
    "\n",
    "# Combine all segments\n",
    "df_clean = pd.concat(results).reset_index()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: Data Quality Check\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rows before cleaning: {len(df_clean)}\")\n",
    "\n",
    "# Check for missing data\n",
    "missing = df_clean[df_clean[[\"SOG\", \"COG\", \"Latitude\", \"Longtitude\"]].isna().any(axis=1)]\n",
    "print(f\"Rows with missing numeric data: {len(missing)} ({len(missing)/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"MMSI with missing data: {missing['MMSI'].nunique()}\")\n",
    "\n",
    "# Remove rows with missing critical data\n",
    "df_clean = df_clean.dropna(subset=[\"SOG\", \"COG\", \"Latitude\", \"Longtitude\", \"MMSI\", \"Segment\"])\n",
    "print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "\n",
    "# Verify time gaps\n",
    "max_gap = df_clean.groupby([\"MMSI\",\"Segment\"])[\"Timestamp\"].diff().dt.total_seconds().div(60).max()\n",
    "print(f\"Maximum time gap in cleaned data: {max_gap:.2f} minutes\")\n",
    "has_large_gaps = (df_clean.groupby([\"MMSI\",\"Segment\"])[\"Timestamp\"]\n",
    "                  .diff().dt.total_seconds().div(60).max() > 5).any()\n",
    "print(f\"Has gaps > 5 minutes: {has_large_gaps}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: Final Dataset Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Unique vessels (MMSI): {df_clean['MMSI'].nunique()}\")\n",
    "print(f\"Total segments: {df_clean.groupby(['MMSI', 'Segment']).ngroups}\")\n",
    "print(f\"Average segment length: {df_clean.groupby(['MMSI', 'Segment']).size().mean():.1f} minutes\")\n",
    "print(f\"Columns: {list(df_clean.columns)}\")\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nCleaned data saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7edcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for GRU model\n",
    "SEQUENCE_LENGTH = 10  # Use 10 minutes of history to predict next minute\n",
    "FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\"]  # Input features\n",
    "TARGET_FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\"]  # What to predict\n",
    "MIN_SEGMENT_LENGTH = SEQUENCE_LENGTH + 5  # Minimum segment length to use\n",
    "\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH} minutes\")\n",
    "print(f\"Input features: {FEATURES}\")\n",
    "print(f\"Target features: {TARGET_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length, features, target_features):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataframe for a single segment\n",
    "    sequence_length : int\n",
    "        Number of timesteps to use as input\n",
    "    features : list\n",
    "        List of feature column names to use as input\n",
    "    target_features : list\n",
    "        List of feature column names to predict\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.array\n",
    "        Input sequences of shape (n_samples, sequence_length, n_features)\n",
    "    y : np.array\n",
    "        Target values of shape (n_samples, n_target_features)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    data_values = data[features].values\n",
    "    target_values = data[target_features].values\n",
    "    \n",
    "    for i in range(len(data_values) - sequence_length):\n",
    "        X.append(data_values[i:i+sequence_length])\n",
    "        y.append(target_values[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def prepare_training_data(df, sequence_length, features, target_features, min_segment_length):\n",
    "    \"\"\"\n",
    "    Prepare training data from the entire dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned AIS data\n",
    "    sequence_length : int\n",
    "        Number of timesteps for input sequences\n",
    "    features : list\n",
    "        Input feature names\n",
    "    target_features : list\n",
    "        Target feature names\n",
    "    min_segment_length : int\n",
    "        Minimum segment length to include\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.array\n",
    "        All input sequences\n",
    "    y : np.array\n",
    "        All target values\n",
    "    segment_info : list\n",
    "        Information about which segments were used\n",
    "    \"\"\"\n",
    "    X_all, y_all = [], []\n",
    "    segment_info = []\n",
    "    \n",
    "    for (mmsi, seg), group in df.groupby([\"MMSI\", \"Segment\"]):\n",
    "        # Skip short segments\n",
    "        if len(group) < min_segment_length:\n",
    "            continue\n",
    "            \n",
    "        # Sort by timestamp to ensure correct order\n",
    "        group = group.sort_values(\"Timestamp\")\n",
    "        \n",
    "        # Create sequences for this segment\n",
    "        X_seg, y_seg = create_sequences(group, sequence_length, features, target_features)\n",
    "        \n",
    "        if len(X_seg) > 0:\n",
    "            X_all.append(X_seg)\n",
    "            y_all.append(y_seg)\n",
    "            segment_info.append({\n",
    "                'mmsi': mmsi,\n",
    "                'segment': seg,\n",
    "                'length': len(group),\n",
    "                'sequences': len(X_seg)\n",
    "            })\n",
    "    \n",
    "    # Concatenate all sequences\n",
    "    X = np.concatenate(X_all, axis=0)\n",
    "    y = np.concatenate(y_all, axis=0)\n",
    "    \n",
    "    return X, y, segment_info\n",
    "\n",
    "\n",
    "print(\"Data preparation functions defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Preparing Training Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare sequences\n",
    "X, y, segment_info = prepare_training_data(\n",
    "    df_clean, \n",
    "    SEQUENCE_LENGTH, \n",
    "    FEATURES, \n",
    "    TARGET_FEATURES, \n",
    "    MIN_SEGMENT_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"Total sequences created: {len(X)}\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Segments used: {len(segment_info)}\")\n",
    "print(f\"Average sequences per segment: {len(X) / len(segment_info):.1f}\")\n",
    "\n",
    "# Display some statistics\n",
    "segment_lengths = [s['length'] for s in segment_info]\n",
    "print(f\"\\nSegment statistics:\")\n",
    "print(f\"  Min length: {min(segment_lengths)} minutes\")\n",
    "print(f\"  Max length: {max(segment_lengths)} minutes\")\n",
    "print(f\"  Mean length: {np.mean(segment_lengths):.1f} minutes\")\n",
    "print(f\"  Median length: {np.median(segment_lengths):.1f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70824cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reshape X for normalization: (samples, timesteps, features) -> (samples * timesteps, features)\n",
    "n_samples, n_timesteps, n_features = X.shape\n",
    "X_reshaped = X.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X_reshaped)\n",
    "X_normalized = X_normalized.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "# Normalize targets\n",
    "scaler_y = StandardScaler()\n",
    "y_normalized = scaler_y.fit_transform(y)\n",
    "\n",
    "print(f\"Input data normalized: {X_normalized.shape}\")\n",
    "print(f\"Target data normalized: {y_normalized.shape}\")\n",
    "print(f\"\\nFeature means: {scaler_X.mean_}\")\n",
    "print(f\"Feature stds: {scaler_X.scale_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets BY SHIP (MMSI)\n",
    "print(\"=\"*60)\n",
    "print(\"Splitting Data by Ships (MMSI)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get unique MMSIs from segment_info\n",
    "unique_mmsis = list(set([seg['mmsi'] for seg in segment_info]))\n",
    "n_ships = len(unique_mmsis)\n",
    "\n",
    "print(f\"Total unique ships: {n_ships}\")\n",
    "\n",
    "# Split ships into train (64%), val (16%), test (20%)\n",
    "# First split: 80% train+val, 20% test\n",
    "mmsi_temp, mmsi_test = train_test_split(\n",
    "    unique_mmsis, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: 80% train, 20% val (of the temp set)\n",
    "mmsi_train, mmsi_val = train_test_split(\n",
    "    mmsi_temp, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nShips in training set: {len(mmsi_train)} ({len(mmsi_train)/n_ships*100:.1f}%)\")\n",
    "print(f\"Ships in validation set: {len(mmsi_val)} ({len(mmsi_val)/n_ships*100:.1f}%)\")\n",
    "print(f\"Ships in test set: {len(mmsi_test)} ({len(mmsi_test)/n_ships*100:.1f}%)\")\n",
    "\n",
    "# Create sets of MMSIs for fast lookup\n",
    "mmsi_train_set = set(mmsi_train)\n",
    "mmsi_val_set = set(mmsi_val)\n",
    "mmsi_test_set = set(mmsi_test)\n",
    "\n",
    "# Split sequences based on which ship they belong to\n",
    "train_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_train_set]\n",
    "val_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_val_set]\n",
    "test_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_test_set]\n",
    "\n",
    "# Get the actual sequences for each set\n",
    "X_train = X_normalized[train_indices]\n",
    "y_train = y_normalized[train_indices]\n",
    "\n",
    "X_val = X_normalized[val_indices]\n",
    "y_val = y_normalized[val_indices]\n",
    "\n",
    "X_test = X_normalized[test_indices]\n",
    "y_test = y_normalized[test_indices]\n",
    "\n",
    "print(f\"\\nSequences in training set: {X_train.shape[0]} ({X_train.shape[0]/X_normalized.shape[0]*100:.1f}%)\")\n",
    "print(f\"Sequences in validation set: {X_val.shape[0]} ({X_val.shape[0]/X_normalized.shape[0]*100:.1f}%)\")\n",
    "print(f\"Sequences in test set: {X_test.shape[0]} ({X_test.shape[0]/X_normalized.shape[0]*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ“ Data split by ships - no temporal leakage between sets!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_maritime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
