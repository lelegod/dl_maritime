{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9347f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T13:41:33.831575Z",
     "start_time": "2025-11-29T13:41:33.812038Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from utils import (\n",
    "    create_mmsi_dict_from_file,\n",
    "    filter_stationary_ships,\n",
    "    segment_and_renumber,\n",
    "    haversine_m,\n",
    "    prepare_training_data,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abce7ac",
   "metadata": {},
   "source": [
    "# Fecthing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a217a4cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T13:41:23.914780Z",
     "start_time": "2025-11-29T13:41:23.910524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Successfully created dictionary ---\n"
     ]
    }
   ],
   "source": [
    "file_name = \"data/mmsi_type.txt\"\n",
    "mmsi_map = create_mmsi_dict_from_file(file_name)\n",
    "\n",
    "if mmsi_map:\n",
    "    print(\"--- Successfully created dictionary ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c82378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 stationary ships out of 291.\n",
      "Cleaned DF contains 285 ships.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ais_combined.csv\")\n",
    "df_with_types = df.copy()\n",
    "df_with_types['Type'] = df_with_types['MMSI'].astype(str).map(mmsi_map)\n",
    "allowed_type = ['Cargo ship', 'Cargo ship (HAZ-A)', 'Cargo ship (HAZ-B)', 'Cargo ship (HAZ-D)', 'Tanker', 'Tanker (HAZ-A)', 'Tanker (HAZ-B)', 'Tanker (HAZ-C)', 'Tanker (HAZ-D)']\n",
    "df_cargo = df_with_types[df_with_types['Type'].isin(allowed_type)]\n",
    "\n",
    "df_cargo = df_cargo.drop(columns=[\"Type\"], axis= 1)\n",
    "\n",
    "df_cargo_filtered = filter_stationary_ships(df_cargo) # This df has dropped stationary ships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677b289",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9c94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "GAP_BREAK_MIN = 180\n",
    "INTERPOLATION_LIMIT_MIN = None\n",
    "MAX_DISTANCE_M = 3000\n",
    "MAX_SOG_KNOTS = 40\n",
    "OUTPUT_PATH = \"data/ais_data_5min_clean.csv\"\n",
    "NUM_COLS = [\"SOG\", \"COG\", \"Longtitude\", \"Latitude\"]\n",
    "MIN_SEGMENT_LENGTH = 25\n",
    "INTERVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bbfa19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Data Preprocessing\n",
      "============================================================\n",
      "Initial data shape: (1235258, 7)\n",
      "Data types:\n",
      "MMSI                   int64\n",
      "SOG                  float64\n",
      "COG                  float64\n",
      "Longtitude           float64\n",
      "Latitude             float64\n",
      "Timestamp     datetime64[ns]\n",
      "Segment                int64\n",
      "dtype: object\n",
      "\n",
      "Segmenting trajectories...\n",
      "Downsampling to 5-minute intervals and interpolating...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 1: Data Preprocessing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort data by MMSI and Timestamp\n",
    "df_cargo = df_cargo.sort_values([\"MMSI\", \"Timestamp\"]).reset_index(drop=True)\n",
    "df_cargo[\"Timestamp\"] = pd.to_datetime(df_cargo[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Initial data shape: {df_cargo.shape}\")\n",
    "print(f\"Data types:\\n{df_cargo.dtypes}\\n\")\n",
    "\n",
    "# Segment trajectories based on time gaps\n",
    "print(\"Segmenting trajectories...\")\n",
    "df = segment_and_renumber(df_cargo, GAP_BREAK_MIN)\n",
    "\n",
    "# Downsample & interpolate per segment\n",
    "print(f\"Downsampling to {INTERVAL}-minute intervals and interpolating...\")\n",
    "results = []\n",
    "\n",
    "for (mmsi, seg), g in df.groupby([\"MMSI\", \"Segment\"], observed=True):\n",
    "    g = g.set_index(\"Timestamp\")\n",
    "    \n",
    "    # Downsample to 5-minute intervals (keep last observation)\n",
    "    g1 = g.resample(f\"{INTERVAL}min\").last()\n",
    "    \n",
    "    # Interpolate numeric columns for short gaps only\n",
    "    g1[NUM_COLS] = g1[NUM_COLS].interpolate(\n",
    "        method=\"time\", limit=INTERPOLATION_LIMIT_MIN, limit_direction=\"both\"\n",
    "    )\n",
    "    \n",
    "    # Drop rows where ANY of the critical columns are NaN\n",
    "    g1 = g1.dropna(subset=NUM_COLS, how=\"any\")\n",
    "    \n",
    "    # Fill identifiers\n",
    "    g1[\"MMSI\"] = mmsi\n",
    "    g1[\"Segment\"] = seg\n",
    "    \n",
    "    # Calculate distance and speed between consecutive points\n",
    "    lat = g1[\"Latitude\"].to_numpy()\n",
    "    lon = g1[\"Longtitude\"].to_numpy()\n",
    "    lat_prev, lon_prev = np.roll(lat, 1), np.roll(lon, 1)\n",
    "    lat_prev[0], lon_prev[0] = lat[0], lon[0]\n",
    "    \n",
    "    g1[\"distance_m\"] = haversine_m(lat, lon, lat_prev, lon_prev)\n",
    "    g1.loc[g1.index[0], \"distance_m\"] = 0.0\n",
    "    g1[\"speed_mps_track\"] = g1[\"distance_m\"] / 60.0\n",
    "    \n",
    "    # Filter unrealistic movement or SOG\n",
    "    g1 = g1[(g1[\"distance_m\"] < MAX_DISTANCE_M) & (g1[\"SOG\"] <= MAX_SOG_KNOTS)]\n",
    "    \n",
    "    results.append(g1)\n",
    "\n",
    "# Combine all segments\n",
    "df_clean = pd.concat(results).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9c066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: Data Quality Check\n",
      "============================================================\n",
      "Rows before cleaning: 51903\n",
      "Rows with missing numeric data: 0 (0.00%)\n",
      "MMSI with missing data: 0\n",
      "Rows after cleaning: 51903\n",
      "Maximum time gap before re-segmentation: 250.00 minutes\n",
      "\n",
      "Re-segmenting based on gaps > 5 minutes...\n",
      "Maximum time gap after re-segmentation: 5.00 minutes\n",
      "Has gaps > 5 minutes: False\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 2: Data Quality Check\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rows before cleaning: {len(df_clean)}\")\n",
    "\n",
    "# Check for missing data\n",
    "missing = df_clean[df_clean[[\"SOG\", \"COG\", \"Latitude\", \"Longtitude\"]].isna().any(axis=1)]\n",
    "print(f\"Rows with missing numeric data: {len(missing)} ({len(missing)/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"MMSI with missing data: {missing['MMSI'].nunique()}\")\n",
    "\n",
    "# Remove rows with missing critical data\n",
    "df_clean = df_clean.dropna(subset=[\"SOG\", \"COG\", \"Latitude\", \"Longtitude\", \"MMSI\", \"Segment\"])\n",
    "print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "\n",
    "# Verify time gaps before re-segmentation\n",
    "max_gap = df_clean.groupby([\"MMSI\",\"Segment\"])[\"Timestamp\"].diff().dt.total_seconds().div(60).max()\n",
    "print(f\"Maximum time gap before re-segmentation: {max_gap:.2f} minutes\")\n",
    "\n",
    "# Re-segment based on time gaps created by filtering (gaps > INTERVAL min = new segment)\n",
    "print(f\"\\nRe-segmenting based on gaps > {INTERVAL} minutes...\")\n",
    "df_clean = df_clean.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"]).reset_index(drop=True)\n",
    "df_clean[\"time_gap\"] = df_clean.groupby([\"MMSI\", \"Segment\"])[\"Timestamp\"].diff().dt.total_seconds().div(60)\n",
    "df_clean[\"new_seg\"] = (df_clean[\"time_gap\"] > INTERVAL) | (df_clean[\"time_gap\"].isna())\n",
    "df_clean[\"Segment\"] = df_clean.groupby(\"MMSI\")[\"new_seg\"].cumsum()\n",
    "df_clean = df_clean.drop(columns=[\"time_gap\", \"new_seg\"])\n",
    "\n",
    "# Verify time gaps after re-segmentation\n",
    "max_gap_after = df_clean.groupby([\"MMSI\",\"Segment\"])[\"Timestamp\"].diff().dt.total_seconds().div(60).max()\n",
    "print(f\"Maximum time gap after re-segmentation: {max_gap_after:.2f} minutes\")\n",
    "has_large_gaps = (df_clean.groupby([\"MMSI\",\"Segment\"])[\"Timestamp\"]\n",
    "                  .diff().dt.total_seconds().div(60).max() > INTERVAL).any()\n",
    "print(f\"Has gaps > {INTERVAL} minutes: {has_large_gaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91fb519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: Segment Length Filtering\n",
      "============================================================\n",
      "\n",
      "Filtering segments with < 25 points...\n",
      "Segments before filtering: 453\n",
      "Rows before filtering: 51903\n",
      "Segments after filtering: 309\n",
      "Rows after filtering: 51023\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 3: Segment Length Filtering\")\n",
    "print(\"=\"*60)\n",
    "# Filter out short segments\n",
    "print(f\"\\nFiltering segments with < {MIN_SEGMENT_LENGTH} points...\")\n",
    "print(f\"Segments before filtering: {df_clean.groupby(['MMSI', 'Segment']).ngroups}\")\n",
    "print(f\"Rows before filtering: {len(df_clean)}\")\n",
    "\n",
    "segment_sizes = df_clean.groupby([\"MMSI\", \"Segment\"]).size()\n",
    "valid_segments = segment_sizes[segment_sizes >= MIN_SEGMENT_LENGTH].index\n",
    "df_clean = df_clean.set_index([\"MMSI\", \"Segment\"]).loc[valid_segments].reset_index()\n",
    "\n",
    "print(f\"Segments after filtering: {df_clean.groupby(['MMSI', 'Segment']).ngroups}\")\n",
    "print(f\"Rows after filtering: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "353dc651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Final Dataset Summary\n",
      "============================================================\n",
      "Total rows: 51023\n",
      "Unique vessels (MMSI): 279\n",
      "Total segments: 309\n",
      "Average segment length: 825.6 minutes\n",
      "Columns: ['MMSI', 'Segment', 'Timestamp', 'SOG', 'COG', 'Longtitude', 'Latitude', 'distance_m', 'speed_mps_track']\n",
      "\n",
      "Cleaned data saved to: data/ais_data_5min_clean.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: Final Dataset Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Unique vessels (MMSI): {df_clean['MMSI'].nunique()}\")\n",
    "print(f\"Total segments: {df_clean.groupby(['MMSI', 'Segment']).ngroups}\")\n",
    "print(f\"Average segment length: {df_clean.groupby(['MMSI', 'Segment']).size().mean()*INTERVAL:.1f} minutes\")\n",
    "print(f\"Columns: {list(df_clean.columns)}\")\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nCleaned data saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513a522",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a3028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for model training\n",
    "SEQUENCE_LENGTH = 20\n",
    "FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\"] \n",
    "TARGET_FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\"]\n",
    "MIN_SEGMENT_LENGTH = SEQUENCE_LENGTH + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6deb1147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences created: 44843\n",
      "Input shape: (44843, 20, 4)\n",
      "Target shape: (44843, 4)\n",
      "Segments used: 44843\n",
      "Average sequences per segment: 1.0\n",
      "\n",
      "Segment statistics:\n",
      "  Min length: 125 minutes\n",
      "  Max length: 1440 minutes\n",
      "  Mean length: 1149.0 minutes\n",
      "  Median length: 1370.0 minutes\n"
     ]
    }
   ],
   "source": [
    "#Prepare sequences\n",
    "X, y, segment_info = prepare_training_data(\n",
    "    df_clean,\n",
    "    SEQUENCE_LENGTH,\n",
    "    FEATURES,\n",
    "    TARGET_FEATURES,\n",
    "    MIN_SEGMENT_LENGTH\n",
    ")\n",
    "print(f\"Total sequences created: {len(X)}\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Segments used: {len(segment_info)}\")\n",
    "print(f\"Average sequences per segment: {len(X) / len(segment_info):.1f}\")\n",
    "\n",
    "# Display some statistics\n",
    "segment_lengths = [s['length'] for s in segment_info]\n",
    "print(f\"\\nSegment statistics:\")\n",
    "print(f\"  Min length: {min(segment_lengths)*INTERVAL} minutes\")\n",
    "print(f\"  Max length: {max(segment_lengths)*INTERVAL} minutes\")\n",
    "print(f\"  Mean length: {np.mean(segment_lengths)*INTERVAL:.1f} minutes\")\n",
    "print(f\"  Median length: {np.median(segment_lengths)*INTERVAL:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5bb5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Splitting Data by Ships (MMSI)\n",
      "============================================================\n",
      "Total unique ships: 279\n",
      "\n",
      "Ships in training set: 178 (63.8%)\n",
      "Ships in validation set: 45 (16.1%)\n",
      "Ships in test set: 56 (20.1%)\n",
      "\n",
      "Sequences in training set: 28586 (63.7%)\n",
      "Sequences in validation set: 7689 (17.1%)\n",
      "Sequences in test set: 8568 (19.1%)\n"
     ]
    }
   ],
   "source": [
    "# Split data by ships (MMSI)\n",
    "print(\"=\"*60)\n",
    "print(\"Splitting Data by Ships (MMSI)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get unique MMSIs from segment_info\n",
    "unique_mmsis = list(set([seg['mmsi'] for seg in segment_info]))\n",
    "n_ships = len(unique_mmsis)\n",
    "\n",
    "print(f\"Total unique ships: {n_ships}\")\n",
    "\n",
    "# Split ships into train (64%), val (16%), test (20%)\n",
    "mmsi_temp, mmsi_test = train_test_split(\n",
    "    unique_mmsis, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "mmsi_train, mmsi_val = train_test_split(\n",
    "    mmsi_temp, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nShips in training set: {len(mmsi_train)} ({len(mmsi_train)/n_ships*100:.1f}%)\")\n",
    "print(f\"Ships in validation set: {len(mmsi_val)} ({len(mmsi_val)/n_ships*100:.1f}%)\")\n",
    "print(f\"Ships in test set: {len(mmsi_test)} ({len(mmsi_test)/n_ships*100:.1f}%)\")\n",
    "\n",
    "# Create sets of MMSIs for fast lookup\n",
    "mmsi_train_set = set(mmsi_train)\n",
    "mmsi_val_set = set(mmsi_val)\n",
    "mmsi_test_set = set(mmsi_test)\n",
    "\n",
    "# Split sequences based on which ship they belong to\n",
    "train_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_train_set]\n",
    "val_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_val_set]\n",
    "test_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_test_set]\n",
    "\n",
    "# Get the actual sequences for each set (RAW)\n",
    "X_train_raw = X[train_indices]\n",
    "y_train_raw = y[train_indices]\n",
    "\n",
    "X_val_raw = X[val_indices]\n",
    "y_val_raw = y[val_indices]\n",
    "\n",
    "X_test_raw = X[test_indices]\n",
    "y_test_raw = y[test_indices]\n",
    "\n",
    "print(f\"\\nSequences in training set: {X_train_raw.shape[0]} ({X_train_raw.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Sequences in validation set: {X_val_raw.shape[0]} ({X_val_raw.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Sequences in test set: {X_test_raw.shape[0]} ({X_test_raw.shape[0]/X.shape[0]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93fec503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Normalizing Data\n",
      "============================================================\n",
      "Input data normalized\n",
      "Target data normalized\n",
      "\n",
      "Feature means: [ 56.00486371  11.13368651   3.58015288 174.77718504]\n",
      "Feature stds: [  1.09773542   2.05185728   2.78642971 106.00857452]\n",
      "\n",
      "Target means: [ 56.00520574  11.14744614   3.57007037 174.82785785]\n",
      "Target stds: [  1.09808968   2.05125326   2.78513621 105.8189964 ]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reshape X_train for normalization\n",
    "n_samples_train, n_timesteps, n_features = X_train_raw.shape\n",
    "X_train_reshaped = X_train_raw.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler_X = StandardScaler()\n",
    "X_train_normalized_reshaped = scaler_X.fit_transform(X_train_reshaped)\n",
    "X_train = X_train_normalized_reshaped.reshape(n_samples_train, n_timesteps, n_features)\n",
    "\n",
    "# Transform val and test\n",
    "X_val_reshaped = X_val_raw.reshape(-1, n_features)\n",
    "X_val = scaler_X.transform(X_val_reshaped).reshape(X_val_raw.shape[0], n_timesteps, n_features)\n",
    "\n",
    "X_test_reshaped = X_test_raw.reshape(-1, n_features)\n",
    "X_test = scaler_X.transform(X_test_reshaped).reshape(X_test_raw.shape[0], n_timesteps, n_features)\n",
    "\n",
    "# Normalize targets\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train_raw)\n",
    "y_val = scaler_y.transform(y_val_raw)\n",
    "y_test = scaler_y.transform(y_test_raw)\n",
    "\n",
    "print(f\"Input data normalized\")\n",
    "print(f\"Target data normalized\")\n",
    "print(f\"\\nFeature means: {scaler_X.mean_}\")\n",
    "print(f\"Feature stds: {scaler_X.scale_}\")\n",
    "print(f\"\\nTarget means: {scaler_y.mean_}\")\n",
    "print(f\"Target stds: {scaler_y.scale_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
