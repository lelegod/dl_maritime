{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9eb9c5",
   "metadata": {},
   "source": [
    "# Read clean , import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27781256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import create_mmsi_dict_from_file\n",
    "from utils import filter_stationary_ships\n",
    "from utils import prepare_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa80432",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85382793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_clean = pd.read_csv(\"data/ais_data_5min_clean.csv\")\n",
    "df_clean.head()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def encode_cog(df):\n",
    "    # Convert degrees → radians\n",
    "    rad = np.deg2rad(df[\"COG\"].values)\n",
    "\n",
    "    df[\"COG_sin\"] = np.sin(rad)\n",
    "    df[\"COG_cos\"] = np.cos(rad)\n",
    "\n",
    "    return df.drop(columns=[\"COG\"])\n",
    "\n",
    "df_clean = encode_cog(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf88706",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/mmsi_type.txt\"\n",
    "mmsi_map = create_mmsi_dict_from_file(file_name)\n",
    "\n",
    "\n",
    "if mmsi_map:\n",
    "    print(\"--- Successfully created dictionary ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb34120",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441db738",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for model training\n",
    "SEQUENCE_LENGTH = 20\n",
    "FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG_sin\", \"COG_cos\"] \n",
    "TARGET_FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG_sin\", \"COG_cos\"]\n",
    "MIN_SEGMENT_LENGTH = SEQUENCE_LENGTH + 5\n",
    "INTERVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare sequences\n",
    "X, y, segment_info = prepare_training_data(\n",
    "    df_clean,\n",
    "    SEQUENCE_LENGTH,\n",
    "    FEATURES,\n",
    "    TARGET_FEATURES,\n",
    "    MIN_SEGMENT_LENGTH\n",
    ")\n",
    "\n",
    "# Display some statistics\n",
    "segment_lengths = [s['length'] for s in segment_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by ships (MMSI)\n",
    "# Get unique MMSIs from segment_info\n",
    "unique_mmsis = list(set([seg['mmsi'] for seg in segment_info]))\n",
    "n_ships = len(unique_mmsis)\n",
    "\n",
    "# Split ships into train (64%), val (16%), test (20%)\n",
    "mmsi_temp, mmsi_test = train_test_split(\n",
    "    unique_mmsis, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "mmsi_train, mmsi_val = train_test_split(\n",
    "    mmsi_temp, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Create sets of MMSIs for fast lookup\n",
    "mmsi_train_set = set(mmsi_train)\n",
    "mmsi_val_set = set(mmsi_val)\n",
    "mmsi_test_set = set(mmsi_test)\n",
    "\n",
    "# Split sequences based on which ship they belong to\n",
    "train_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_train_set]\n",
    "val_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_val_set]\n",
    "test_indices = [i for i, seg in enumerate(segment_info) if seg['mmsi'] in mmsi_test_set]\n",
    "\n",
    "# Get the actual sequences for each set (RAW)\n",
    "X_train_raw = X[train_indices]\n",
    "y_train_raw = y[train_indices]\n",
    "\n",
    "X_val_raw = X[val_indices]\n",
    "y_val_raw = y[val_indices]\n",
    "\n",
    "X_test_raw = X[test_indices]\n",
    "y_test_raw = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc359842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reshape X_train for normalization\n",
    "n_samples_train, n_timesteps, n_features = X_train_raw.shape\n",
    "X_train_reshaped = X_train_raw.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler_X = StandardScaler()\n",
    "X_train_normalized_reshaped = scaler_X.fit_transform(X_train_reshaped)\n",
    "X_train = X_train_normalized_reshaped.reshape(n_samples_train, n_timesteps, n_features)\n",
    "\n",
    "# Transform val and test\n",
    "X_val_reshaped = X_val_raw.reshape(-1, n_features)\n",
    "X_val = scaler_X.transform(X_val_reshaped).reshape(X_val_raw.shape[0], n_timesteps, n_features)\n",
    "\n",
    "X_test_reshaped = X_test_raw.reshape(-1, n_features)\n",
    "X_test = scaler_X.transform(X_test_reshaped).reshape(X_test_raw.shape[0], n_timesteps, n_features)\n",
    "\n",
    "# Normalize targets\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train_raw)\n",
    "y_val = scaler_y.transform(y_val_raw)\n",
    "y_test = scaler_y.transform(y_test_raw)\n",
    "\n",
    "print(f\"Input data normalized\")\n",
    "print(f\"Target data normalized\")\n",
    "print(f\"\\nFeature means: {scaler_X.mean_}\")\n",
    "print(f\"Feature stds: {scaler_X.scale_}\")\n",
    "print(f\"\\nTarget means: {scaler_y.mean_}\")\n",
    "print(f\"Target stds: {scaler_y.scale_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05546f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delta features from df_clean\n",
    "print(\"=\"*60)\n",
    "print(\"Creating Delta Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_delta_features(df):\n",
    "    \"\"\"\n",
    "    Create delta (relative change) features from absolute positions.\n",
    "    \n",
    "    For each segment, compute:\n",
    "    - delta_lat = current_lat - previous_lat\n",
    "    - delta_lon = current_lon - previous_lon\n",
    "    - Keep SOG and COG as-is\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for (mmsi, seg), group in df.groupby([\"MMSI\", \"Segment\"]):\n",
    "        group = group.sort_values(\"Timestamp\").copy()\n",
    "        \n",
    "        # Calculate deltas\n",
    "        group[\"delta_lat\"] = group[\"Latitude\"].diff()\n",
    "        group[\"delta_lon\"] = group[\"Longtitude\"].diff()\n",
    "        \n",
    "        # First row will have NaN deltas, so we drop it\n",
    "        group = group.dropna(subset=[\"delta_lat\", \"delta_lon\"])\n",
    "        \n",
    "        results.append(group)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Create delta dataframe\n",
    "df_delta = create_delta_features(df_clean)\n",
    "\n",
    "print(f\"Original data shape: {df_clean.shape}\")\n",
    "print(f\"Delta data shape: {df_delta.shape}\")\n",
    "print(f\"\\nNew columns added: delta_lat, delta_lon\")\n",
    "print(f\"\\nDelta statistics:\")\n",
    "print(df_delta[[\"delta_lat\", \"delta_lon\", \"SOG\", \"COG_sin\", \"COG_cos\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679865ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Delta-Based GRU model with Absolute Position\n",
    "DELTA_SEQUENCE_LENGTH = 10  # Use 10 minutes of delta history\n",
    "# Add absolute position (Latitude, Longtitude) to help model know where it is\n",
    "DELTA_INPUT_FEATURES = [\"delta_lat\", \"delta_lon\", \"SOG\", \"COG_sin\", \"COG_cos\", \"Latitude\", \"Longtitude\"]\n",
    "DELTA_TARGET_FEATURES = [\"delta_lat\", \"delta_lon\", \"SOG\", \"COG_sin\", \"COG_cos\",]\n",
    "MIN_DELTA_SEGMENT_LENGTH = DELTA_SEQUENCE_LENGTH + 5\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Delta Model Configuration (with Absolute Position)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sequence length: {DELTA_SEQUENCE_LENGTH} minutes\")\n",
    "print(f\"Input features: {DELTA_INPUT_FEATURES}\")\n",
    "print(f\"  - Deltas: delta_lat, delta_lon (relative movement)\")\n",
    "print(f\"  - Dynamics: SOG, COG (speed and heading)\")\n",
    "print(f\"  - Position: Latitude, Longtitude (absolute location)\")\n",
    "print(f\"Target features: {DELTA_TARGET_FEATURES}\")\n",
    "print(f\"Minimum segment length: {MIN_DELTA_SEGMENT_LENGTH} minutes\")\n",
    "print(\"\\n✓ Model will now know both WHERE it is and HOW it's moving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare delta-based training data\n",
    "print(\"=\"*60)\n",
    "print(\"Preparing Delta-Based Training Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare sequences using delta features\n",
    "X_delta, y_delta, delta_segment_info = prepare_training_data(\n",
    "    df_delta, \n",
    "    DELTA_SEQUENCE_LENGTH, \n",
    "    DELTA_INPUT_FEATURES, \n",
    "    DELTA_TARGET_FEATURES, \n",
    "    MIN_DELTA_SEGMENT_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"Total delta sequences created: {len(X_delta)}\")\n",
    "print(f\"Input shape: {X_delta.shape}\")\n",
    "print(f\"Target shape: {y_delta.shape}\")\n",
    "print(f\"Segments used: {len(delta_segment_info)}\")\n",
    "print(f\"Average sequences per segment: {len(X_delta) / len(delta_segment_info):.1f}\")\n",
    "\n",
    "# Display statistics\n",
    "segment_lengths_delta = [s['length'] for s in delta_segment_info]\n",
    "print(f\"\\nSegment statistics:\")\n",
    "print(f\"  Min length: {min(segment_lengths_delta)} minutes\")\n",
    "print(f\"  Max length: {max(segment_lengths_delta)} minutes\")\n",
    "print(f\"  Mean length: {np.mean(segment_lengths_delta):.1f} minutes\")\n",
    "print(f\"  Median length: {np.median(segment_lengths_delta):.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef10c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize delta data\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing Delta Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reshape X for normalization\n",
    "n_samples_delta, n_timesteps_delta, n_features_delta = X_delta.shape\n",
    "X_delta_reshaped = X_delta.reshape(-1, n_features_delta)\n",
    "\n",
    "# Fit scaler on delta training data\n",
    "scaler_X_delta = StandardScaler()\n",
    "X_delta_normalized = scaler_X_delta.fit_transform(X_delta_reshaped)\n",
    "X_delta_normalized = X_delta_normalized.reshape(n_samples_delta, n_timesteps_delta, n_features_delta)\n",
    "\n",
    "# Normalize delta targets\n",
    "scaler_y_delta = StandardScaler()\n",
    "y_delta_normalized = scaler_y_delta.fit_transform(y_delta)\n",
    "\n",
    "print(f\"Input data normalized: {X_delta_normalized.shape}\")\n",
    "print(f\"Target data normalized: {y_delta_normalized.shape}\")\n",
    "print(f\"\\nFeature means: {scaler_X_delta.mean_}\")\n",
    "print(f\"Feature stds: {scaler_X_delta.scale_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f004e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split delta data by ships (MMSI)\n",
    "print(\"=\"*60)\n",
    "print(\"Splitting Delta Data by Ships (MMSI)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get unique MMSIs from delta segment_info\n",
    "unique_mmsis_delta = list(set([seg['mmsi'] for seg in delta_segment_info]))\n",
    "n_ships_delta = len(unique_mmsis_delta)\n",
    "\n",
    "print(f\"Total unique ships: {n_ships_delta}\")\n",
    "\n",
    "# Split ships: 64% train, 16% val, 20% test\n",
    "mmsi_temp_delta, mmsi_test_delta = train_test_split(\n",
    "    unique_mmsis_delta, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "mmsi_train_delta, mmsi_val_delta = train_test_split(\n",
    "    mmsi_temp_delta, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nShips in training set: {len(mmsi_train_delta)} ({len(mmsi_train_delta)/n_ships_delta*100:.1f}%)\")\n",
    "print(f\"Ships in validation set: {len(mmsi_val_delta)} ({len(mmsi_val_delta)/n_ships_delta*100:.1f}%)\")\n",
    "print(f\"Ships in test set: {len(mmsi_test_delta)} ({len(mmsi_test_delta)/n_ships_delta*100:.1f}%)\")\n",
    "\n",
    "# Create sets for fast lookup\n",
    "mmsi_train_set_delta = set(mmsi_train_delta)\n",
    "mmsi_val_set_delta = set(mmsi_val_delta)\n",
    "mmsi_test_set_delta = set(mmsi_test_delta)\n",
    "\n",
    "# Split sequences based on ship MMSI\n",
    "train_indices_delta = [i for i, seg in enumerate(delta_segment_info) if seg['mmsi'] in mmsi_train_set_delta]\n",
    "val_indices_delta = [i for i, seg in enumerate(delta_segment_info) if seg['mmsi'] in mmsi_val_set_delta]\n",
    "test_indices_delta = [i for i, seg in enumerate(delta_segment_info) if seg['mmsi'] in mmsi_test_set_delta]\n",
    "\n",
    "# Get actual sequences\n",
    "X_train_delta = X_delta_normalized[train_indices_delta]\n",
    "y_train_delta = y_delta_normalized[train_indices_delta]\n",
    "\n",
    "X_val_delta = X_delta_normalized[val_indices_delta]\n",
    "y_val_delta = y_delta_normalized[val_indices_delta]\n",
    "\n",
    "X_test_delta = X_delta_normalized[test_indices_delta]\n",
    "y_test_delta = y_delta_normalized[test_indices_delta]\n",
    "\n",
    "print(f\"\\nSequences in training set: {X_train_delta.shape[0]} ({X_train_delta.shape[0]/X_delta_normalized.shape[0]*100:.1f}%)\")\n",
    "print(f\"Sequences in validation set: {X_val_delta.shape[0]} ({X_val_delta.shape[0]/X_delta_normalized.shape[0]*100:.1f}%)\")\n",
    "print(f\"Sequences in test set: {X_test_delta.shape[0]} ({X_test_delta.shape[0]/X_delta_normalized.shape[0]*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ Delta data split by ships - no temporal leakage between sets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0389ab0",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ba901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_rnn_model(input_shape, output_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Stacked RNN layers\n",
    "    model.add(SimpleRNN(2*128, return_sequences=True, activation='tanh', input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(SimpleRNN(2*64, return_sequences=False, activation='tanh'))\n",
    "\n",
    "    # Dense layers for nonlinear mapping\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2*128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Final regression output\n",
    "    model.add(Dense(output_dim, kernel_regularizer=l2(1e-4)))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model \"\"\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_rnn_model(input_shape, output_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # --- Recurrent encoder ---\n",
    "    # Small, stable GRU for smooth motion\n",
    "    model.add(GRU(\n",
    "        units=32,\n",
    "        return_sequences=False,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "\n",
    "    # --- Lightweight feature mixing ---\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "    # --- Output ---\n",
    "    model.add(Dense(output_dim))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=5e-4),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0facb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_targets = y_train.shape[1]\n",
    "\n",
    "model = build_rnn_model(\n",
    "    input_shape=(n_timesteps, n_features),\n",
    "    output_dim=n_targets\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def autoregressive_predict(model, initial_window, steps):\n",
    "    \"\"\"\n",
    "    model: trained model with .predict()\n",
    "    initial_window: np.array shape (10, 5)\n",
    "    steps: how many future points to generate\n",
    "    \"\"\"\n",
    "    window = initial_window.copy()\n",
    "    outputs = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # Model expects shape (1, 10, 5) or (10,5) depending on architecture\n",
    "        pred = model.predict(window[np.newaxis, ...])  # shape (1,5)\n",
    "        outputs.append(pred.squeeze())\n",
    "        pred = scaler_y.inverse_transform(pred)\n",
    "        pred = scaler_X.transform(pred)\n",
    "        pred = pred.squeeze()  # shape (5,)\n",
    "\n",
    "        #outputs.append(pred)\n",
    "\n",
    "        # Slide the window: drop first row, append prediction\n",
    "        window = np.vstack([window[1:], pred])\n",
    "\n",
    "    return np.array(outputs)  # shape = (steps, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_xy_trajectory(original, preds):\n",
    "    \"\"\"\n",
    "    original: (10, 5)  → use [:,0] = X and [:,1] = Y\n",
    "    preds:    (N, 5)  → use [:,0] = X and [:,1] = Y\n",
    "    \"\"\"\n",
    "    ox, oy = original[:, 0], original[:, 1]\n",
    "    px, py = preds[:, 0], preds[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # Original trajectory\n",
    "    plt.plot(ox, oy, marker='o', label=\"Original (input window)\", linewidth=2)\n",
    "\n",
    "    # Predicted trajectory (autoregressive)\n",
    "    plt.plot(px, py, marker='x', label=\"Predicted (autoregressive)\", linestyle='--', linewidth=2)\n",
    "\n",
    "    # Mark start & end points\n",
    "    plt.scatter(ox[0], oy[0], c='green', s=80, label=\"Start\")\n",
    "    plt.scatter(px[-1], py[-1], c='red', s=80, label=\"Final prediction\")\n",
    "\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(\"XY Trajectory: Original Window vs Autoregressive Predictions\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"equal\")  # keep aspect ratio consistent\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_train[0][np.newaxis, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(X_train[0][i])\n",
    "print(\"~~~~~~~\")\n",
    "print(X_train[0])\n",
    "print(pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ff97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_steps = 10\n",
    "preds = autoregressive_predict(model, X_test[0], future_steps)\n",
    "print(preds.shape)   # (50, 5)\n",
    "\n",
    "plot_xy_trajectory(X_test[0], preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ff42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Model MAE Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368005b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_norm = model.predict(X_test)\n",
    "\n",
    "y_pred = scaler_y.inverse_transform(y_pred_norm)\n",
    "y_true = scaler_y.inverse_transform(y_test)\n",
    "X_true = scaler_X.inverse_transform(X_test[0])\n",
    "#print(X_true)\n",
    "#print(y_true[0])\n",
    "print(y_pred[0])\n",
    "\n",
    "true = np.vstack([X_true, y_true[0].reshape(1,-1)])\n",
    "print(\"AAAAAAAAAAAAAA\")\n",
    "print(true)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(true[:,1], true[:,0], label=\"Actual\", linewidth=2, marker=\"o\")\n",
    "plt.scatter(y_pred[0][1], y_pred[0][0], label=\"Predicted\", linestyle=\"--\", linewidth=2, color='red')\n",
    "\n",
    "# Styling\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Actual vs Predicted Trajectory\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef9c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSL Deep Learning project",
   "language": "python",
   "name": "deep_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
