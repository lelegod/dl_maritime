{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ship_trajectory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1763406207160,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "JvZQaY8hMkrZ",
    "outputId": "026e7594-42d2-4280-db9c-225835f240c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Œ≤Œ±œÉŒπŒ∫Œ≠œÇ œÅœÖŒ∏ŒºŒØœÉŒµŒπœÇ\n",
    "\n",
    "# Path Œ≥ŒπŒ± œÑŒø CSV œÉŒøœÖ\n",
    "CSV_PATH = \"data/ais_data_5min_clean.csv\"  # Œ¨ŒªŒªŒ±ŒæŒ≠ œÑŒø Œ±ŒΩ œáœÅŒµŒπŒ¨Œ∂ŒµœÑŒ±Œπ\n",
    "\n",
    "# Hyperparameters Œ≥ŒπŒ± œÑŒ± œÄŒ±œÅŒ¨Œ∏œÖœÅŒ±\n",
    "INPUT_LEN = 20     # œÄœåœÉŒ± ŒπœÉœÑŒøœÅŒπŒ∫Œ¨ œÉŒ∑ŒºŒµŒØŒ± Œ≤Œ¨Œ∂ŒøœÖŒºŒµ œÉœÑŒø input\n",
    "OUTPUT_LEN = 6     # œÄœåœÉŒ± ŒºŒµŒªŒªŒøŒΩœÑŒπŒ∫Œ¨ œÉŒ∑ŒºŒµŒØŒ± œÄœÅŒøŒ≤ŒªŒ≠œÄŒøœÖŒºŒµ\n",
    "\n",
    "# Split by MMSI\n",
    "TRAIN_FRAC = 0.7\n",
    "VAL_FRAC   = 0.15\n",
    "TEST_FRAC  = 0.15\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Model hyperparameters\n",
    "INPUT_DIM  = 6   # [Latitude, Longtitude, SOG, COG, speed_mps_track]\n",
    "OUTPUT_DIM = 5   # [Latitude, Longtitude]\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT    = 0.1\n",
    "BATCH_SIZE = 128\n",
    "LR         = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J--xwHIWM3Zb"
   },
   "outputs": [],
   "source": [
    "# Cell 2A: Split the original CSV into train/val/test CSVs by MMSI and save them\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_and_save_csv_by_mmsi(\n",
    "    full_csv_path: str,\n",
    "    train_csv_path: str,\n",
    "    val_csv_path: str,\n",
    "    test_csv_path: str,\n",
    "    train_frac: float = 0.7,\n",
    "    val_frac: float = 0.15,\n",
    "    test_frac: float = 0.15,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    df = pd.read_csv(full_csv_path)\n",
    "\n",
    "    # Example function to apply to your raw DataFrame before splitting/scaling\n",
    "    def apply_cog_encoding(df: pd.DataFrame):\n",
    "        # 1. Convert COG from degrees to radians\n",
    "        df['COG_rad'] = np.deg2rad(df['COG'])\n",
    "\n",
    "        # 2. Create the two new cyclical features\n",
    "        df['COG_sin'] = np.sin(df['COG_rad'])\n",
    "        df['COG_cos'] = np.cos(df['COG_rad'])\n",
    "\n",
    "        # 3. Remove the original COG and temporary radian columns\n",
    "        # Use errors='ignore' in case COG was already dropped in a previous run\n",
    "        df = df.drop(columns=['COG', 'COG_rad'], errors='ignore')\n",
    "        return df\n",
    "\n",
    "    # Ensure Timestamp is datetime\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "    \n",
    "    # <<<--- 1. INSERT THE FUNCTION CALL HERE --->>>\n",
    "    df = apply_cog_encoding(df)\n",
    "\n",
    "    # 2. UPDATE THE REQUIRED COLUMNS LIST (Remove 'COG', Add 'COG_sin', 'COG_cos')\n",
    "    required_cols = [\n",
    "        \"Timestamp\", \"MMSI\", \"SOG\", \n",
    "        \"Longtitude\", \"Latitude\",\n",
    "        \"COG_sin\", \"COG_cos\" # <-- New features for checking\n",
    "    ]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"WARNING: Missing columns:\", missing)\n",
    "\n",
    "    df = df.dropna(subset=[\"MMSI\", \"Timestamp\", \"Latitude\", \"Longtitude\"])\n",
    "\n",
    "    # Shuffle MMSIs and split\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    unique_mmsi = df[\"MMSI\"].unique()\n",
    "    rng.shuffle(unique_mmsi)\n",
    "\n",
    "    n = len(unique_mmsi)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val   = int(n * val_frac)\n",
    "\n",
    "    mmsi_train = unique_mmsi[:n_train]\n",
    "    mmsi_val   = unique_mmsi[n_train:n_train + n_val]\n",
    "    mmsi_test  = unique_mmsi[n_train + n_val:]\n",
    "\n",
    "    df_train = df[df[\"MMSI\"].isin(mmsi_train)].copy()\n",
    "    df_val   = df[df[\"MMSI\"].isin(mmsi_val)].copy()\n",
    "    df_test  = df[df[\"MMSI\"].isin(mmsi_test)].copy()\n",
    "\n",
    "    # Sort each split nicely\n",
    "    df_train = df_train.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "    df_val   = df_val.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "    df_test  = df_test.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "\n",
    "    # Create folder if needed\n",
    "    os.makedirs(os.path.dirname(train_csv_path) or \".\", exist_ok=True)\n",
    "\n",
    "    # These lines save the files with the new COG columns\n",
    "    df_train.to_csv(train_csv_path, index=False)\n",
    "    df_val.to_csv(val_csv_path,   index=False)\n",
    "    df_test.to_csv(test_csv_path, index=False)\n",
    "\n",
    "    print(f\"Saved train CSV to: {train_csv_path} (rows: {len(df_train)})\")\n",
    "    print(f\"Saved val   CSV to: {val_csv_path} (rows: {len(df_val)})\")\n",
    "    print(f\"Saved test  CSV to: {test_csv_path} (rows: {len(df_test)})\")\n",
    "    print(f\"# MMSI -> train: {len(mmsi_train)}, val: {len(mmsi_val)}, test: {len(mmsi_test)}\")\n",
    "\n",
    "    # Optionally, also save MMSI lists separately if you want\n",
    "    np.save(\"mmsi_train.npy\", mmsi_train)\n",
    "    np.save(\"mmsi_val.npy\",   mmsi_val)\n",
    "    np.save(\"mmsi_test.npy\",  mmsi_test)\n",
    "    print(\"Saved mmsi_train.npy, mmsi_val.npy, mmsi_test.npy\")\n",
    "\n",
    "    return df_train, df_val, df_test, mmsi_train, mmsi_val, mmsi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train CSV to: ais_train.csv (rows: 50033)\n",
      "Saved val   CSV to: ais_val.csv (rows: 9760)\n",
      "Saved test  CSV to: ais_test.csv (rows: 11528)\n",
      "# MMSI -> train: 198, val: 42, test: 43\n",
      "Saved mmsi_train.npy, mmsi_val.npy, mmsi_test.npy\n"
     ]
    }
   ],
   "source": [
    "# Choose output file names (you can adjust paths if you want)\n",
    "TRAIN_CSV = \"ais_train.csv\"\n",
    "VAL_CSV   = \"ais_val.csv\"\n",
    "TEST_CSV  = \"ais_test.csv\"\n",
    "\n",
    "df_train_raw, df_val_raw, df_test_raw, mmsi_train, mmsi_val, mmsi_test = split_and_save_csv_by_mmsi(\n",
    "    full_csv_path=CSV_PATH,      # your original full AIS CSV\n",
    "    train_csv_path=TRAIN_CSV,\n",
    "    val_csv_path=VAL_CSV,\n",
    "    test_csv_path=TEST_CSV,\n",
    "    train_frac=TRAIN_FRAC,\n",
    "    val_frac=VAL_FRAC,\n",
    "    test_frac=TEST_FRAC,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4217,
     "status": "ok",
     "timestamp": 1763406289201,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "oF1rW-d0NHw6",
    "outputId": "8fcc9f4b-0051-4a78-818d-9a807033ad80"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Prepare datasets from already-split train/val/test CSV files\n",
    "\n",
    "def prepare_datasets_from_split_csvs(\n",
    "    train_csv: str,\n",
    "    val_csv: str,\n",
    "    test_csv: str,\n",
    "    input_len: int = 20,\n",
    "    output_len: int = 5,\n",
    "):\n",
    "    # Load split CSVs\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_val   = pd.read_csv(val_csv)\n",
    "    df_test  = pd.read_csv(test_csv)\n",
    "\n",
    "    # Make sure Timestamp is datetime type\n",
    "    for df in [df_train, df_val, df_test]:\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "\n",
    "    # Sort them\n",
    "    df_train = df_train.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "    df_val   = df_val.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "    df_test  = df_test.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "\n",
    "# --- WITH THIS NEW LINE ---\n",
    "    feature_cols_in  = [\n",
    "        \"Latitude\", \n",
    "        \"Longtitude\", \n",
    "        \"SOG\", \n",
    "        \"speed_mps_track\", \n",
    "        \"COG_sin\",   # <--- New\n",
    "        \"COG_cos\"    # <--- New\n",
    "    ]\n",
    "    feature_cols_out = [\"Latitude\", \n",
    "    \"Longtitude\",\n",
    "    \"SOG\",          # <--- New output feature\n",
    "    \"COG_sin\",      # <--- New output feature\n",
    "    \"COG_cos\"]\n",
    "    cols_to_scale = list(set(feature_cols_in + feature_cols_out))\n",
    "\n",
    "    # ---- Fit scaler on TRAIN ONLY ----\n",
    "    means = {}\n",
    "    stds  = {}\n",
    "    for col in cols_to_scale:\n",
    "        m = df_train[col].mean()\n",
    "        s = df_train[col].std()\n",
    "        if s == 0 or np.isnan(s):\n",
    "            s = 1.0\n",
    "        means[col] = float(m)\n",
    "        stds[col]  = float(s)\n",
    "\n",
    "    scaler = {\"mean\": means, \"std\": stds}\n",
    "\n",
    "    def normalize_df(df_subset):\n",
    "        df_norm = df_subset.copy()\n",
    "        for col in cols_to_scale:\n",
    "            df_norm[col] = (df_norm[col] - means[col]) / stds[col]\n",
    "        return df_norm\n",
    "\n",
    "    df_train_n = normalize_df(df_train)\n",
    "    df_val_n   = normalize_df(df_val)\n",
    "    df_test_n  = normalize_df(df_test)\n",
    "\n",
    "    # ---- Build sliding-window sequences per (MMSI, Segment) ----\n",
    "    def build_sequences(df_subset_norm):\n",
    "        X_list, Y_list = [], []\n",
    "        total_len = input_len + output_len\n",
    "\n",
    "        for (mmsi, seg_id), seg_df in df_subset_norm.groupby([\"MMSI\", \"Segment\"]):\n",
    "            seg_values_in  = seg_df[feature_cols_in].to_numpy(dtype=np.float32)\n",
    "            seg_values_out = seg_df[feature_cols_out].to_numpy(dtype=np.float32)\n",
    "\n",
    "            n_pts = len(seg_df)\n",
    "            if n_pts < total_len:\n",
    "                continue\n",
    "\n",
    "            for start in range(0, n_pts - total_len + 1):\n",
    "                in_slice  = seg_values_in[start : start + input_len]              # (L, 5)\n",
    "                out_slice = seg_values_out[start + input_len : start + total_len] # (H, 2)\n",
    "                X_list.append(in_slice)\n",
    "                Y_list.append(out_slice)\n",
    "\n",
    "        if not X_list:\n",
    "            X_arr = np.empty((0, input_len, len(feature_cols_in)), dtype=np.float32)\n",
    "            Y_arr = np.empty((0, output_len, len(feature_cols_out)), dtype=np.float32)\n",
    "        else:\n",
    "            X_arr = np.stack(X_list, axis=0)\n",
    "            Y_arr = np.stack(Y_list, axis=0)\n",
    "\n",
    "        return X_arr, Y_arr\n",
    "\n",
    "    X_train, Y_train = build_sequences(df_train_n)\n",
    "    X_val,   Y_val   = build_sequences(df_val_n)\n",
    "    X_test,  Y_test  = build_sequences(df_test_n)\n",
    "\n",
    "    print(f\"Train samples: X{X_train.shape}, Y{Y_train.shape}\")\n",
    "    print(f\"Val   samples: X{X_val.shape},   Y{Y_val.shape}\")\n",
    "    print(f\"Test  samples: X{X_test.shape},  Y{Y_test.shape}\")\n",
    "\n",
    "    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: X(43883, 20, 6), Y(43883, 6, 5)\n",
      "Val   samples: X(8610, 20, 6),   Y(8610, 6, 5)\n",
      "Test  samples: X(10303, 20, 6),  Y(10303, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "# Use the three CSVs instead of the big original file\n",
    "\n",
    "(train_X, train_Y), (val_X, val_Y), (test_X, test_Y), scaler = prepare_datasets_from_split_csvs(\n",
    "    train_csv=TRAIN_CSV,\n",
    "    val_csv=VAL_CSV,\n",
    "    test_csv=TEST_CSV,\n",
    "    input_len=INPUT_LEN,\n",
    "    output_len=OUTPUT_LEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1763406303785,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "lUj0l91iNMem",
    "outputId": "8d307e76-47d6-49ff-bccc-32767f5d0c27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343, 68, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Dataset & DataLoaders\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "train_dataset = TrajectoryDataset(train_X, train_Y)\n",
    "val_dataset   = TrajectoryDataset(val_X,   val_Y)\n",
    "test_dataset  = TrajectoryDataset(test_X,  test_Y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OvwxzRx2NOO2"
   },
   "outputs": [],
   "source": [
    "# Cell 5: ŒüœÅŒπœÉŒºœåœÇ LSTM Encoder‚ÄìDecoder (Updated for INPUT_DIM=6, OUTPUT_DIM=5)\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMEncoderDecoder(nn.Module):\n",
    "    # input_dim=6 (Lat, Lon, SOG, speed, COG_sin, COG_cos)\n",
    "    # output_dim=5 (Lat, Lon, SOG, COG_sin, COG_cos)\n",
    "    def __init__(self, input_dim=6, output_dim=5,\n",
    "                 hidden_dim=128, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder must accept input_dim=6\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Decoder must accept input_dim=6 (The full input vector for the next step)\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Output FC layer produces output_dim=5 features (Lat, Lon, SOG, COG_sin, COG_cos)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: (B, L, 6)   -- Input sequence (History)\n",
    "        tgt: (B, H, 5)   -- Target sequence (True Future: Lat/Lon/SOG/COG_sin/COG_cos)\n",
    "        returns: (B, H, 5) -- Predicted sequence\n",
    "        \"\"\"\n",
    "        batch_size, H, out_dim = tgt.shape # out_dim = 5\n",
    "\n",
    "        # 1. Encode history\n",
    "        _, (h, c) = self.encoder(src)\n",
    "\n",
    "        # 2. First decoder input = full LAST historical vector\n",
    "        # dec_input must start as (B, 1, 6)\n",
    "        dec_input = src[:, -1, :].unsqueeze(1) \n",
    "\n",
    "        outputs = []\n",
    "        for t in range(H):\n",
    "            # Decoder step\n",
    "            dec_out, (h, c) = self.decoder(dec_input, (h, c))\n",
    "            step_output = self.fc_out(dec_out) # (B, 1, 5) -> Lat/Lon/SOG/sin/cos\n",
    "            outputs.append(step_output)\n",
    "\n",
    "            # Teacher forcing logic\n",
    "            if self.training and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                # Use ALL 5 ground truth features for the next step\n",
    "                next_input_features = tgt[:, t:t+1, :]   # (B, 1, 5) ground truth\n",
    "            else:\n",
    "                # Use ALL 5 predicted features for the next step\n",
    "                next_input_features = step_output        # (B, 1, 5) prediction\n",
    "\n",
    "            # 3. Update next input for decoder (Autoregressive Step)\n",
    "            # dec_input must maintain 6 features: Lat/Lon/SOG/speed_track/COG_sin/COG_cos\n",
    "            # The next input uses the PREDICTED/TRUE values for Lat/Lon/SOG/COG_sin/COG_cos (5 features)\n",
    "            # and keeps the speed_mps_track feature from the previous time step.\n",
    "            \n",
    "            dec_input = dec_input.clone()        # (B, 1, 6)\n",
    "            \n",
    "            # Update the first 3 elements (Lat, Lon, SOG)\n",
    "            dec_input[:, :, :3] = next_input_features[:, :, :3] # Update Lat, Lon, SOG\n",
    "            \n",
    "            # Update the last 2 elements (COG_sin, COG_cos)\n",
    "            dec_input[:, :, 4:6] = next_input_features[:, :, 3:5] # Update COG_sin/cos (skipping index 3, which is speed_mps_track)\n",
    "\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (B, H, 5)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10861,
     "status": "ok",
     "timestamp": 1763406328283,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "Nsch-JbzNP_m",
    "outputId": "2c62fcb5-5474-42a0-cd54-c117d3fa8d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMEncoderDecoder(\n",
      "  (encoder): LSTM(6, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (decoder): LSTM(6, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (fc_out): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ, loss & optimizer\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. DEFINE THE HAVERSINE LOSS CLASS\n",
    "# ----------------------------------------------------\n",
    "class HaversineLoss(torch.nn.Module):\n",
    "    def __init__(self, lat_mean, lat_std, lon_mean, lon_std, R=6371000.0):\n",
    "        super().__init__()\n",
    "        # Store denormalization constants and Earth radius (R)\n",
    "        # Use torch.tensor for constants that might be used in calculations\n",
    "        self.lat_mean = torch.tensor(lat_mean, dtype=torch.float32)\n",
    "        self.lat_std  = torch.tensor(lat_std, dtype=torch.float32)\n",
    "        self.lon_mean = torch.tensor(lon_mean, dtype=torch.float32)\n",
    "        self.lon_std  = torch.tensor(lon_std, dtype=torch.float32)\n",
    "        self.R = R\n",
    "        # Use nn.L1Loss as a base to aggregate the distances (Mean Absolute Error)\n",
    "        self.agg_loss = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def denormalize(self, norm_lat, norm_lon):\n",
    "        # Denormalize back to degrees and move to the device of the input tensor\n",
    "        device = norm_lat.device\n",
    "        lat = norm_lat * self.lat_std.to(device) + self.lat_mean.to(device)\n",
    "        lon = norm_lon * self.lon_std.to(device) + self.lon_mean.to(device)\n",
    "        # Convert to radians for Haversine formula\n",
    "        return torch.deg2rad(lat), torch.deg2rad(lon)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Calculates the Haversine distance between prediction and target.\n",
    "        pred, target shape: (B, H, 2), where 2 is (normalized_lat, normalized_lon)\n",
    "        \"\"\"\n",
    "        # --- 1. Denormalize and convert to radians ---\n",
    "        pred_lat_rad, pred_lon_rad = self.denormalize(pred[..., 0], pred[..., 1])\n",
    "        true_lat_rad, true_lon_rad = self.denormalize(target[..., 0], target[..., 1])\n",
    "\n",
    "        # --- 2. Haversine Formula ---\n",
    "        dlat = true_lat_rad - pred_lat_rad\n",
    "        dlon = true_lon_rad - pred_lon_rad\n",
    "\n",
    "        a = torch.sin(dlat / 2.0)**2 + \\\n",
    "            torch.cos(pred_lat_rad) * torch.cos(true_lat_rad) * torch.sin(dlon / 2.0)**2\n",
    "\n",
    "        # Clamp 'a' to [0, 1] to prevent issues with arcsin(sqrt(a))\n",
    "        a = torch.clamp(a, 0.0, 1.0)\n",
    "        \n",
    "        c = 2.0 * torch.asin(torch.sqrt(a))\n",
    "        \n",
    "        # Distance in meters\n",
    "        distance_m = self.R * c\n",
    "\n",
    "        # --- 3. Aggregate Loss (Mean Absolute Error of Distance) ---\n",
    "        return self.agg_loss(distance_m, torch.zeros_like(distance_m))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. MODEL AND LOSS INSTANTIATION\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# (Model definition using the class defined in Cell 5)\n",
    "model = LSTMEncoderDecoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Extract denormalization constants from your already calculated scaler (from Cell 6/7)\n",
    "lat_mean = scaler[\"mean\"][\"Latitude\"]\n",
    "lat_std  = scaler[\"std\"][\"Latitude\"]\n",
    "lon_mean = scaler[\"mean\"][\"Longtitude\"]\n",
    "lon_std  = scaler[\"std\"][\"Longtitude\"]\n",
    "\n",
    "# Instantiate the Haversine Loss, passing the necessary scaling constants\n",
    "criterion = HaversineLoss(\n",
    "    lat_mean=lat_mean,\n",
    "    lat_std=lat_std,\n",
    "    lon_mean=lon_mean,\n",
    "    lon_std=lon_std,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2542276,
     "status": "ok",
     "timestamp": 1763415523105,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "hANSh_l_NRTe",
    "outputId": "f3c29d24-1683-498e-f279-c12dd6438d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• New best model saved to data/lstm_best_with_5min.pth! Val Haversine Loss (m) = 3393.0751\n",
      "Epoch 1/10 - Train Haversine Loss (m): 14837.7910 - Val Haversine Loss (m): 3393.0751 (Best Val: 3393.0751)\n",
      "Epoch 2/10 - Train Haversine Loss (m): 4253.8784 - Val Haversine Loss (m): 4288.7049 (Best Val: 3393.0751)\n",
      "üî• New best model saved to data/lstm_best_with_5min.pth! Val Haversine Loss (m) = 2111.6975\n",
      "Epoch 3/10 - Train Haversine Loss (m): 3928.2044 - Val Haversine Loss (m): 2111.6975 (Best Val: 2111.6975)\n",
      "Epoch 4/10 - Train Haversine Loss (m): 3775.1920 - Val Haversine Loss (m): 3001.3424 (Best Val: 2111.6975)\n",
      "Epoch 5/10 - Train Haversine Loss (m): 3501.9388 - Val Haversine Loss (m): 2244.0617 (Best Val: 2111.6975)\n",
      "Epoch 6/10 - Train Haversine Loss (m): 3257.3103 - Val Haversine Loss (m): 2872.4044 (Best Val: 2111.6975)\n",
      "Epoch 7/10 - Train Haversine Loss (m): 3306.2424 - Val Haversine Loss (m): 3227.5457 (Best Val: 2111.6975)\n",
      "üî• New best model saved to data/lstm_best_with_5min.pth! Val Haversine Loss (m) = 1792.7237\n",
      "Epoch 8/10 - Train Haversine Loss (m): 2989.4450 - Val Haversine Loss (m): 1792.7237 (Best Val: 1792.7237)\n",
      "Epoch 9/10 - Train Haversine Loss (m): 2951.0232 - Val Haversine Loss (m): 2668.5473 (Best Val: 1792.7237)\n",
      "Epoch 10/10 - Train Haversine Loss (m): 2942.9964 - Val Haversine Loss (m): 2791.7894 (Best Val: 1792.7237)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "NUM_EPOCHS = 10 # run for 10 epochs\n",
    "\n",
    "# üëâ Change this to whatever path/filename you want:\n",
    "# e.g. \"/content/drive/MyDrive/deep learning/models/lstm_best.pth\"\n",
    "MODEL_PATH = \"data/lstm_best_with_5min.pth\"\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# TRAINING LOOP WITH BEST-MODEL SAVING\n",
    "# --------------------------\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  # ----- TRAIN -----\n",
    "  model.train()\n",
    "  total_train_loss = 0.0\n",
    "\n",
    "  for X_batch, Y_batch in train_loader:\n",
    "    X_batch = X_batch.to(device)\n",
    "    Y_batch = Y_batch.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(X_batch, Y_batch, teacher_forcing_ratio=0.5)\n",
    "    loss = criterion(preds, Y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "  avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "  # ----- VALIDATION -----\n",
    "  model.eval()\n",
    "  total_val_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X_batch, Y_batch in val_loader:\n",
    "      X_batch = X_batch.to(device)\n",
    "      Y_batch = Y_batch.to(device)\n",
    "\n",
    "      preds = model(X_batch, Y_batch, teacher_forcing_ratio=0.0)\n",
    "      loss = criterion(preds, Y_batch)\n",
    "      total_val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "  avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "  # ----- SAVE BEST MODEL -----\n",
    "  if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "\n",
    "    torch.save({\n",
    "      \"model_state_dict\": model.state_dict(),\n",
    "      \"scaler\": scaler,\n",
    "      \"input_len\": INPUT_LEN,\n",
    "      \"output_len\": OUTPUT_LEN,\n",
    "    }, MODEL_PATH)\n",
    "\n",
    "    # CHANGE 1: Update print statement to Haversine Loss (m)\n",
    "    print(f\"üî• New best model saved to {MODEL_PATH}! Val Haversine Loss (m) = {best_val_loss:.4f}\")\n",
    "\n",
    "  # ----- PRINT EPOCH RESULTS -----\n",
    "  # CHANGE 2: Update print statement labels and formatting\n",
    "  print(\n",
    "    f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
    "    f\"Train Haversine Loss (m): {avg_train_loss:.4f} - Val Haversine Loss (m): {avg_val_loss:.4f} \"\n",
    "    f\"(Best Val: {best_val_loss:.4f})\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load best model from file\n",
    "checkpoint = torch.load(\n",
    "    \"data/lstm_best_with_5min.pth\",\n",
    "    map_location=device,\n",
    "    weights_only=False,  # <-- add this\n",
    ")\n",
    "\n",
    "loaded_model = LSTMEncoderDecoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_model.eval()\n",
    "\n",
    "loaded_scaler = checkpoint[\"scaler\"]\n",
    "loaded_input_len  = checkpoint[\"input_len\"]\n",
    "loaded_output_len = checkpoint[\"output_len\"]\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Haversine Loss (Overall Mean): 2226.3507 meters\n",
      "Mean Haversine distance per prediction step (meters): [1672.5104 1690.351  1869.1471 2200.091  2664.0513 3261.961 ]\n",
      "Overall mean Haversine distance (meters): 2226.3507\n"
     ]
    }
   ],
   "source": [
    "def haversine_m(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# --------- EVALUATE LOADED MODEL ---------\n",
    "loaded_model.eval()\n",
    "\n",
    "# Retrieve scaling constants\n",
    "lat_mean = loaded_scaler[\"mean\"][\"Latitude\"]\n",
    "lat_std  = loaded_scaler[\"std\"][\"Latitude\"]\n",
    "lon_mean = loaded_scaler[\"mean\"][\"Longtitude\"]\n",
    "lon_std  = loaded_scaler[\"std\"][\"Longtitude\"]\n",
    "\n",
    "all_distances = []\n",
    "all_loss_value = 0.0 # Renamed from all_mse\n",
    "n_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "\n",
    "        preds = loaded_model(X_batch, Y_batch, teacher_forcing_ratio=0.0)\n",
    "\n",
    "        # The loss is now the mean Haversine distance in meters\n",
    "        loss = criterion(preds, Y_batch)\n",
    "        all_loss_value += loss.item() * X_batch.size(0)\n",
    "        n_samples += X_batch.size(0)\n",
    "\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        true_np  = Y_batch.cpu().numpy()\n",
    "\n",
    "        # Denormalize (Still needed for the per-step distance breakdown)\n",
    "        true_lat = true_np[..., 0] * lat_std + lat_mean\n",
    "        true_lon = true_np[..., 1] * lon_std + lon_mean\n",
    "        pred_lat = preds_np[..., 0] * lat_std + lat_mean\n",
    "        pred_lon = preds_np[..., 1] * lon_std + lon_mean\n",
    "\n",
    "        # Compute Haversine distance\n",
    "        d = haversine_m(true_lat, true_lon, pred_lat, pred_lon)\n",
    "        all_distances.append(d)\n",
    "\n",
    "all_distances = np.concatenate(all_distances, axis=0)\n",
    "mean_per_step = all_distances.mean(axis=0)\n",
    "overall_mean_haversine = all_loss_value / n_samples # This is the same as all_distances.mean()\n",
    "\n",
    "print(f\"Test Haversine Loss (Overall Mean): {overall_mean_haversine:.4f} meters\")\n",
    "print(\"Mean Haversine distance per prediction step (meters):\", mean_per_step)\n",
    "print(f\"Overall mean Haversine distance (meters): {overall_mean_haversine:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "FEATURES = [\n",
    "    \"Latitude\", \n",
    "    \"Longtitude\", \n",
    "    \"SOG\", \n",
    "    \"speed_mps_track\", \n",
    "    \"COG_sin\",     # <--- Replaces COG\n",
    "    \"COG_cos\"      # <--- New cyclical component\n",
    "]\n",
    "\n",
    "def inverse_transform_array(X, scaler):\n",
    "    X = np.asarray(X)\n",
    "    mean = np.array([scaler[\"mean\"][f] for f in FEATURES], dtype=float)\n",
    "    std  = np.array([scaler[\"std\"][f]  for f in FEATURES], dtype=float)\n",
    "    return X * std + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import itertools\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# NOTE: FEATURES list must be correct (6 features)\n",
    "FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"speed_mps_track\", \"COG_sin\", \"COG_cos\"]\n",
    "LAT_IDX = FEATURES.index(\"Latitude\")\n",
    "LON_IDX = FEATURES.index(\"Longtitude\")\n",
    "\n",
    "\n",
    "def inverse_transform_array(X, scaler):\n",
    "    X = np.asarray(X)\n",
    "    mean = np.array([scaler[\"mean\"][f] for f in FEATURES], dtype=float)\n",
    "    std  = np.array([scaler[\"std\"][f]  for f in FEATURES], dtype=float)\n",
    "    return X * std + mean\n",
    "\n",
    "\n",
    "def plot_predicted_vs_true_folium(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    scaler,\n",
    "    batch_idx=0,\n",
    "    sample_idx=0,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    PLOTS on Folium:\n",
    "      - Predicted path (History + Prediction) in RED.\n",
    "      - True path (History + True Future) in GREEN (dashed).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # -------- 1. Get one batch from test_loader and Denormalize --------\n",
    "    batch = next(itertools.islice(test_loader, batch_idx, batch_idx + 1))\n",
    "    X_batch, Y_batch = batch\n",
    "\n",
    "    X_batch = X_batch.to(device)\n",
    "    Y_batch = Y_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_batch, Y_batch, teacher_forcing_ratio=0.0)\n",
    "\n",
    "    X_sample = X_batch[sample_idx].cpu().numpy() # (L, 6)\n",
    "    Y_true_sample = Y_batch[sample_idx].cpu().numpy() # (H, 5)\n",
    "    Y_pred_sample = preds[sample_idx].cpu().numpy() # (H, 5)\n",
    "\n",
    "\n",
    "    # -------- 2. DENORMALIZATION LOGIC (FIXED) --------\n",
    "    \n",
    "    # Denormalize history (Always size 6)\n",
    "    hist_denorm = inverse_transform_array(X_sample, scaler)\n",
    "    hist_lat = hist_denorm[:, LAT_IDX]\n",
    "    hist_lon = hist_denorm[:, LON_IDX]\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # Case 1: Output is 5 features (Multi-Task Learning)\n",
    "    # -----------------------------------------------------------------\n",
    "    if Y_true_sample.shape[1] == 5: \n",
    "        \n",
    "        # Denormalize future ground truth (We only need Lat/Lon, indices 0, 1)\n",
    "        # Create a temporary 6-column array (Lat, Lon, SOG, 0, sin, cos) for denorm lookup\n",
    "        true_stack = np.zeros((Y_true_sample.shape[0], len(FEATURES)))\n",
    "        # Map Y_true features (size 5) into the 6-column stack:\n",
    "        true_stack[:, [0, 1, 2, 4, 5]] = Y_true_sample[:, [0, 1, 2, 3, 4]]\n",
    "        true_denorm = inverse_transform_array(true_stack, scaler)\n",
    "        true_lat = true_denorm[:, LAT_IDX]\n",
    "        true_lon = true_denorm[:, LON_IDX]\n",
    "\n",
    "        # Denormalize future predictions\n",
    "        pred_stack = np.zeros((Y_pred_sample.shape[0], len(FEATURES)))\n",
    "        # Map prediction features (size 5) into the 6-column stack\n",
    "        pred_stack[:, [0, 1, 2, 4, 5]] = Y_pred_sample[:, [0, 1, 2, 3, 4]]\n",
    "        pred_denorm = inverse_transform_array(pred_stack, scaler)\n",
    "        pred_lat = pred_denorm[:, LAT_IDX]\n",
    "        pred_lon = pred_denorm[:, LON_IDX]\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Case 2: Output is 2 features (Position-Only Learning) - Fallback from original code\n",
    "    # -----------------------------------------------------------------\n",
    "    elif Y_true_sample.shape[1] == 2:\n",
    "        \n",
    "        # Denormalize future ground truth\n",
    "        true_stack = np.zeros((Y_true_sample.shape[0], len(FEATURES)))\n",
    "        true_stack[:, :2] = Y_true_sample  \n",
    "        true_denorm = inverse_transform_array(true_stack, scaler)\n",
    "        true_lat = true_denorm[:, LAT_IDX]\n",
    "        true_lon = true_denorm[:, LON_IDX]\n",
    "\n",
    "        # Denormalize future predictions\n",
    "        pred_stack = np.zeros((Y_pred_sample.shape[0], len(FEATURES)))\n",
    "        pred_stack[:, :2] = Y_pred_sample\n",
    "        pred_denorm = inverse_transform_array(pred_stack, scaler)\n",
    "        pred_lat = pred_denorm[:, LAT_IDX]\n",
    "        pred_lon = pred_denorm[:, LON_IDX]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected Y shape {Y_true_sample.shape}; Cannot denormalize.\")\n",
    "\n",
    "\n",
    "    # -------- 3. Center of map --------\n",
    "    all_lats = np.concatenate([hist_lat, true_lat, pred_lat])\n",
    "    all_lons = np.concatenate([hist_lon, true_lon, pred_lon])\n",
    "\n",
    "    center_lat = float(all_lats.mean())\n",
    "    center_lon = float(all_lons.mean())\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=9)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # *** FULL CONTINUOUS TRAJECTORY PLOTTING ***\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    # Combine History (Input Sequence) and Predicted Future (Output Sequence)\n",
    "    full_predicted_lat = np.concatenate([hist_lat, pred_lat])\n",
    "    full_predicted_lon = np.concatenate([hist_lon, pred_lon])\n",
    "\n",
    "    # Combine History (Input Sequence) and True Future (Ground Truth)\n",
    "    full_true_lat = np.concatenate([hist_lat, true_lat])\n",
    "    full_true_lon = np.concatenate([hist_lon, true_lon])\n",
    "    \n",
    "    \n",
    "    # -------- 5. Plot True Full Trajectory (GREEN, dashed) --------\n",
    "    folium.PolyLine(\n",
    "        list(zip(full_true_lat, full_true_lon)),\n",
    "        popup=\"True Full Trajectory\",\n",
    "        tooltip=\"True Full Trajectory (History + True Future)\",\n",
    "        color='green', \n",
    "        weight=4,\n",
    "        dash_array='5, 5' \n",
    "    ).add_to(m)\n",
    "    # Mark the final true point\n",
    "    folium.CircleMarker(\n",
    "        location=[true_lat[-1], true_lon[-1]],\n",
    "        radius=5,\n",
    "        color='green',\n",
    "        fill=True,\n",
    "        fill_color='green',\n",
    "        tooltip='True Final Position'\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "    # -------- 6. Plot Predicted Full Trajectory (RED, solid) --------\n",
    "    folium.PolyLine(\n",
    "        list(zip(full_predicted_lat, full_predicted_lon)),\n",
    "        popup=\"Predicted Full Trajectory\",\n",
    "        tooltip=\"Predicted Full Trajectory (History + Prediction)\",\n",
    "        color='red', \n",
    "        weight=4,\n",
    "    ).add_to(m)\n",
    "    # Mark the final predicted point\n",
    "    folium.CircleMarker(\n",
    "        location=[pred_lat[-1], pred_lon[-1]],\n",
    "        radius=5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        tooltip='Predicted Final Position'\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Mark the divergence point (where History ends / Prediction begins)\n",
    "    folium.CircleMarker(\n",
    "        location=[hist_lat[-1], hist_lon[-1]],\n",
    "        radius=5,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "        fill_color='yellow',\n",
    "        tooltip='Prediction Divergence Point'\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "    if save_path is not None:\n",
    "        m.save(save_path)\n",
    "        print(f\"Map saved to {save_path}\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to pred_vs_true_with_5min.html\n"
     ]
    }
   ],
   "source": [
    "m = plot_predicted_vs_true_folium(\n",
    "    model=loaded_model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    scaler=loaded_scaler,\n",
    "    batch_idx=0,\n",
    "    sample_idx=0,\n",
    "    save_path=\"pred_vs_true_with_5min.html\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "COMPREHENSIVE TEST METRICS\n",
      "========================================\n",
      "1. Mean Haversine Distance: 2226.3507 meters\n",
      "2. Haversine RMSE: 2770.6143 meters\n",
      "3. MAE Latitude: 0.016704 degrees\n",
      "4. MAE Longitude: 0.013238 degrees\n",
      "5. MAE SOG (Speed Error): 1.6683 units\n",
      "6. Directional Vector MSE (Turn-Rate Proxy): 1.555829\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# These variables (all_distances, true_np, preds_np, loaded_scaler, overall_mean_haversine)\n",
    "# must be available from the previous evaluation cell (Cell 71).\n",
    "\n",
    "# --- 1. Retrieve necessary scaling constants ---\n",
    "lat_mean = loaded_scaler[\"mean\"][\"Latitude\"]\n",
    "lat_std = loaded_scaler[\"std\"][\"Latitude\"]\n",
    "lon_mean = loaded_scaler[\"mean\"][\"Longtitude\"]\n",
    "lon_std = loaded_scaler[\"std\"][\"Longtitude\"]\n",
    "sog_mean = loaded_scaler[\"mean\"][\"SOG\"]\n",
    "sog_std = loaded_scaler[\"std\"][\"SOG\"]\n",
    "\n",
    "# --- 2. Denormalize SOG and Lat/Lon ---\n",
    "# Denormalize Lat/Lon (indices 0 and 1)\n",
    "true_lat = true_np[..., 0] * lat_std + lat_mean\n",
    "pred_lat = preds_np[..., 0] * lat_std + lat_mean\n",
    "true_lon = true_np[..., 1] * lon_std + lon_mean\n",
    "pred_lon = preds_np[..., 1] * lon_std + lon_mean\n",
    "\n",
    "# Denormalize SOG (index 2)\n",
    "true_sog = true_np[..., 2] * sog_std + sog_mean\n",
    "pred_sog = preds_np[..., 2] * sog_std + sog_mean\n",
    "\n",
    "\n",
    "# --- 3. CALCULATE METRICS ---\n",
    "\n",
    "# A. Haversine RMSE (Root Mean Square Error)\n",
    "haversine_rmse = np.sqrt(np.mean(all_distances**2))\n",
    "\n",
    "# B. MAE Latitude / MAE Longitude (in degrees)\n",
    "mae_lat = np.mean(np.abs(true_lat - pred_lat))\n",
    "mae_lon = np.mean(np.abs(true_lon - pred_lon))\n",
    "\n",
    "# C. Speed Error (Mean Absolute Error on SOG, in physical units)\n",
    "mae_sog = np.mean(np.abs(true_sog - pred_sog))\n",
    "\n",
    "# D. Directional Vector Error (MSE on COG_sin/cos vector)\n",
    "# This serves as the proxy for Turn-Rate Error\n",
    "# COG_sin/cos are at indices 3 and 4 in the 5-feature output array\n",
    "true_dir = true_np[..., 3:5]\n",
    "pred_dir = preds_np[..., 3:5]\n",
    "dir_error_mse = np.mean((true_dir - pred_dir)**2)\n",
    "\n",
    "\n",
    "# --- 4. PRINT RESULTS ---\n",
    "print(\"=\" * 40)\n",
    "print(\"COMPREHENSIVE TEST METRICS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"1. Mean Haversine Distance: {overall_mean_haversine:.4f} meters\")\n",
    "print(f\"2. Haversine RMSE: {haversine_rmse:.4f} meters\")\n",
    "print(f\"3. MAE Latitude: {mae_lat:.6f} degrees\")\n",
    "print(f\"4. MAE Longitude: {mae_lon:.6f} degrees\")\n",
    "print(f\"5. MAE SOG (Speed Error): {mae_sog:.4f} units\")\n",
    "print(f\"6. Directional Vector MSE (Turn-Rate Proxy): {dir_error_mse:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP14I89y8jKLLqOdKGvbouS",
   "gpuType": "T4",
   "mount_file_id": "1WCdOogw_O6YHZnF2m-xCkhyBIHJBID-K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
