{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ship_trajectory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1763406207160,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "JvZQaY8hMkrZ",
    "outputId": "026e7594-42d2-4280-db9c-225835f240c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚\n",
    "\n",
    "# Path Î³Î¹Î± Ï„Î¿ CSV ÏƒÎ¿Ï…\n",
    "CSV_PATH = \"data/ais_data_final.csv\"  # Î¬Î»Î»Î±Î¾Î­ Ï„Î¿ Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹\n",
    "\n",
    "# Hyperparameters Î³Î¹Î± Ï„Î± Ï€Î±ÏÎ¬Î¸Ï…ÏÎ±\n",
    "INPUT_LEN = 20     # Ï€ÏŒÏƒÎ± Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ ÏƒÎ·Î¼ÎµÎ¯Î± Î²Î¬Î¶Î¿Ï…Î¼Îµ ÏƒÏ„Î¿ input\n",
    "OUTPUT_LEN = 5     # Ï€ÏŒÏƒÎ± Î¼ÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ¬ ÏƒÎ·Î¼ÎµÎ¯Î± Ï€ÏÎ¿Î²Î»Î­Ï€Î¿Ï…Î¼Îµ\n",
    "\n",
    "# Split by MMSI\n",
    "TRAIN_FRAC = 0.7\n",
    "VAL_FRAC   = 0.15\n",
    "TEST_FRAC  = 0.15\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Model hyperparameters\n",
    "INPUT_DIM  = 5   # [Latitude, Longtitude, SOG, COG, speed_mps_track]\n",
    "OUTPUT_DIM = 2   # [Latitude, Longtitude]\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT    = 0.1\n",
    "BATCH_SIZE = 128\n",
    "LR         = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J--xwHIWM3Zb"
   },
   "outputs": [],
   "source": [
    "# Cell 2A: Split the original CSV into train/val/test CSVs by MMSI and save them\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_and_save_csv_by_mmsi(\n",
    "    full_csv_path: str,\n",
    "    train_csv_path: str,\n",
    "    val_csv_path: str,\n",
    "    test_csv_path: str,\n",
    "    train_frac: float = 0.7,\n",
    "    val_frac: float = 0.15,\n",
    "    test_frac: float = 0.15,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    df = pd.read_csv(full_csv_path)\n",
    "\n",
    "    # Ensure Timestamp is datetime\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "\n",
    "    # Basic required columns\n",
    "    required_cols = [\n",
    "        \"Timestamp\", \"MMSI\", \"SOG\", \"COG\",\n",
    "        \"Longtitude\", \"Latitude\"\n",
    "    ]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"WARNING: Missing columns:\", missing)\n",
    "\n",
    "    df = df.dropna(subset=[\"MMSI\", \"Timestamp\", \"Latitude\", \"Longtitude\"])\n",
    "\n",
    "    # Shuffle MMSIs and split\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    unique_mmsi = df[\"MMSI\"].unique()\n",
    "    rng.shuffle(unique_mmsi)\n",
    "\n",
    "    n = len(unique_mmsi)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val   = int(n * val_frac)\n",
    "\n",
    "    mmsi_train = unique_mmsi[:n_train]\n",
    "    mmsi_val   = unique_mmsi[n_train:n_train + n_val]\n",
    "    mmsi_test  = unique_mmsi[n_train + n_val:]\n",
    "\n",
    "    df_train = df[df[\"MMSI\"].isin(mmsi_train)].copy()\n",
    "    df_val   = df[df[\"MMSI\"].isin(mmsi_val)].copy()\n",
    "    df_test  = df[df[\"MMSI\"].isin(mmsi_test)].copy()\n",
    "\n",
    "    # Sort each split nicely\n",
    "    df_train = df_train.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "    df_val   = df_val.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "    df_test  = df_test.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "\n",
    "    # Create folder if needed\n",
    "    os.makedirs(os.path.dirname(train_csv_path) or \".\", exist_ok=True)\n",
    "\n",
    "    df_train.to_csv(train_csv_path, index=False)\n",
    "    df_val.to_csv(val_csv_path,   index=False)\n",
    "    df_test.to_csv(test_csv_path, index=False)\n",
    "\n",
    "    print(f\"Saved train CSV to: {train_csv_path} (rows: {len(df_train)})\")\n",
    "    print(f\"Saved val   CSV to: {val_csv_path} (rows: {len(df_val)})\")\n",
    "    print(f\"Saved test  CSV to: {test_csv_path} (rows: {len(df_test)})\")\n",
    "    print(f\"# MMSI -> train: {len(mmsi_train)}, val: {len(mmsi_val)}, test: {len(mmsi_test)}\")\n",
    "\n",
    "    # Optionally, also save MMSI lists separately if you want\n",
    "    np.save(\"mmsi_train.npy\", mmsi_train)\n",
    "    np.save(\"mmsi_val.npy\",   mmsi_val)\n",
    "    np.save(\"mmsi_test.npy\",  mmsi_test)\n",
    "    print(\"Saved mmsi_train.npy, mmsi_val.npy, mmsi_test.npy\")\n",
    "\n",
    "    return df_train, df_val, df_test, mmsi_train, mmsi_val, mmsi_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikos\\AppData\\Local\\Temp\\ipykernel_18316\\1146753099.py:17: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train CSV to: ais_train.csv (rows: 191877)\n",
      "Saved val   CSV to: ais_val.csv (rows: 39665)\n",
      "Saved test  CSV to: ais_test.csv (rows: 45543)\n",
      "# MMSI -> train: 156, val: 33, test: 35\n",
      "Saved mmsi_train.npy, mmsi_val.npy, mmsi_test.npy\n"
     ]
    }
   ],
   "source": [
    "# Choose output file names (you can adjust paths if you want)\n",
    "TRAIN_CSV = \"ais_train.csv\"\n",
    "VAL_CSV   = \"ais_val.csv\"\n",
    "TEST_CSV  = \"ais_test.csv\"\n",
    "\n",
    "df_train_raw, df_val_raw, df_test_raw, mmsi_train, mmsi_val, mmsi_test = split_and_save_csv_by_mmsi(\n",
    "    full_csv_path=CSV_PATH,      # your original full AIS CSV\n",
    "    train_csv_path=TRAIN_CSV,\n",
    "    val_csv_path=VAL_CSV,\n",
    "    test_csv_path=TEST_CSV,\n",
    "    train_frac=TRAIN_FRAC,\n",
    "    val_frac=VAL_FRAC,\n",
    "    test_frac=TEST_FRAC,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4217,
     "status": "ok",
     "timestamp": 1763406289201,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "oF1rW-d0NHw6",
    "outputId": "8fcc9f4b-0051-4a78-818d-9a807033ad80"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Prepare datasets from already-split train/val/test CSV files\n",
    "\n",
    "def prepare_datasets_from_split_csvs(\n",
    "    train_csv: str,\n",
    "    val_csv: str,\n",
    "    test_csv: str,\n",
    "    input_len: int = 20,\n",
    "    output_len: int = 5,\n",
    "):\n",
    "    # Load split CSVs\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_val   = pd.read_csv(val_csv)\n",
    "    df_test  = pd.read_csv(test_csv)\n",
    "\n",
    "    # Make sure Timestamp is datetime type\n",
    "    for df in [df_train, df_val, df_test]:\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "\n",
    "    # Sort them\n",
    "    df_train = df_train.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "    df_val   = df_val.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "    df_test  = df_test.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "\n",
    "    feature_cols_in  = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\", \"speed_mps_track\"]\n",
    "    feature_cols_out = [\"Latitude\", \"Longtitude\"]\n",
    "    cols_to_scale = list(set(feature_cols_in + feature_cols_out))\n",
    "\n",
    "    # ---- Fit scaler on TRAIN ONLY ----\n",
    "    means = {}\n",
    "    stds  = {}\n",
    "    for col in cols_to_scale:\n",
    "        m = df_train[col].mean()\n",
    "        s = df_train[col].std()\n",
    "        if s == 0 or np.isnan(s):\n",
    "            s = 1.0\n",
    "        means[col] = float(m)\n",
    "        stds[col]  = float(s)\n",
    "\n",
    "    scaler = {\"mean\": means, \"std\": stds}\n",
    "\n",
    "    def normalize_df(df_subset):\n",
    "        df_norm = df_subset.copy()\n",
    "        for col in cols_to_scale:\n",
    "            df_norm[col] = (df_norm[col] - means[col]) / stds[col]\n",
    "        return df_norm\n",
    "\n",
    "    df_train_n = normalize_df(df_train)\n",
    "    df_val_n   = normalize_df(df_val)\n",
    "    df_test_n  = normalize_df(df_test)\n",
    "\n",
    "    # ---- Build sliding-window sequences per (MMSI, Segment) ----\n",
    "    def build_sequences(df_subset_norm):\n",
    "        X_list, Y_list = [], []\n",
    "        total_len = input_len + output_len\n",
    "\n",
    "        for (mmsi, seg_id), seg_df in df_subset_norm.groupby([\"MMSI\", \"Segment\"]):\n",
    "            seg_values_in  = seg_df[feature_cols_in].to_numpy(dtype=np.float32)\n",
    "            seg_values_out = seg_df[feature_cols_out].to_numpy(dtype=np.float32)\n",
    "\n",
    "            n_pts = len(seg_df)\n",
    "            if n_pts < total_len:\n",
    "                continue\n",
    "\n",
    "            for start in range(0, n_pts - total_len + 1):\n",
    "                in_slice  = seg_values_in[start : start + input_len]              # (L, 5)\n",
    "                out_slice = seg_values_out[start + input_len : start + total_len] # (H, 2)\n",
    "                X_list.append(in_slice)\n",
    "                Y_list.append(out_slice)\n",
    "\n",
    "        if not X_list:\n",
    "            X_arr = np.empty((0, input_len, len(feature_cols_in)), dtype=np.float32)\n",
    "            Y_arr = np.empty((0, output_len, len(feature_cols_out)), dtype=np.float32)\n",
    "        else:\n",
    "            X_arr = np.stack(X_list, axis=0)\n",
    "            Y_arr = np.stack(Y_list, axis=0)\n",
    "\n",
    "        return X_arr, Y_arr\n",
    "\n",
    "    X_train, Y_train = build_sequences(df_train_n)\n",
    "    X_val,   Y_val   = build_sequences(df_val_n)\n",
    "    X_test,  Y_test  = build_sequences(df_test_n)\n",
    "\n",
    "    print(f\"Train samples: X{X_train.shape}, Y{Y_train.shape}\")\n",
    "    print(f\"Val   samples: X{X_val.shape},   Y{Y_val.shape}\")\n",
    "    print(f\"Test  samples: X{X_test.shape},  Y{Y_test.shape}\")\n",
    "\n",
    "    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: X(186134, 20, 5), Y(186134, 5, 2)\n",
      "Val   samples: X(38524, 20, 5),   Y(38524, 5, 2)\n",
      "Test  samples: X(44267, 20, 5),  Y(44267, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Use the three CSVs instead of the big original file\n",
    "\n",
    "(train_X, train_Y), (val_X, val_Y), (test_X, test_Y), scaler = prepare_datasets_from_split_csvs(\n",
    "    train_csv=TRAIN_CSV,\n",
    "    val_csv=VAL_CSV,\n",
    "    test_csv=TEST_CSV,\n",
    "    input_len=INPUT_LEN,\n",
    "    output_len=OUTPUT_LEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1763406303785,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "lUj0l91iNMem",
    "outputId": "8d307e76-47d6-49ff-bccc-32767f5d0c27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 301, 346)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Dataset & DataLoaders\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "train_dataset = TrajectoryDataset(train_X, train_Y)\n",
    "val_dataset   = TrajectoryDataset(val_X,   val_Y)\n",
    "test_dataset  = TrajectoryDataset(test_X,  test_Y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OvwxzRx2NOO2"
   },
   "outputs": [],
   "source": [
    "# Cell 5: ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ LSTM Encoderâ€“Decoder\n",
    "\n",
    "class LSTMEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=5, output_dim=2,\n",
    "                 hidden_dim=128, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder Ï€Î±Î¯ÏÎ½ÎµÎ¹ ÏŒÎ»Î¿ Ï„Î¿ Î´Î¹Î¬Î½Ï…ÏƒÎ¼Î± ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… (5 dims)\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # ğŸ”§ FIX: o decoder Î½Î± Ï€Î±Î¯ÏÎ½ÎµÎ¹ ÎµÏ€Î¯ÏƒÎ·Ï‚ ÎŸÎ›ÎŸ Ï„Î¿ input_dim (5 dims),\n",
    "        # ÏŒÏ‡Î¹ Î¼ÏŒÎ½Î¿ Ï„Î± 2 (lat, lon)\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_dim,   # <-- Î®Ï„Î±Î½ output_dim\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Î’Î³Î¬Î¶Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ (lat, lon)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: (B, L, input_dim)   -- normalized\n",
    "        tgt: (B, H, output_dim)  -- normalized (lat, lon)\n",
    "        returns: (B, H, output_dim) -- normalized\n",
    "        \"\"\"\n",
    "        batch_size, H, out_dim = tgt.shape\n",
    "\n",
    "        # Encode Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ\n",
    "        _, (h, c) = self.encoder(src)  # h, c: (num_layers, B, hidden_dim)\n",
    "\n",
    "        # ğŸ”§ FIX: Î ÏÏÏ„Î¿ input Ï„Î¿Ï… decoder = Ï€Î»Î®ÏÎµÏ‚ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ Î´Î¹Î¬Î½Ï…ÏƒÎ¼Î± (lat, lon, SOG, COG, speed)\n",
    "        dec_input = src[:, -1, :]          # (B, input_dim=5)\n",
    "        dec_input = dec_input.unsqueeze(1) # (B, 1, 5)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(H):\n",
    "            # Decoder step\n",
    "            dec_out, (h, c) = self.decoder(dec_input, (h, c))  # (B, 1, hidden_dim)\n",
    "            step_output = self.fc_out(dec_out)                 # (B, 1, 2) -> lat, lon\n",
    "            outputs.append(step_output)\n",
    "\n",
    "            # Teacher forcing: ÎµÏ€Î¹Î»Î­Î³Ï‰ Î±Î½ Î¸Î± Î²Î¬Î»Ï‰ Ï„Î¿ ground truth Î® Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·\n",
    "            if self.training and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                next_latlon = tgt[:, t:t+1, :]   # (B, 1, 2) ground truth\n",
    "            else:\n",
    "                next_latlon = step_output        # (B, 1, 2) prediction\n",
    "\n",
    "            # ğŸ”§ FIX: ÎºÏÎ±Ï„Î¬Î¼Îµ SOG/COG/speed ÏŒÏ€Ï‰Ï‚ Î®Ï„Î±Î½,\n",
    "            # ÎºÎ±Î¹ Î±Î½Ï„Î¹ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î± 2 Ï€ÏÏÏ„Î± ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± (lat, lon)\n",
    "            dec_input = dec_input.clone()        # (B, 1, 5)\n",
    "            dec_input[:, :, :2] = next_latlon    # Î²Î¬Î¶Î¿Ï…Î¼Îµ Î½Î­Î¿ lat, lon\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (B, H, 2)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10861,
     "status": "ok",
     "timestamp": 1763406328283,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "Nsch-JbzNP_m",
    "outputId": "2c62fcb5-5474-42a0-cd54-c117d3fa8d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMEncoderDecoder(\n",
      "  (encoder): LSTM(5, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (decoder): LSTM(5, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (fc_out): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î¼Î¿Î½Ï„Î­Î»Î¿Ï…, loss & optimizer\n",
    "\n",
    "model = LSTMEncoderDecoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2542276,
     "status": "ok",
     "timestamp": 1763415523105,
     "user": {
      "displayName": "Nikos Fergadakis",
      "userId": "12784977084887524032"
     },
     "user_tz": -60
    },
    "id": "hANSh_l_NRTe",
    "outputId": "f3c29d24-1683-498e-f279-c12dd6438d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ New best model saved to data/lstm_best_2.pth! Val MSE = 0.000150\n",
      "Epoch 1/10 - Train MSE: 0.010706 - Val MSE: 0.000150 (Best Val: 0.000150)\n",
      "Epoch 2/10 - Train MSE: 0.000399 - Val MSE: 0.000229 (Best Val: 0.000150)\n",
      "ğŸ”¥ New best model saved to data/lstm_best_2.pth! Val MSE = 0.000101\n",
      "Epoch 3/10 - Train MSE: 0.000358 - Val MSE: 0.000101 (Best Val: 0.000101)\n",
      "Epoch 4/10 - Train MSE: 0.000318 - Val MSE: 0.000182 (Best Val: 0.000101)\n",
      "Epoch 5/10 - Train MSE: 0.000288 - Val MSE: 0.000148 (Best Val: 0.000101)\n",
      "Epoch 6/10 - Train MSE: 0.000228 - Val MSE: 0.000477 (Best Val: 0.000101)\n",
      "Epoch 7/10 - Train MSE: 0.000230 - Val MSE: 0.000146 (Best Val: 0.000101)\n",
      "Epoch 8/10 - Train MSE: 0.000221 - Val MSE: 0.000127 (Best Val: 0.000101)\n",
      "Epoch 9/10 - Train MSE: 0.000181 - Val MSE: 0.000310 (Best Val: 0.000101)\n",
      "Epoch 10/10 - Train MSE: 0.000188 - Val MSE: 0.000143 (Best Val: 0.000101)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "NUM_EPOCHS = 10  # run for 10 epochs\n",
    "\n",
    "# ğŸ‘‰ Change this to whatever path/filename you want:\n",
    "# e.g. \"/content/drive/MyDrive/deep learning/models/lstm_best.pth\"\n",
    "MODEL_PATH = \"data/lstm_best_2.pth\"\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# TRAINING LOOP WITH BEST-MODEL SAVING\n",
    "# --------------------------\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ----- TRAIN -----\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch, Y_batch, teacher_forcing_ratio=0.5)\n",
    "        loss = criterion(preds, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ----- VALIDATION -----\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "\n",
    "            preds = model(X_batch, Y_batch, teacher_forcing_ratio=0.0)\n",
    "            loss = criterion(preds, Y_batch)\n",
    "            total_val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # ----- SAVE BEST MODEL -----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"scaler\": scaler,\n",
    "            \"input_len\": INPUT_LEN,\n",
    "            \"output_len\": OUTPUT_LEN,\n",
    "        }, MODEL_PATH)\n",
    "\n",
    "        print(f\"ğŸ”¥ New best model saved to {MODEL_PATH}! Val MSE = {best_val_loss:.6f}\")\n",
    "\n",
    "    # ----- PRINT EPOCH RESULTS -----\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
    "        f\"Train MSE: {avg_train_loss:.6f} - Val MSE: {avg_val_loss:.6f} \"\n",
    "        f\"(Best Val: {best_val_loss:.6f})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load best model from file\n",
    "checkpoint = torch.load(\n",
    "    \"data/lstm_best_2.pth\",\n",
    "    map_location=device,\n",
    "    weights_only=False,  # <-- add this\n",
    ")\n",
    "\n",
    "loaded_model = LSTMEncoderDecoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_model.eval()\n",
    "\n",
    "loaded_scaler = checkpoint[\"scaler\"]\n",
    "loaded_input_len  = checkpoint[\"input_len\"]\n",
    "loaded_output_len = checkpoint[\"output_len\"]\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (normalized): 0.000105\n",
      "Mean Haversine distance per prediction step (meters): [1449.128  1531.5199 1601.2687 1708.5358 1866.7987]\n",
      "Overall mean Haversine distance (meters): 1631.4495\n"
     ]
    }
   ],
   "source": [
    "def haversine_m(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# --------- EVALUATE LOADED MODEL ---------\n",
    "loaded_model.eval()\n",
    "\n",
    "lat_mean = loaded_scaler[\"mean\"][\"Latitude\"]\n",
    "lat_std  = loaded_scaler[\"std\"][\"Latitude\"]\n",
    "lon_mean = loaded_scaler[\"mean\"][\"Longtitude\"]\n",
    "lon_std  = loaded_scaler[\"std\"][\"Longtitude\"]\n",
    "\n",
    "all_distances = []\n",
    "all_mse = 0.0\n",
    "n_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "\n",
    "        preds = loaded_model(X_batch, Y_batch, teacher_forcing_ratio=0.0)\n",
    "\n",
    "        # MSE in normalized space\n",
    "        loss = criterion(preds, Y_batch)\n",
    "        all_mse += loss.item() * X_batch.size(0)\n",
    "        n_samples += X_batch.size(0)\n",
    "\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        true_np  = Y_batch.cpu().numpy()\n",
    "\n",
    "        # Denormalize\n",
    "        true_lat = true_np[..., 0] * lat_std + lat_mean\n",
    "        true_lon = true_np[..., 1] * lon_std + lon_mean\n",
    "        pred_lat = preds_np[..., 0] * lat_std + lat_mean\n",
    "        pred_lon = preds_np[..., 1] * lon_std + lon_mean\n",
    "\n",
    "        # Compute Haversine distance\n",
    "        d = haversine_m(true_lat, true_lon, pred_lat, pred_lon)\n",
    "        all_distances.append(d)\n",
    "\n",
    "all_distances = np.concatenate(all_distances, axis=0)\n",
    "mean_per_step = all_distances.mean(axis=0)\n",
    "overall_mean  = all_distances.mean()\n",
    "\n",
    "avg_test_mse = all_mse / n_samples\n",
    "\n",
    "print(f\"Test MSE (normalized): {avg_test_mse:.6f}\")\n",
    "print(\"Mean Haversine distance per prediction step (meters):\", mean_per_step)\n",
    "print(\"Overall mean Haversine distance (meters):\", overall_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\", \"speed_mps_track\"]\n",
    "\n",
    "\n",
    "def inverse_transform_array(X, scaler):\n",
    "    X = np.asarray(X)\n",
    "    mean = np.array([scaler[\"mean\"][f] for f in FEATURES], dtype=float)\n",
    "    std  = np.array([scaler[\"std\"][f]  for f in FEATURES], dtype=float)\n",
    "    return X * std + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "FEATURES = [\"Latitude\", \"Longtitude\", \"SOG\", \"COG\", \"speed_mps_track\"]\n",
    "LAT_IDX = FEATURES.index(\"Latitude\")\n",
    "LON_IDX = FEATURES.index(\"Longtitude\")\n",
    "\n",
    "\n",
    "def plot_predicted_vs_true_folium(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    scaler,\n",
    "    batch_idx=0,\n",
    "    sample_idx=0,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    PLOTS on Folium:\n",
    "      - History (blue)\n",
    "      - True future (green)\n",
    "      - Predicted future (red)\n",
    "\n",
    "    Assumes X and Y in test_loader are in NORMALIZED space.\n",
    "    Uses `scaler` (object or dict) to denormalize back to real degrees.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # -------- 1. Get one batch from test_loader --------\n",
    "    batch = next(itertools.islice(test_loader, batch_idx, batch_idx + 1))\n",
    "    X_batch, Y_batch = batch  # X: (B, L, F), Y: (B, H, 2 or F)\n",
    "\n",
    "    X_batch = X_batch.to(device)\n",
    "    Y_batch = Y_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_batch, Y_batch, teacher_forcing_ratio=0.0)\n",
    "\n",
    "    # choose one sample\n",
    "    X_sample = X_batch[sample_idx].cpu().numpy()       # (L, F) normalized\n",
    "    Y_true_sample = Y_batch[sample_idx].cpu().numpy()  # (H, 2 or F)\n",
    "    Y_pred_sample = preds[sample_idx].cpu().numpy()    # (H, 2 or F)\n",
    "\n",
    "    # -------- 2. Denormalize lat/lon --------\n",
    "    if Y_true_sample.shape[1] == X_sample.shape[1]:\n",
    "        # Y has same F features as X (all normalized)\n",
    "        hist_denorm = inverse_transform_array(X_sample, scaler)\n",
    "        true_denorm = inverse_transform_array(Y_true_sample, scaler)\n",
    "        pred_denorm = inverse_transform_array(Y_pred_sample, scaler)\n",
    "\n",
    "        hist_lat = hist_denorm[:, LAT_IDX]\n",
    "        hist_lon = hist_denorm[:, LON_IDX]\n",
    "        true_lat = true_denorm[:, LAT_IDX]\n",
    "        true_lon = true_denorm[:, LON_IDX]\n",
    "        pred_lat = pred_denorm[:, LAT_IDX]\n",
    "        pred_lon = pred_denorm[:, LON_IDX]\n",
    "\n",
    "    elif Y_true_sample.shape[1] == 2:\n",
    "        # Denormalize history\n",
    "        hist_denorm = inverse_transform_array(X_sample, scaler)\n",
    "        hist_lat = hist_denorm[:, LAT_IDX]\n",
    "        hist_lon = hist_denorm[:, LON_IDX]\n",
    "\n",
    "        # Denormalize future ground truth\n",
    "        true_stack = np.zeros((Y_true_sample.shape[0], len(FEATURES)))\n",
    "        true_stack[:, :2] = Y_true_sample  # put normalized lat/lon in first 2 columns\n",
    "        true_denorm = inverse_transform_array(true_stack, scaler)\n",
    "        true_lat = true_denorm[:, LAT_IDX]\n",
    "        true_lon = true_denorm[:, LON_IDX]\n",
    "\n",
    "        # Denormalize future predictions (same trick)\n",
    "        pred_stack = np.zeros((Y_pred_sample.shape[0], len(FEATURES)))\n",
    "        pred_stack[:, :2] = Y_pred_sample\n",
    "        pred_denorm = inverse_transform_array(pred_stack, scaler)\n",
    "        pred_lat = pred_denorm[:, LAT_IDX]\n",
    "        pred_lon = pred_denorm[:, LON_IDX]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected Y shape {Y_true_sample.shape}; \"\n",
    "            \"update the denormalization logic accordingly.\"\n",
    "        )\n",
    "\n",
    "    # -------- 3. Center of map --------\n",
    "    all_lats = np.concatenate([hist_lat, true_lat, pred_lat])\n",
    "    all_lons = np.concatenate([hist_lon, true_lon, pred_lon])\n",
    "\n",
    "    center_lat = float(all_lats.mean())\n",
    "    center_lon = float(all_lons.mean())\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "    # -------- 4. Plot history (blue) --------\n",
    "    folium.PolyLine(\n",
    "        list(zip(hist_lat, hist_lon)),\n",
    "        popup=\"History\",\n",
    "        tooltip=\"History\",\n",
    "        weight=3,\n",
    "    ).add_to(m)\n",
    "\n",
    "    # -------- 5. Plot true future (green) --------\n",
    "    folium.PolyLine(\n",
    "        list(zip(true_lat, true_lon)),\n",
    "        popup=\"True future\",\n",
    "        tooltip=\"True future\",\n",
    "        weight=3,\n",
    "    ).add_to(m)\n",
    "\n",
    "    # -------- 6. Plot predicted future (red) --------\n",
    "    folium.PolyLine(\n",
    "        list(zip(pred_lat, pred_lon)),\n",
    "        popup=\"Predicted future\",\n",
    "        tooltip=\"Predicted future\",\n",
    "        weight=3,\n",
    "    ).add_to(m)\n",
    "\n",
    "    if save_path is not None:\n",
    "        m.save(save_path)\n",
    "        print(f\"Map saved to {save_path}\")\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to pred_vs_true_2.html\n"
     ]
    }
   ],
   "source": [
    "m = plot_predicted_vs_true_folium(\n",
    "    model=loaded_model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    scaler=loaded_scaler,\n",
    "    batch_idx=0,\n",
    "    sample_idx=0,\n",
    "    save_path=\"pred_vs_true_2.html\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP14I89y8jKLLqOdKGvbouS",
   "gpuType": "T4",
   "mount_file_id": "1WCdOogw_O6YHZnF2m-xCkhyBIHJBID-K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
